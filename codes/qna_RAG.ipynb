{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697f1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cptaswadu/new-rescue/RESCUE-n8n/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import glob\n",
    "import PyPDF2 \n",
    "import hashlib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08084eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/cptaswadu/new-rescue/RESCUE-n8n'\n",
    "load_dotenv(dotenv_path=os.path.join(path, \".env\"))\n",
    "openai_api_key = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "chatgpt_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPolicyRetriever:\n",
    "    def __init__(self, policy_folder_path, openai_api_key=None, perplexity_api_key=None, llm_model=\"gpt-4o\", cache_dir=None, embedder_id = \"all-MiniLM-L6-v2\"):\n",
    "        # Initialize class variables and embedder\n",
    "        self.policy_folder_path = policy_folder_path\n",
    "        self.policies = {}\n",
    "        self.embeddings = {}\n",
    "        self.embedder = SentenceTransformer(embedder_id)\n",
    "        self.llm_model = llm_model\n",
    "        # Set up cache directory and model clients (OpenAI / Perplexity)\n",
    "        self.openai_client = OpenAI(api_key=openai_api_key) if openai_api_key else None\n",
    "        self.perplexity_api_key = perplexity_api_key\n",
    "        self.cache_dir = cache_dir or os.path.join(\n",
    "            os.path.dirname(policy_folder_path), \"..\", \"cache\"\n",
    "        )\n",
    "        self.cache_dir = os.path.abspath(self.cache_dir)\n",
    "        self.embedder_id = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "    def load_policies(self):\n",
    "        # Load all pdf policies and extract content / compute MD5\n",
    "        pdf_files = glob.glob(os.path.join(self.policy_folder_path, \"*.pdf\")) # find all the pdf files from the path\n",
    "        self.allowed_prefixes = sorted({os.path.basename(p).split(\"_\")[0] for p in pdf_files}) # Build allowed insurer tokens from filename prefixes\n",
    "        self.doc_md5s = {}\n",
    "        for pdf_file in pdf_files:\n",
    "            with open(pdf_file, \"rb\") as f: # open the pdf file in binary mode\n",
    "                reader = PyPDF2.PdfReader(f) \n",
    "                text = \"\".join(page.extract_text() or \"\" for page in reader.pages) # extract text from each page and concatenate\n",
    "\n",
    "            fname = os.path.basename(pdf_file) # Filename only\n",
    "            self.policies[fname] = text # Store text by filename\n",
    "            self.doc_md5s[fname] = self.calculate_pdf_md5(pdf_file) # Compute and store MD5 hash for file\n",
    "        print(f\"‚úÖ Loaded {len(self.policies)} policies.\") \n",
    "\n",
    "    def calculate_pdf_md5(self, pdf_path):\n",
    "        # Compute content MD5 of a PDF file\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            return hashlib.md5(f.read()).hexdigest() # calculate the md5 hash of the pdf file (hexadecimal string)\n",
    "\n",
    "    def embed_policies(self):\n",
    "        # Create or load cached embeddings for all policies\n",
    "\n",
    "        # Ensure policies are loaded before embedding\n",
    "        if not getattr(self, \"doc_md5s\", None):\n",
    "            raise RuntimeError(\"call load_policies() before embed_policies()\")\n",
    "        \n",
    "        # Build corpus-level hash for cache invalidation\n",
    "        items = [f\"{name}:{self.doc_md5s[name]}\" for name in sorted(self.policies.keys())]\n",
    "        corpus_hash = hashlib.md5(\"\\n\".join(items).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        cache_root = os.path.join(self.cache_dir, self.embedder_id, corpus_hash)\n",
    "        os.makedirs(cache_root, exist_ok=True)\n",
    "        \n",
    "        # Paths for cached names and vectors\n",
    "        names_path = os.path.join(cache_root, \"doc_names.json\")\n",
    "        vecs_path  = os.path.join(cache_root, \"embeddings.npy\")\n",
    "\n",
    "        # Load cached names and vectors\n",
    "        if os.path.exists(names_path) and os.path.exists(vecs_path):\n",
    "            try:\n",
    "                with open(names_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    names = json.load(f)\n",
    "                vecs = np.load(vecs_path)\n",
    "                if (len(names) == len(vecs) and set(names) == set(self.policies.keys())):\n",
    "                    self.embeddings = {name: vecs[i] for i, name in enumerate(names)}\n",
    "                    print(f\"‚úÖ Loaded embeddings from cache ({len(names)} docs).\")\n",
    "                    return\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to load cache. Recomputing‚Ä¶ ({e})\")\n",
    "\n",
    "        # Compute embeddings from scratch\n",
    "        self.embeddings = {}\n",
    "        names = []\n",
    "        vec_list = []\n",
    "        for doc_name, doc_text in self.policies.items():\n",
    "            vec = self.embedder.encode([doc_text])[0]\n",
    "            self.embeddings[doc_name] = vec\n",
    "            names.append(doc_name)\n",
    "            vec_list.append(vec)\n",
    "\n",
    "        # Persist names and vectors to cache\n",
    "        try:\n",
    "            with open(names_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(names, f, ensure_ascii=False)\n",
    "            np.save(vecs_path, np.stack(vec_list, axis=0))\n",
    "            print(f\"‚úÖ Embeddings created & cached ({len(names)} docs).\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to write cache: {e}\")\n",
    "\n",
    "    def clean_json_response(self, response_text):\n",
    "        # Extract and parse clean JSON from LLM output\n",
    "        original = response_text.strip()\n",
    "\n",
    "        # Step 0: Check for hallucinated greeting (Perplexity fallback)\n",
    "        if \"how can I assist you\" in original.lower() or \"insurance-related questions\" in original.lower():\n",
    "            raise ValueError(\"Perplexity returned generic assistant response instead of JSON.\")\n",
    "\n",
    "        # Step 1: Try direct parsing\n",
    "        try:\n",
    "            return json.loads(original)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Step 2: Remove code block wrappers\n",
    "        cleaned = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", original, flags=re.IGNORECASE).strip()\n",
    "        try:\n",
    "            return json.loads(cleaned)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Step 3: Try to extract the first {...} JSON-like block\n",
    "        match = re.search(r\"(\\{[\\s\\S]*?\\})\", original)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "        raise ValueError(\"No valid JSON found in the response.\")\n",
    "    \n",
    "    def extract_insurance_and_test(self, patient_info):\n",
    "        # Extract insurance and test type from patient information using LLM\n",
    "        allowed = \", \".join([f'\"{p}\"' for p in getattr(self, \"allowed_prefixes\", [])])\n",
    "        prompt = f\"\"\"Return STRICT JSON ONLY with keys \"insurance\",\"test\"\n",
    "\n",
    "Given the patient information below, identify:\n",
    "- insurance: choose exactly one from [{allowed}] that best matches the wording\n",
    "  (e.g., \"UnitedHealthcare\", \"UHC\", \"Federal Employee Program\", \"FEP Blue\" ‚Üí map to the closest allowed token).\n",
    "- test: choose one of [\"WES\",\"WGS\",\"CMA\",\"BRCA1/2\"].\n",
    "\n",
    "PATIENT INFORMATION:\n",
    "{patient_info}\n",
    "\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an information extraction system for genetic testing insurance.\"}, # assigning system role\n",
    "            {\"role\": \"user\", \"content\": prompt} \n",
    "        ]\n",
    "\n",
    "        if self.llm_model.startswith(\"gpt\"):\n",
    "    # Check if it's a GPT-5 model and conditionally set temperature parameter\n",
    "            api_params = {\n",
    "                \"model\": self.llm_model,\n",
    "                \"messages\": messages\n",
    "            }\n",
    "    \n",
    "    # Only add temperature parameter for non-GPT-5 models\n",
    "            if \"gpt-5\" not in self.llm_model.lower():\n",
    "                api_params[\"temperature\"] = 0\n",
    "    \n",
    "            response = self.openai_client.chat.completions.create(**api_params)\n",
    "            output = response.choices[0].message.content.strip()\n",
    "\n",
    "        elif self.llm_model == \"perplexity\":\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.perplexity_api_key}\", \n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            data = {\n",
    "                \"model\": \"sonar-pro\",\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "            url = \"https://api.perplexity.ai/chat/completions\"\n",
    "            res = requests.post(url, headers=headers, json=data)\n",
    "            output = res.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported LLM model\")\n",
    "\n",
    "        try: # parsing\n",
    "            output_json = self.clean_json_response(output)\n",
    "            insurance = output_json.get(\"insurance\", None)\n",
    "            test_name = output_json.get(\"test\", None)\n",
    "            return insurance, test_name\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùó JSON parsing error in extract_insurance_and_test: {e}\")\n",
    "            print(f\"üîç Raw output: {output}\")\n",
    "            return None, None\n",
    "\n",
    "    def filter_policies_by_insurance(self, insurance_name): \n",
    "        # Filter policies by insurer token found in filename & policy text\n",
    "        # Return all policies if no insurance name provided\n",
    "        if not insurance_name:\n",
    "            return self.policies\n",
    "    \n",
    "        # Clean insurance name: remove spaces and convert to lowercase\n",
    "        insurance_clean = insurance_name.replace(\" \", \"\").lower()\n",
    "        filtered = {}\n",
    "    \n",
    "        for doc_name, doc_text in self.policies.items():\n",
    "        # Clean document name for comparison\n",
    "            doc_name_clean = doc_name.replace(\" \", \"\").lower()\n",
    "        \n",
    "            # First attempt: Match against filename (fast)\n",
    "            if insurance_clean in doc_name_clean:\n",
    "                filtered[doc_name] = doc_text # BCBS_FEP_204102 Whole Exome and.pdf -> bcbs_fep_204102wholeexomeand.pdf\n",
    "\n",
    "            # Second attempt: Match against document content beginning (fallback)\n",
    "\n",
    "            elif insurance_clean in doc_text.lower()[:1000]:\n",
    "                filtered[doc_name] = doc_text \n",
    "    \n",
    "        return filtered\n",
    "\n",
    "    def get_test_keywords(self, test_name):\n",
    "        # Map test name to a keyword list for filtering\n",
    "        if not test_name:\n",
    "            return []\n",
    "        \n",
    "        test_keywords_map = {\n",
    "    \"brca1/2\": [\n",
    "        \"brca\", \"brca1\", \"brca2\", \"brca1/2\", \"brca 1/2\"\n",
    "        \"breast cancer\", \"ovarian cancer\", \"pancreatic cancer\", \"prostate cancer\", \"metastatic\"\n",
    "    ],\n",
    "    \"wes\": [\n",
    "        \"whole exome sequencing\", \"wes\", \"exome sequencing\",\n",
    "        \"multiple congenital anomalies\", \"neurodevelopmental disorder\", \n",
    "        \"developmental delay\", \"unexplained disorder\", \"autism\", \"organ abnormality\"\n",
    "    ],\n",
    "    \"wgs\": [\n",
    "        \"whole genome sequencing\", \"wgs\", \"genome sequencing\",\n",
    "        \"congenital disorder\", \"fetal anomalies\", \n",
    "        \"unexplained anomalies\", \"developmental disorder\", \"organ system abnormality\"\n",
    "    ],\n",
    "    \"cma\": [\n",
    "        \"chromosomal microarray\", \"cma\", \"copy number variation\", \n",
    "        \"developmental delay\", \"intellectual disability\", \"autism\", \n",
    "        \"congenital anomalies\", \"global developmental delay\"\n",
    "    ]\n",
    "}\n",
    "    \n",
    "        test_lower = test_name.lower()\n",
    "        for test_type, keywords in test_keywords_map.items():\n",
    "            if test_type in test_lower:\n",
    "                return keywords\n",
    "    \n",
    "        return [test_name.lower()]\n",
    "\n",
    "\n",
    "    def filter_by_test_keywords(self, policies_dict, test_name):\n",
    "        # Filter policies whose text contains any test keyword\n",
    "        if not test_name or not policies_dict:\n",
    "            return policies_dict\n",
    "        \n",
    "        test_keywords = self.get_test_keywords(test_name)\n",
    "        filtered = {}\n",
    "    \n",
    "        for doc_name, doc_text in policies_dict.items():\n",
    "            doc_text_lower = doc_text.lower()\n",
    "            for keyword in test_keywords:\n",
    "                if keyword in doc_text_lower:\n",
    "                    filtered[doc_name] = doc_text\n",
    "                    break  \n",
    "                \n",
    "        return filtered\n",
    "\n",
    "    def find_top_policies(self, patient_info, insurance_name, test_name=None, top_k=5):\n",
    "        # Retrieve top-k policies by cosine similarity\n",
    "        filtered_policies = self.filter_policies_by_insurance(insurance_name)\n",
    "        if not filtered_policies:\n",
    "            print(\"‚ùó No policies matched the insurance. Using all policies.\")\n",
    "            filtered_policies = self.policies\n",
    "\n",
    "        if test_name and filtered_policies:\n",
    "            test_filtered = self.filter_by_test_keywords(filtered_policies, test_name)\n",
    "            if test_filtered:\n",
    "                print(f\"‚úÖ Found {len(test_filtered)} policies matching test '{test_name}'\")\n",
    "                filtered_policies = test_filtered\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No policies matched test '{test_name}'. Using insurance-filtered policies.\")\n",
    "\n",
    "        query_embedding = self.embedder.encode([patient_info])[0]\n",
    "        scored_policies = []\n",
    "        for doc_name, doc_text in filtered_policies.items():\n",
    "            doc_embedding = self.embeddings[doc_name]\n",
    "            score = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "            scored_policies.append((doc_name, score, doc_text))\n",
    "\n",
    "        scored_policies.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored_policies[:top_k]\n",
    "\n",
    "    def rerank_policies(self, patient_info, candidates):\n",
    "        # Use LLM to pick the best among candidates\n",
    "        candidate_texts = [c[2][:500].replace(\"\\n\", \" \") for c in candidates]\n",
    "\n",
    "        prompt = f\"\"\"You are an expert insurance policy analyst specializing in genetic testing coverage.\n",
    "\n",
    "You will be given patient information and a list of candidate insurance policies.\n",
    "Select the policy that BEST COVERS the patient's specific genetic test and medical condition.\n",
    "\n",
    "Patient Information:\n",
    "{patient_info}\n",
    "\n",
    "Candidate Policies:\"\"\"\n",
    "\n",
    "        for idx, text in enumerate(candidate_texts, 1):\n",
    "            prompt += f\"\\n\\nPolicy {idx}:\\n{text}\"\n",
    "\n",
    "        prompt += \"\"\"\n",
    "\n",
    "Please answer with only the number of the most appropriate policy.\n",
    "Do not explain. Just output the number.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an information extraction system for ranking the most appropriate insurance policies.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        if self.llm_model.startswith(\"gpt\"):\n",
    "    # Check if it's a GPT-5 model and conditionally set temperature parameter\n",
    "            api_params = {\n",
    "                \"model\": self.llm_model,\n",
    "                \"messages\": messages\n",
    "            }\n",
    "    \n",
    "    # Only add temperature parameter for non-GPT-5 models\n",
    "            if \"gpt-5\" not in self.llm_model.lower():\n",
    "                api_params[\"temperature\"] = 0\n",
    "    \n",
    "            response = self.openai_client.chat.completions.create(**api_params)\n",
    "            result = response.choices[0].message.content.strip()\n",
    "\n",
    "        elif self.llm_model == \"perplexity\":\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            data = {\n",
    "                \"model\": \"sonar-pro\",\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "            url = \"https://api.perplexity.ai/chat/completions\"\n",
    "            res = requests.post(url, headers=headers, json=data)\n",
    "            result = res.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported LLM model\")\n",
    "\n",
    "        match = re.search(r'(\\d+)', result)\n",
    "        selected_idx = int(match.group(1)) - 1 if match else 0\n",
    "        return candidates[selected_idx]\n",
    "\n",
    "    def find_policies_with_matching_check(self, patient_info, expected_md5, top_k=10):\n",
    "        # End-to-end retrieval + (optional) rerank + MD5 match\n",
    "        insurance, test = self.extract_insurance_and_test(patient_info)\n",
    "        candidates = self.find_top_policies(patient_info, insurance, test, top_k=top_k)\n",
    "\n",
    "        # If top_k=1, rerank to pick best; else check in order\n",
    "        if top_k == 1:\n",
    "            print(\"üîÑ Performing reranking for top-1\")\n",
    "            best_policy = self.rerank_policies(patient_info, candidates)\n",
    "            doc_name, doc_text = best_policy[0], best_policy[2]\n",
    "        \n",
    "            pdf_path = os.path.join(self.policy_folder_path, doc_name)\n",
    "            predicted_md5 = self.calculate_pdf_md5(pdf_path)\n",
    "        \n",
    "            if predicted_md5 == expected_md5:\n",
    "                return doc_name, doc_text, predicted_md5\n",
    "        else:\n",
    "            # Check candidates in order until MD5 match found when top_k > 1\n",
    "            for doc_name, score, doc_text in candidates:\n",
    "                pdf_path = os.path.join(self.policy_folder_path, doc_name)\n",
    "                predicted_md5 = self.calculate_pdf_md5(pdf_path)\n",
    "            \n",
    "                if predicted_md5 == expected_md5:\n",
    "                    return doc_name, doc_text, predicted_md5\n",
    "    \n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dfc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QnAExecutor:\n",
    "    # Initialize the executor with necessary parameters\n",
    "    def __init__(self, questions_list, llm_model=\"gpt-4o\", openai_client=None, perplexity_api_key=None):\n",
    "        self.questions_list = questions_list\n",
    "        self.formatted_questions = self.format_questions()\n",
    "        self.llm_model = llm_model\n",
    "        self.openai_client = openai_client\n",
    "        self.perplexity_api_key = perplexity_api_key\n",
    "\n",
    "    def format_question_block(self, q, indent=2):\n",
    "        # Format a single question block with indentation\n",
    "        indent_str = \" \" * indent\n",
    "        question_line = f\"{q['question']}\"\n",
    "        question_line += f\"\\n{indent_str}Options: {', '.join(q['options'])}\"\n",
    "        return question_line\n",
    "\n",
    "\n",
    "    def format_questions(self):\n",
    "        # Join all questions into a single prompt chunk\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"{q['id']}. {self.format_question_block(q)}\"\n",
    "            for q in self.questions_list\n",
    "        ])\n",
    "\n",
    "\n",
    "    def clean_json_response(self, response_text):\n",
    "        # Clean and extract JSON from the response text\n",
    "        original = response_text.strip()\n",
    "\n",
    "        # Step 0: Check for hallucinated greeting (Perplexity fallback)\n",
    "        if \"how can I assist you\" in original.lower() or \"insurance-related questions\" in original.lower():\n",
    "            raise ValueError(\"Perplexity returned generic assistant response instead of JSON.\")\n",
    "\n",
    "        # Step 1: Try direct parsing\n",
    "        try:\n",
    "            return json.loads(original)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Step 2: Remove code block wrappers\n",
    "        cleaned = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", original, flags=re.IGNORECASE).strip()\n",
    "        try:\n",
    "            return json.loads(cleaned)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Step 3: Try to extract the first {...} JSON-like block\n",
    "        match = re.search(r\"(\\{[\\s\\S]*?\\})\", original)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "        raise ValueError(\"No valid JSON found in the response.\")\n",
    "\n",
    "    def run_qna(self, patient_info, policy_name, policy_text, case_id, retrieval_model, qna_model, predicted_md5 = None, top_k=None):\n",
    "        prompt = f\"\"\"\n",
    "You are a clinical insurance assistant specializing in genetic testing coverage policies.\n",
    "You MUST answer in JSON format only.\n",
    "You will be given:\n",
    "\n",
    "1. Patient clinical information (including their insurance provider, plan type, and state of residence).\n",
    "2. Official insurance policy document text (strictly use this policy content for insurance coverage decision making).\n",
    "\n",
    "Instructions:\n",
    "- Answer all questions strictly based on the insurance policy document provided.\n",
    "- Do NOT refer to general guidelines or policies from other insurance providers.\n",
    "- If policy document does not clearly specify rules, you MAY use patient's clinical information to infer answers carefully.\n",
    "- Do NOT assume coverage criteria from other insurers or general clinical guidelines unless explicitly stated in the policy.\n",
    "- Output answers in JSON format ONLY.\n",
    "\n",
    "Focus on sections for uploaded policy document:\n",
    "- **Age criteria**\n",
    "- **Medical necessity criteria**\n",
    "- **Prior test criteria**\n",
    "- **Family history information** \n",
    "- **Related CPT codes**\n",
    "- **Coverage criteria**\n",
    "- **Counseling / Provider criteria**\n",
    "\n",
    "Patient Information:\n",
    "{patient_info}\n",
    "\n",
    "Insurance Policy Document (source: {policy_name})\n",
    "{policy_text}\n",
    "\n",
    "Based on the uploaded policy document and patient information, answer these questions:\n",
    "{self.formatted_questions}\n",
    "\n",
    "Output your answers in JSON format only, with no explanation.\n",
    "Your response must follow this exact structure:\n",
    "{{\n",
    "  \"Q0\": \"WES\",\n",
    "  \"Q1\": \"Yes\",\n",
    "  \"Q2\": \"Not Specified\",\n",
    "  \"Q3\": \"Not Specified\",\n",
    "  \"Q4\": \"No\",\n",
    "  \"Q5\": \"No\", \n",
    "  \"Q6\": \"Not Specified\",\n",
    "  \"Q7\": \"81415\",\n",
    "  \"Q8\": \"No\"\n",
    "}}\n",
    "\n",
    "Answer options for each question:\n",
    "- Q0: [\"WES\", \"WGS\", \"BRCA1/2\", \"CMA\"]\n",
    "- Q1: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q2: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q3: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q4: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q5: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q6: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q7: [\"81162\", \"81277\", \"81228\", \"81415\", \"81425\", \"Not Specified\"]\n",
    "- Q8: [\"Yes\", \"No\"]\n",
    "\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a clinical insurance assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        if qna_model.startswith(\"gpt\"):\n",
    "            api_params = {\n",
    "                \"model\": qna_model,  # üëà ÏàòÏ†ï\n",
    "                \"messages\": messages\n",
    "            }\n",
    "            if \"gpt-5\" not in qna_model.lower():\n",
    "                api_params[\"temperature\"] = 0\n",
    "            response = self.openai_client.chat.completions.create(**api_params)\n",
    "            result_content = response.choices[0].message.content.strip()\n",
    "        \n",
    "        elif qna_model == \"perplexity\":  # üëà ÏàòÏ†ï\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            data = {\n",
    "                \"model\": \"sonar-pro\",\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "            url = \"https://api.perplexity.ai/chat/completions\"\n",
    "            res = requests.post(url, headers=headers, json=data)\n",
    "            result_content = res.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported QnA model: {qna_model}\")\n",
    "\n",
    "        result_json = {}\n",
    "\n",
    "        try:\n",
    "            result_json = self.clean_json_response(result_content)\n",
    "            final_result = result_json.copy() \n",
    "            \n",
    "            if predicted_md5 is not None:\n",
    "                final_result[\"predicted_md5\"] = predicted_md5\n",
    "\n",
    "            model_name = f\"{retrieval_model}_{qna_model}\"\n",
    "            if top_k is not None:\n",
    "                save_dir = f\"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/{model_name}/top{top_k}\"\n",
    "            else:\n",
    "                save_dir = f\"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/{model_name}\"\n",
    "    \n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            filename = os.path.join(save_dir, f\"{case_id}_qna_result.json\")\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(final_result, f, indent=2)\n",
    "\n",
    "            print(f\"‚úÖ QnA result saved to {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"‚ùó JSON parsing error:\", e)\n",
    "            final_result = {\n",
    "                \"error\": \"JSON parsing failed\",\n",
    "                \"raw_content\": result_content\n",
    "            }\n",
    "\n",
    "        print(\"QnA Result JSON:\", final_result)\n",
    "        return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128f07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_file_path = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/Insurance_Genetic_Testing_QA.json\"\n",
    "\n",
    "with open(questions_file_path, \"r\") as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "questions_list = questions_data[\"questions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Running: gpt-5-mini_gpt-5-mini\n",
      "‚úÖ Loaded 789 policies.\n",
      "‚úÖ Embeddings created & cached (789 docs).\n",
      "\n",
      "=== Processing with Top-1 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case10917 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case8051 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top1/Case11124_top1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Not Specified', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-1): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case7376 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case10451 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top1/Case19321_top1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-1): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case9349 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case10363 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case4512 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case18257 (Top-1): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top1/gpt-5-mini_gpt-5-mini_sample_top1_matching.csv\n",
      "\n",
      "=== Processing with Top-3 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚ùå Case10917 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top3/Case8051_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'No', 'Q4': 'No', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top3/Case11124_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top3/Case7376_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Not Specified', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Not Specified', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top3/Case10451_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top3/Case19321_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-3): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚ùå Case9349 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top3/Case10363_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-3): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚ùå Case4512 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚ùå Case18257 (Top-3): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top3/gpt-5-mini_gpt-5-mini_sample_top3_matching.csv\n",
      "\n",
      "=== Processing with Top-5 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/Case10917_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Not Specified', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10917 (Top-5): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/Case8051_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'Not Specified', 'Q6': 'Yes', 'Q7': 'Not Specified', 'Q8': 'No', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/Case11124_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Not Specified', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/Case7376_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Not Specified', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Not Specified', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/Case10451_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/Case19321_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-5): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/Case9349_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case9349 (Top-5): QnA executed\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/Case10363_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-5): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/Case4512_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'No', 'Q2': 'Not Specified', 'Q3': 'No', 'Q4': 'No', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': 'Not Specified', 'Q8': 'No', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case4512 (Top-5): QnA executed\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚ùå Case18257 (Top-5): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/gpt-5-mini_gpt-5-mini_sample_top5_matching.csv\n",
      "\n",
      "=== Processing with Top-10 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/Case10917_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Not Specified', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10917 (Top-10): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/Case8051_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/Case11124_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Not Specified', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/Case7376_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Not Specified', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/Case10451_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/Case19321_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-10): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/Case9349_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case9349 (Top-10): QnA executed\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/Case10363_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-10): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/Case4512_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'No', 'Q2': 'Not Specified', 'Q3': 'No', 'Q4': 'Not Specified', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case4512 (Top-10): QnA executed\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/Case18257_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'BRCA1/2', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81162', 'Q8': 'No', 'predicted_md5': '626eac4d60df057ea93ece78f8cc3dfc'}\n",
      "‚úÖ Case18257 (Top-10): QnA executed\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/gpt-5-mini_gpt-5-mini_sample_top10_matching.csv\n",
      "\n",
      "üöÄ Running: gpt-5-mini_perplexity\n",
      "‚úÖ Loaded 789 policies.\n",
      "‚úÖ Loaded embeddings from cache (789 docs).\n",
      "\n",
      "=== Processing with Top-1 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case10917 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case8051 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top1/Case11124_top1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-1): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case7376 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case10451 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top1/Case19321_top1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-1): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case9349 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case10363 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case4512 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case18257 (Top-1): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top1/gpt-5-mini_perplexity_sample_top1_matching.csv\n",
      "\n",
      "=== Processing with Top-3 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚ùå Case10917 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top3/Case8051_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Not Specified', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top3/Case11124_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top3/Case7376_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'No', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top3/Case10451_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top3/Case19321_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-3): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚ùå Case9349 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top3/Case10363_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-3): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚ùå Case4512 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚ùå Case18257 (Top-3): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top3/gpt-5-mini_perplexity_sample_top3_matching.csv\n",
      "\n",
      "=== Processing with Top-5 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/Case10917_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10917 (Top-5): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/Case8051_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/Case11124_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/Case7376_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'No', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/Case10451_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/Case19321_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-5): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/Case9349_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case9349 (Top-5): QnA executed\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/Case10363_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-5): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/Case4512_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'No', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case4512 (Top-5): QnA executed\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚ùå Case18257 (Top-5): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/gpt-5-mini_perplexity_sample_top5_matching.csv\n",
      "\n",
      "=== Processing with Top-10 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/Case10917_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10917 (Top-10): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/Case8051_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Not Specified', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/Case11124_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/Case7376_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'No', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/Case10451_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Not Specified', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/Case19321_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-10): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/Case9349_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case9349 (Top-10): QnA executed\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/Case10363_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-10): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/Case4512_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case4512 (Top-10): QnA executed\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/Case18257_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'BRCA1/2', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'No', 'Q6': 'No', 'Q7': '81162', 'Q8': 'No', 'predicted_md5': '626eac4d60df057ea93ece78f8cc3dfc'}\n",
      "‚úÖ Case18257 (Top-10): QnA executed\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/gpt-5-mini_perplexity_sample_top10_matching.csv\n",
      "\n",
      "üöÄ Running: perplexity_gpt-5-mini\n",
      "‚úÖ Loaded 789 policies.\n",
      "‚úÖ Loaded embeddings from cache (789 docs).\n",
      "\n",
      "=== Processing with Top-1 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Failed on Case10917 (perplexity_gpt-5-mini, Top-1): list index out of range\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case8051 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top1/Case11124_top1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Not Specified', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-1): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case7376 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case10451 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top1/Case19321_top1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Not Specified', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-1): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case9349 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case10363 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case4512 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case18257 (Top-1): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top1/perplexity_gpt-5-mini_sample_top1_matching.csv\n",
      "\n",
      "=== Processing with Top-3 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚ùå Case10917 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top3/Case8051_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top3/Case11124_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Not Specified', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top3/Case7376_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Not Specified', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top3/Case10451_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top3/Case19321_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-3): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚ùå Case9349 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top3/Case10363_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-3): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚ùå Case4512 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚ùå Case18257 (Top-3): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top3/perplexity_gpt-5-mini_sample_top3_matching.csv\n",
      "\n",
      "=== Processing with Top-5 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/Case10917_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Not Specified', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Not Specified', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10917 (Top-5): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/Case8051_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Not Specified', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/Case11124_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/Case7376_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Not Specified', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Not Specified', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/Case10451_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/Case19321_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-5): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/Case9349_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case9349 (Top-5): QnA executed\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/Case10363_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Not Specified', 'Q2': 'No', 'Q3': 'No', 'Q4': 'Not Specified', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-5): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/Case4512_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'No', 'Q2': 'Not Specified', 'Q3': 'No', 'Q4': 'No', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case4512 (Top-5): QnA executed\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚ùå Case18257 (Top-5): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/perplexity_gpt-5-mini_sample_top5_matching.csv\n",
      "\n",
      "=== Processing with Top-10 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/Case10917_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Not Specified', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10917 (Top-10): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/Case8051_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/Case11124_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Not Specified', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/Case7376_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Not Specified', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Not Specified', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/Case10451_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/Case19321_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-10): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/Case9349_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case9349 (Top-10): QnA executed\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/Case10363_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-10): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/Case4512_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'No', 'Q2': 'Not Specified', 'Q3': 'No', 'Q4': 'No', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': 'Not Specified', 'Q8': 'No', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case4512 (Top-10): QnA executed\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/Case18257_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'BRCA1/2', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'No', 'Q6': 'No', 'Q7': '81162', 'Q8': 'No', 'predicted_md5': '626eac4d60df057ea93ece78f8cc3dfc'}\n",
      "‚úÖ Case18257 (Top-10): QnA executed\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/perplexity_gpt-5-mini_sample_top10_matching.csv\n",
      "\n",
      "üöÄ Running: perplexity_perplexity\n",
      "‚úÖ Loaded 789 policies.\n",
      "‚úÖ Loaded embeddings from cache (789 docs).\n",
      "\n",
      "=== Processing with Top-1 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Failed on Case10917 (perplexity_perplexity, Top-1): list index out of range\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case8051 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top1/Case11124_top1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-1): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case7376 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case10451 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top1/Case19321_top1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-1): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Failed on Case9349 (perplexity_perplexity, Top-1): list index out of range\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case10363 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case4512 (Top-1): No matching policy - QnA skipped\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "üîÑ Performing reranking for top-1\n",
      "‚ùå Case18257 (Top-1): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top1/perplexity_perplexity_sample_top1_matching.csv\n",
      "\n",
      "=== Processing with Top-3 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚ùå Case10917 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top3/Case8051_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top3/Case11124_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top3/Case7376_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'No', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top3/Case10451_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-3): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top3/Case19321_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-3): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚ùå Case9349 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top3/Case10363_top3_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-3): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚ùå Case4512 (Top-3): No matching policy - QnA skipped\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚ùå Case18257 (Top-3): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top3/perplexity_perplexity_sample_top3_matching.csv\n",
      "\n",
      "=== Processing with Top-5 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/Case10917_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10917 (Top-5): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/Case8051_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/Case11124_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/Case7376_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'No', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/Case10451_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-5): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/Case19321_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-5): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/Case9349_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case9349 (Top-5): QnA executed\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/Case10363_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-5): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/Case4512_top5_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'No', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case4512 (Top-5): QnA executed\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚ùå Case18257 (Top-5): No matching policy - QnA skipped\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/perplexity_perplexity_sample_top5_matching.csv\n",
      "\n",
      "=== Processing with Top-10 ===\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/Case10917_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10917 (Top-10): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/Case8051_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case8051 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/Case11124_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case11124 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/Case7376_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'No', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case7376 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/Case10451_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'No', 'Q6': 'No', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10451 (Top-10): QnA executed\n",
      "‚úÖ Found 11 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/Case19321_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case19321 (Top-10): QnA executed\n",
      "‚úÖ Found 135 policies matching test 'WES'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/Case9349_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WES', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81415', 'Q8': 'Yes', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case9349 (Top-10): QnA executed\n",
      "‚úÖ Found 9 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/Case10363_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'No', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': '4fadf6b3ca9d4d08131cb31365e3aa7d'}\n",
      "‚úÖ Case10363 (Top-10): QnA executed\n",
      "‚úÖ Found 62 policies matching test 'WGS'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/Case4512_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'WGS', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'No', 'Q4': 'Yes', 'Q5': 'Yes', 'Q6': 'Yes', 'Q7': '81425', 'Q8': 'No', 'predicted_md5': 'd5e9701c13de1dca302ad0ce45524039'}\n",
      "‚úÖ Case4512 (Top-10): QnA executed\n",
      "‚úÖ Found 23 policies matching test 'BRCA1/2'\n",
      "‚úÖ QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/Case18257_top10_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'BRCA1/2', 'Q1': 'Yes', 'Q2': 'Not Specified', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'No', 'Q6': 'No', 'Q7': '81162', 'Q8': 'No', 'predicted_md5': '626eac4d60df057ea93ece78f8cc3dfc'}\n",
      "‚úÖ Case18257 (Top-10): QnA executed\n",
      "üìä Matching stats saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/perplexity_perplexity_sample_top10_matching.csv\n"
     ]
    }
   ],
   "source": [
    "case_file_path = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/sample_qna_free_text.json\"\n",
    "with open(case_file_path, \"r\") as f:\n",
    "    case_ex = json.load(f)\n",
    "\n",
    "retrieval_models = [\"gpt-5-mini\", \"perplexity\"]\n",
    "qna_models = [\"gpt-5-mini\", \"perplexity\"]\n",
    "\n",
    "model_combinations = [(r, q) for r in retrieval_models for q in qna_models]\n",
    "\n",
    "evaluation_dir = f\"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual\"\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "top_k_values = [1, 3, 5, 10]\n",
    "for retrieval_model, qna_model in model_combinations:\n",
    "    print(f\"\\nüöÄ Running: {retrieval_model}_{qna_model}\")\n",
    "    model_name = f\"{retrieval_model}_{qna_model}\"\n",
    "    \n",
    "    retriever = RAGPolicyRetriever(\n",
    "        policy_folder_path=\"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/insurance_policy\",\n",
    "        openai_api_key=openai_api_key,\n",
    "        perplexity_api_key=perplexity_api_key,\n",
    "        llm_model=retrieval_model\n",
    "    )\n",
    "    retriever.load_policies()\n",
    "    retriever.embed_policies()\n",
    "\n",
    "    executor = QnAExecutor(\n",
    "        questions_list=questions_list,\n",
    "        llm_model=qna_model,\n",
    "        openai_client=retriever.openai_client,\n",
    "        perplexity_api_key=perplexity_api_key\n",
    "    )\n",
    "\n",
    "    for k in top_k_values:\n",
    "        print(f\"\\n=== Processing with Top-{k} ===\")\n",
    "        \n",
    "        matching_stats = []\n",
    "\n",
    "        for case in case_ex:\n",
    "            case_id = case[\"id\"]\n",
    "            patient_info = case[\"patient_info\"]\n",
    "            expected_md5 = case[\"expected_md5\"]  \n",
    "\n",
    "            try:\n",
    "                result = retriever.find_policies_with_matching_check(\n",
    "                    patient_info, expected_md5, top_k=k)\n",
    "                \n",
    "                matching_stats.append({\n",
    "                    'case_id': case_id,\n",
    "                    'matched': result[0] is not None,\n",
    "                    'policy_name': result[0] if result[0] else None,\n",
    "                    'predicted_md5': result[2] if result[2] else None\n",
    "                })\n",
    "                \n",
    "                if result[0] is not None:\n",
    "                    policy_name, policy_text, predicted_md5 = result\n",
    "                    executor.run_qna(\n",
    "                        patient_info=patient_info,\n",
    "                        policy_name=policy_name,\n",
    "                        policy_text=policy_text,\n",
    "                        case_id=f\"{case_id}_top{k}\",\n",
    "                        retrieval_model=retrieval_model,\n",
    "                        qna_model=qna_model,\n",
    "                        predicted_md5=predicted_md5,\n",
    "                        top_k=k\n",
    "                    )\n",
    "                    print(f\"‚úÖ {case_id} (Top-{k}): QnA executed\")\n",
    "                else:\n",
    "                    print(f\"‚ùå {case_id} (Top-{k}): No matching policy - QnA skipped\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                matching_stats.append({\n",
    "                    'case_id': case_id,\n",
    "                    'matched': False,\n",
    "                    'policy_name': None,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                print(f\"‚ùå Failed on {case_id} ({retrieval_model}_{qna_model}, Top-{k}): {e}\")\n",
    "\n",
    "        model_folder = f\"{retrieval_model}_{qna_model}\"\n",
    "        topk_dir = os.path.join(\n",
    "            \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample\",\n",
    "            model_folder,\n",
    "            f\"top{k}\"\n",
    "        )\n",
    "        os.makedirs(topk_dir, exist_ok=True)\n",
    "\n",
    "        stats_df = pd.DataFrame(matching_stats)\n",
    "        stats_path = os.path.join(\n",
    "            topk_dir,\n",
    "            f\"{model_folder}_sample_top{k}_matching.csv\"\n",
    "        )\n",
    "        stats_df.to_csv(stats_path, index=False)\n",
    "        print(f\"üìä Matching stats saved: {stats_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_bool(x):\n",
    "    if isinstance(x, (int, float, bool)):\n",
    "        return bool(x)\n",
    "    return str(x).strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"t\"}\n",
    "\n",
    "def calculate_policy_match_rates(base_dir):\n",
    "    # Aggregate policy match rates across model/topK folders\n",
    "    match_rate_results = []\n",
    "\n",
    "    # Walk model folders, skip non-dirs and \"evaluation\"\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        if not os.path.isdir(folder_path) or folder_name == \"evaluation\":\n",
    "            continue\n",
    "\n",
    "        # Find all topK subfolders\n",
    "        for subfolder in os.listdir(folder_path):\n",
    "            subfolder_path = os.path.join(folder_path, subfolder)\n",
    "            if not (os.path.isdir(subfolder_path) and subfolder.startswith(\"top\")):\n",
    "                continue\n",
    "\n",
    "            stats_files = [f for f in os.listdir(subfolder_path) if f.endswith(\"_matching.csv\")]\n",
    "            for stats_file in stats_files:\n",
    "                csv_path = os.path.join(subfolder_path, stats_file)\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Failed to read {csv_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if \"matched\" not in df.columns:\n",
    "                    print(f\"‚ö†Ô∏è 'matched' column not found in {csv_path}. Skipped.\")\n",
    "                    continue\n",
    "\n",
    "                matched_series = df[\"matched\"].map(_to_bool) # Coerce matched values to booleans\n",
    "                total_cases = len(matched_series)\n",
    "                matched_cases = int(matched_series.sum())\n",
    "                match_rate = (matched_cases / total_cases * 100) if total_cases > 0 else 0.0\n",
    "\n",
    "                # Derive label from filename (model_combo_topK)\n",
    "                model_combination = stats_file.replace(\"_matching.csv\", \"\")\n",
    "\n",
    "                match_rate_results.append({\n",
    "                    \"Model_Combination\": model_combination,\n",
    "                    \"Total_Cases\": total_cases,\n",
    "                    \"Matched_Cases\": matched_cases,\n",
    "                    \"Policy_Match_Rate\": f\"{match_rate:.2f}%\"\n",
    "                })\n",
    "\n",
    "    if not match_rate_results:\n",
    "        print(f\"‚ö†Ô∏è No matching CSVs found under model/topk folders in: {base_dir}\")\n",
    "\n",
    "    return match_rate_results\n",
    "\n",
    "\n",
    "def get_policy_match_rate(model_name, csv_file):\n",
    "    base_dir = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample\"\n",
    "\n",
    "    # extract model name\n",
    "    model_topk = csv_file.replace(\".csv\", \"\")\n",
    "    m = re.search(r\"_top(\\d+)$\", model_topk)\n",
    "    if not m:\n",
    "        print(f\"‚ö†Ô∏è Could not parse top-k from filename: {csv_file}\")\n",
    "        return \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "    # Extract K (Top-K) and base model combo string\n",
    "    k = m.group(1)\n",
    "    base_model = model_topk[:m.start()]               \n",
    "\n",
    "    folder_path = os.path.join(base_dir, base_model, f\"top{k}\")\n",
    "\n",
    "    \n",
    "    filename_candidates = [\n",
    "        f\"{base_model}_top{k}_matching.csv\",\n",
    "        f\"{base_model}_sample_top{k}_matching.csv\",\n",
    "    ]\n",
    "\n",
    "    \n",
    "    for fname in filename_candidates:\n",
    "        matching_stats_path = os.path.join(folder_path, fname)\n",
    "        if os.path.exists(matching_stats_path):\n",
    "            try:\n",
    "                match_df = pd.read_csv(matching_stats_path)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to read {matching_stats_path}: {e}\")\n",
    "                return \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "            if \"matched\" not in match_df.columns:\n",
    "                print(f\"‚ö†Ô∏è 'matched' column not found in {matching_stats_path}\")\n",
    "                return \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "            # Compute and return (rate%, total, matched)\n",
    "            matched_series = match_df[\"matched\"].map(_to_bool)\n",
    "            total_attempted = len(matched_series)\n",
    "            matched_count = int(matched_series.sum())\n",
    "            match_rate = (matched_count / total_attempted * 100) if total_attempted > 0 else 0.0\n",
    "            return f\"{match_rate:.2f}%\", total_attempted, matched_count\n",
    "\n",
    "    # If all attempts fail, list files in the folder for debugging\n",
    "    try:\n",
    "        listing = os.listdir(folder_path)\n",
    "        print(f\"‚ö†Ô∏è Matching file not found in {folder_path}. Files: {listing}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Matching file not found and failed to list {folder_path}: {e}\")\n",
    "\n",
    "    return \"N/A\", \"N/A\", \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/gpt-5-mini_gpt-5-mini_top10.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top1/gpt-5-mini_gpt-5-mini_top1.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top3/gpt-5-mini_gpt-5-mini_top3.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/gpt-5-mini_gpt-5-mini_top5.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/perplexity_gpt-5-mini_top10.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top1/perplexity_gpt-5-mini_top1.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top3/perplexity_gpt-5-mini_top3.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/perplexity_gpt-5-mini_top5.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/gpt-5-mini_perplexity_top10.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top1/gpt-5-mini_perplexity_top1.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top3/gpt-5-mini_perplexity_top3.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/gpt-5-mini_perplexity_top5.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/perplexity_perplexity_top10.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top1/perplexity_perplexity_top1.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top3/perplexity_perplexity_top3.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/perplexity_perplexity_top5.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_qna_jsons_to_csv(folder_path, output_csv_path):\n",
    "    all_data = []\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\"_qna_result.json\"):\n",
    "            # Extract case_id (e.g., \"Case884\" from \"Case884_qna_result.json\") \n",
    "            case_id = file.replace(\"_qna_result.json\", \"\")\n",
    "            json_path = os.path.join(folder_path, file)\n",
    "\n",
    "            with open(json_path, \"r\") as f:\n",
    "                try:\n",
    "                    result = json.load(f)\n",
    "                    flat_result = {\"case_id\": case_id}\n",
    "\n",
    "                    for k, v in result.items():\n",
    "                        if isinstance(v, list):\n",
    "                            flat_result[k] = \"; \".join(map(str, v))\n",
    "                        else:\n",
    "                            flat_result[k] = v\n",
    "\n",
    "                    all_data.append(flat_result)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùó Failed to parse {file}: {e}\")\n",
    "\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"‚úÖ Merged CSV saved to: {output_csv_path}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No valid QnA result files found in: {folder_path}\")\n",
    "\n",
    "\n",
    "def merge_all_combinations_to_csv(base_dir):\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        if os.path.isdir(folder_path) and folder_name != \"evaluation\":  # evaluation excluded\n",
    "            # Check for top_k subfolders\n",
    "            for subfolder in os.listdir(folder_path):\n",
    "                subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path) and subfolder.startswith(\"top\"):\n",
    "                    # Extract top_k number (e.g., \"top3\" ‚Üí \"3\")\n",
    "                    top_k = subfolder.replace(\"top\", \"\")\n",
    "                    output_csv = os.path.join(subfolder_path, f\"{folder_name}_top{top_k}.csv\")  # ÏàòÏ†ïÎêú Í≤ΩÎ°ú\n",
    "                    merge_qna_jsons_to_csv(subfolder_path, output_csv)\n",
    "\n",
    "merge_all_combinations_to_csv(\n",
    "    base_dir=\"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_combination_files(base_dir, include_matching=False):\n",
    "    # Collect folder CSV from {retrieval}_{qna} subfolder topK\n",
    "    rag_combination_files = []\n",
    "\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        # exclude evaluation folder\n",
    "        if not os.path.isdir(folder_path) or folder_name == \"evaluation\":\n",
    "            continue\n",
    "\n",
    "        for topk_folder in os.listdir(folder_path):\n",
    "            topk_path = os.path.join(folder_path, topk_folder)\n",
    "            if not (os.path.isdir(topk_path) and topk_folder.startswith(\"top\")):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                files = os.listdir(topk_path)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to list {topk_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Include all CSV files when include_matching is True\n",
    "            if include_matching:\n",
    "                wanted = [f for f in files if f.endswith(\".csv\")]\n",
    "            \n",
    "            # Exclude \"*matching.csv\" when include_matching is False\n",
    "            else:\n",
    "                wanted = [f for f in files if f.endswith(\".csv\") and not f.endswith(\"_matching.csv\")]\n",
    "\n",
    "            # Append (full_path, filename) to results\n",
    "            for csv_file in wanted:\n",
    "                full_path = os.path.join(topk_path, csv_file)\n",
    "                rag_combination_files.append((full_path, csv_file))\n",
    "\n",
    "    return rag_combination_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55011b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_csv_results(csv_path, gold_answers, model_name):\n",
    "    # Evaluate a QnA CSV against gold answers and write case/overall stats\n",
    "    df_results = pd.read_csv(csv_path)\n",
    "    base_dir = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample\"\n",
    "\n",
    "    # 1) Prepare model directory\n",
    "    base_model = model_name.split(\"_top\")[0] if \"_top\" in model_name else model_name.split(\"_aggregated\")[0]\n",
    "    model_dir = os.path.join(base_dir, \"evaluation\", \"individual\", base_model)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # 2) Determine file type (topk/aggregated) and maintain standard prefix\n",
    "    csv_filename = os.path.basename(csv_path)\n",
    "    if \"_aggregated\" in csv_filename or \"_aggregated\" in model_name:\n",
    "        file_type = \"aggregated\"\n",
    "    else:\n",
    "        model_topk = csv_filename.replace(\".csv\", \"\")\n",
    "        if \"_top\" in model_topk:\n",
    "            n = model_topk.split(\"_top\")[1]\n",
    "            file_type = f\"top{n}\"   \n",
    "        else:\n",
    "            file_type = \"top1\"\n",
    "\n",
    "    case_level_stats = []\n",
    "\n",
    "    print(f\"üîÑ Evaluating {len(df_results)} cases from {csv_path}\")\n",
    "\n",
    "    # 3) Evaluate each case\n",
    "    for _, row in df_results.iterrows():\n",
    "        case_id = row[\"case_id\"]\n",
    "        clean_case_id = case_id.split(\"_top\")[0] if \"_top\" in case_id else case_id\n",
    "\n",
    "        gold_result = gold_answers.get(clean_case_id)\n",
    "        if gold_result is None:\n",
    "            print(f\"‚ö†Ô∏è No gold standard found for {case_id}\")\n",
    "            continue\n",
    "\n",
    "        predicted_result = row.to_dict()\n",
    "\n",
    "        correct_count = 0\n",
    "        total_count = 0\n",
    "\n",
    "        # Only evaluate Q0-Q8 questions\n",
    "        for qid in [\"Q0\", \"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\", \"Q6\", \"Q7\", \"Q8\"]:\n",
    "            if qid not in gold_result:\n",
    "                continue\n",
    "\n",
    "            pred_answer = predicted_result.get(qid, \"\")\n",
    "            gold_answer = gold_result.get(qid, \"\")\n",
    "\n",
    "            # Normalize NaN/list/string\n",
    "            if pred_answer is None or (isinstance(pred_answer, float) and np.isnan(pred_answer)):\n",
    "                pred_answer = \"\"\n",
    "            if isinstance(pred_answer, list):\n",
    "                pred_answer = \", \".join(map(str, pred_answer))\n",
    "            if isinstance(gold_answer, list):\n",
    "                gold_answer = \", \".join(map(str, gold_answer))\n",
    "\n",
    "            pred_answer = str(pred_answer).strip()\n",
    "            gold_answer = str(gold_answer).strip()\n",
    "\n",
    "            is_correct = pred_answer == gold_answer\n",
    "            correct_count += 1 if is_correct else 0\n",
    "            total_count  += 1\n",
    "\n",
    "        accuracy = correct_count / total_count * 100 if total_count > 0 else 0.0\n",
    "\n",
    "        case_stats = {\n",
    "            \"case_id\": case_id,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"total_count\": total_count\n",
    "        }\n",
    "        case_level_stats.append(case_stats)\n",
    "        print(f\"‚úÖ {case_id}: {accuracy:.2f}% accuracy ({correct_count}/{total_count})\")\n",
    "\n",
    "    if not case_level_stats:\n",
    "        return None, None\n",
    "\n",
    "    # 4) Save case-level statistics\n",
    "    case_df = pd.DataFrame(case_level_stats)\n",
    "    if file_type.startswith(\"top\"):\n",
    "        n = file_type.replace(\"top\", \"\")\n",
    "        legacy_case = os.path.join(model_dir, f\"{n}_case_level.csv\")\n",
    "        if n.isdigit() and os.path.exists(legacy_case):\n",
    "            os.remove(legacy_case)\n",
    "\n",
    "    case_csv_path = os.path.join(model_dir, f\"{file_type}_case_level.csv\")\n",
    "    case_df.to_csv(case_csv_path, index=False)\n",
    "    print(f\"‚úÖ Case-level statistics saved to: {case_csv_path}\")\n",
    "\n",
    "    # 5) Overall statistics calculation\n",
    "    all_accuracies = case_df[\"accuracy\"].values\n",
    "    overall_stats = {\n",
    "        \"total_cases\": len(case_level_stats),\n",
    "        \"mean_accuracy\": all_accuracies.mean(),\n",
    "        \"std_accuracy\":  all_accuracies.std(),\n",
    "        \"min_accuracy\":  all_accuracies.min(),\n",
    "        \"max_accuracy\":  all_accuracies.max(),\n",
    "        \"median_accuracy\": np.median(all_accuracies),\n",
    "    }\n",
    "\n",
    "    # 6) (Optional) Include match rate from the same model‚ÄìTopK folder into overall\n",
    "    model_topk_noext = os.path.splitext(csv_filename)[0]  # Remove extension\n",
    "    m = re.search(r\"_top(\\d+)$\", model_topk_noext)\n",
    "    if m:\n",
    "        k = m.group(1)\n",
    "        model_folder = model_topk_noext[:m.start()]  # '..._sample'\n",
    "        matching_filename = f\"{model_topk_noext}_matching.csv\"\n",
    "        matching_stats_path = os.path.join(base_dir, model_folder, f\"top{k}\", matching_filename)\n",
    "        if os.path.exists(matching_stats_path):\n",
    "            mdf = pd.read_csv(matching_stats_path)\n",
    "\n",
    "            def _to_bool(x):\n",
    "                if isinstance(x, (int, float, bool)):\n",
    "                    return bool(x)\n",
    "                return str(x).strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"t\"}\n",
    "\n",
    "            matched_series = mdf[\"matched\"].map(_to_bool)\n",
    "            total_attempted = len(matched_series)\n",
    "            matched_count  = int(matched_series.sum())\n",
    "            match_rate_pct = (matched_count / total_attempted * 100) if total_attempted > 0 else 0.0\n",
    "\n",
    "            # Add match rate column to overall statistics\n",
    "            overall_stats.update({\n",
    "                \"policy_match_rate\": match_rate_pct,\n",
    "                \"total_attempted\": total_attempted,\n",
    "                \"matched_count\": matched_count,\n",
    "            })\n",
    "\n",
    "    overall_df = pd.DataFrame([overall_stats])\n",
    "\n",
    "    # 7) Save overall statistics (+ legacy filename cleanup)\n",
    "    if file_type.startswith(\"top\"):\n",
    "        n = file_type.replace(\"top\", \"\")\n",
    "        legacy_overall = os.path.join(model_dir, f\"{n}_overall.csv\")\n",
    "        if n.isdigit() and os.path.exists(legacy_overall):\n",
    "            os.remove(legacy_overall)\n",
    "\n",
    "    overall_csv_path = os.path.join(model_dir, f\"{file_type}_overall.csv\")\n",
    "    overall_df.to_csv(overall_csv_path, index=False)\n",
    "    print(f\"‚úÖ Overall statistics saved to: {overall_csv_path}\")\n",
    "\n",
    "    print(f\"Total cases evaluated: {overall_stats['total_cases']}\")\n",
    "    print(f\"QnA Accuracy (Mean %, Std): {overall_stats['mean_accuracy']:.2f}%, {overall_stats['std_accuracy']:.2f}%\")\n",
    "    print(f\"Min accuracy: {overall_stats['min_accuracy']:.2f}%\")\n",
    "    print(f\"Max accuracy: {overall_stats['max_accuracy']:.2f}%\")\n",
    "    print(f\"Median accuracy: {overall_stats['median_accuracy']:.2f}%\")\n",
    "\n",
    "    return case_df, overall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473725f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_folders_with_summary(base_dir, gold_answers, summary_output_csv):\n",
    "    # Evaluate all model/topK CSVs and write a per-model summary\n",
    "    summary_records = []\n",
    "\n",
    "    # 1) Prepare model directory\n",
    "    csv_files_tuples = get_rag_combination_files(base_dir, include_matching=False)\n",
    "    if not csv_files_tuples:\n",
    "        print(\"‚ö†Ô∏è No RAG combination CSV files found\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìä Evaluating {len(csv_files_tuples)} model combinations\")\n",
    "\n",
    "    # Collect case performance by model\n",
    "    model_results = {}  # {'<base_model>': [ {case_id, accuracy, ...}, ... ] }\n",
    "\n",
    "    # 2) Run evaluation for each model-topk CSV ‚Üí Accumulate cases by model\n",
    "    for csv_path, csv_file in csv_files_tuples:\n",
    "        model_topk = csv_file.replace('.csv', '')  # Example: gpt-5-mini_gpt-5-mini_sample_top1\n",
    "        print(f\"üîÑ Evaluating {model_topk}\")\n",
    "\n",
    "        case_df, overall_df = evaluate_csv_results(\n",
    "            csv_path=csv_path,\n",
    "            gold_answers=gold_answers,\n",
    "            model_name=model_topk\n",
    "        )\n",
    "\n",
    "        if case_df is not None:\n",
    "            # base_model = '..._sample'\n",
    "            base_model = model_topk.split(\"_top\")[0] if \"_top\" in model_topk else model_topk.split(\"_aggregated\")[0]\n",
    "            if base_model not in model_results:\n",
    "                model_results[base_model] = []\n",
    "            model_results[base_model].extend(case_df.to_dict('records'))\n",
    "\n",
    "    # 3) Collect matching files\n",
    "    all_files_with_matching = get_rag_combination_files(base_dir, include_matching=True)\n",
    "\n",
    "    for model_name, all_cases in model_results.items():\n",
    "        if not all_cases:\n",
    "            continue\n",
    "\n",
    "        # Collect overall statistics\n",
    "        total_cases = len(all_cases)\n",
    "        total_correct = sum(case['correct_count'] for case in all_cases)\n",
    "        total_questions = sum(case['total_count'] for case in all_cases)\n",
    "        overall_accuracy = (total_correct / total_questions * 100) if total_questions > 0 else 0.0\n",
    "        case_accuracies = [case['accuracy'] for case in all_cases]\n",
    "\n",
    "        # --- Collect model-level match rate (sum of all top-k) ---\n",
    "        # Find matching files in the model folder (topk) with pattern: '{model_name}_top{k}_matching.csv'\n",
    "        prefixes = [model_name, f\"{model_name}_sample\"]\n",
    "        matching_files_for_model = [\n",
    "            (full, name)\n",
    "            for (full, name) in all_files_with_matching\n",
    "            if name.endswith(\"_matching.csv\")\n",
    "            and any(name.startswith(f\"{p}_top\") for p in prefixes)\n",
    "        ]\n",
    "\n",
    "        total_attempted = 0\n",
    "        matched_count = 0\n",
    "        for full_path, _ in matching_files_for_model:\n",
    "            mdf = pd.read_csv(full_path)\n",
    "            matched_series = mdf[\"matched\"].map(\n",
    "                lambda x: (str(x).strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"t\"})\n",
    "                if not isinstance(x, (int, float, bool)) else bool(x)\n",
    "            )\n",
    "            total_attempted += len(matched_series)\n",
    "            matched_count  += int(matched_series.sum())\n",
    "\n",
    "        if total_attempted > 0:\n",
    "            match_rate_str = f\"{(matched_count / total_attempted * 100):.2f}%\"\n",
    "        else:\n",
    "            match_rate_str = \"N/A\"\n",
    "\n",
    "        # --- Collect summary records ---\n",
    "        summary_records.append({\n",
    "            \"Model_Name\": model_name,\n",
    "            \"Overall_Accuracy\": f\"{overall_accuracy:.2f}%\",\n",
    "            \"Case_Accuracy_Mean\": f\"{np.mean(case_accuracies):.2f}%\",\n",
    "            \"Case_Accuracy_Std\": f\"{np.std(case_accuracies):.2f}%\",\n",
    "            \"Total_Cases\": total_cases,\n",
    "            \"Total_Questions\": total_questions,\n",
    "            \"Total_Correct\": total_correct,\n",
    "            \"Policy_Match_Rate\": match_rate_str,\n",
    "            \"Total_Attempted\": total_attempted if total_attempted > 0 else \"N/A\",\n",
    "            \"Matched_Count\": matched_count if total_attempted > 0 else \"N/A\",\n",
    "        })\n",
    "\n",
    "    # 4) Save summary\n",
    "    if summary_records:\n",
    "        comparative_dir = os.path.join(base_dir, \"evaluation\", \"comparative\")\n",
    "        os.makedirs(comparative_dir, exist_ok=True)\n",
    "\n",
    "        # Use provided path if given, otherwise save to default path\n",
    "        out_path = summary_output_csv if summary_output_csv else os.path.join(comparative_dir, \"model_summary.csv\")\n",
    "\n",
    "        summary_df = pd.DataFrame(summary_records)\n",
    "        summary_df.to_csv(out_path, index=False)\n",
    "        print(f\"\\n‚úÖ Summary saved to: {out_path}\")\n",
    "        print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70764f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating 16 model combinations\n",
      "üîÑ Evaluating gpt-5-mini_gpt-5-mini_top10\n",
      "üîÑ Evaluating 10 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top10/gpt-5-mini_gpt-5-mini_top10.csv\n",
      "‚úÖ Case11124_top10: 55.56% accuracy (5/9)\n",
      "‚úÖ Case18257_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case19321_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case7376_top10: 66.67% accuracy (6/9)\n",
      "‚úÖ Case4512_top10: 66.67% accuracy (6/9)\n",
      "‚úÖ Case8051_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case9349_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case10363_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10451_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10917_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_gpt-5-mini/top10_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_gpt-5-mini/top10_overall.csv\n",
      "Total cases evaluated: 10\n",
      "QnA Accuracy (Mean %, Std): 76.67%, 10.48%\n",
      "Min accuracy: 55.56%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 77.78%\n",
      "üîÑ Evaluating gpt-5-mini_gpt-5-mini_top1\n",
      "üîÑ Evaluating 2 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top1/gpt-5-mini_gpt-5-mini_top1.csv\n",
      "‚úÖ Case11124_top1: 55.56% accuracy (5/9)\n",
      "‚úÖ Case19321_top1: 88.89% accuracy (8/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_gpt-5-mini/top1_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_gpt-5-mini/top1_overall.csv\n",
      "Total cases evaluated: 2\n",
      "QnA Accuracy (Mean %, Std): 72.22%, 16.67%\n",
      "Min accuracy: 55.56%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 72.22%\n",
      "üîÑ Evaluating gpt-5-mini_gpt-5-mini_top3\n",
      "üîÑ Evaluating 6 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top3/gpt-5-mini_gpt-5-mini_top3.csv\n",
      "‚úÖ Case19321_top3: 88.89% accuracy (8/9)\n",
      "‚úÖ Case10451_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10363_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case8051_top3: 33.33% accuracy (3/9)\n",
      "‚úÖ Case7376_top3: 55.56% accuracy (5/9)\n",
      "‚úÖ Case11124_top3: 66.67% accuracy (6/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_gpt-5-mini/top3_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_gpt-5-mini/top3_overall.csv\n",
      "Total cases evaluated: 6\n",
      "QnA Accuracy (Mean %, Std): 66.67%, 18.14%\n",
      "Min accuracy: 33.33%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 72.22%\n",
      "üîÑ Evaluating gpt-5-mini_gpt-5-mini_top5\n",
      "üîÑ Evaluating 9 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_gpt-5-mini/top5/gpt-5-mini_gpt-5-mini_top5.csv\n",
      "‚úÖ Case10363_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10917_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10451_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case4512_top5: 88.89% accuracy (8/9)\n",
      "‚úÖ Case7376_top5: 55.56% accuracy (5/9)\n",
      "‚úÖ Case19321_top5: 88.89% accuracy (8/9)\n",
      "‚úÖ Case8051_top5: 66.67% accuracy (6/9)\n",
      "‚úÖ Case9349_top5: 88.89% accuracy (8/9)\n",
      "‚úÖ Case11124_top5: 55.56% accuracy (5/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_gpt-5-mini/top5_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_gpt-5-mini/top5_overall.csv\n",
      "Total cases evaluated: 9\n",
      "QnA Accuracy (Mean %, Std): 75.31%, 12.59%\n",
      "Min accuracy: 55.56%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 77.78%\n",
      "üîÑ Evaluating perplexity_gpt-5-mini_top10\n",
      "üîÑ Evaluating 10 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top10/perplexity_gpt-5-mini_top10.csv\n",
      "‚úÖ Case11124_top10: 55.56% accuracy (5/9)\n",
      "‚úÖ Case18257_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case19321_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case7376_top10: 55.56% accuracy (5/9)\n",
      "‚úÖ Case4512_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case8051_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case9349_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10363_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10451_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10917_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_gpt-5-mini/top10_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_gpt-5-mini/top10_overall.csv\n",
      "Total cases evaluated: 10\n",
      "QnA Accuracy (Mean %, Std): 76.67%, 11.60%\n",
      "Min accuracy: 55.56%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 77.78%\n",
      "üîÑ Evaluating perplexity_gpt-5-mini_top1\n",
      "üîÑ Evaluating 2 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top1/perplexity_gpt-5-mini_top1.csv\n",
      "‚úÖ Case11124_top1: 55.56% accuracy (5/9)\n",
      "‚úÖ Case19321_top1: 77.78% accuracy (7/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_gpt-5-mini/top1_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_gpt-5-mini/top1_overall.csv\n",
      "Total cases evaluated: 2\n",
      "QnA Accuracy (Mean %, Std): 66.67%, 11.11%\n",
      "Min accuracy: 55.56%\n",
      "Max accuracy: 77.78%\n",
      "Median accuracy: 66.67%\n",
      "üîÑ Evaluating perplexity_gpt-5-mini_top3\n",
      "üîÑ Evaluating 6 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top3/perplexity_gpt-5-mini_top3.csv\n",
      "‚úÖ Case19321_top3: 88.89% accuracy (8/9)\n",
      "‚úÖ Case10451_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10363_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case8051_top3: 66.67% accuracy (6/9)\n",
      "‚úÖ Case7376_top3: 66.67% accuracy (6/9)\n",
      "‚úÖ Case11124_top3: 55.56% accuracy (5/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_gpt-5-mini/top3_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_gpt-5-mini/top3_overall.csv\n",
      "Total cases evaluated: 6\n",
      "QnA Accuracy (Mean %, Std): 72.22%, 10.64%\n",
      "Min accuracy: 55.56%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 72.22%\n",
      "üîÑ Evaluating perplexity_gpt-5-mini_top5\n",
      "üîÑ Evaluating 9 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_gpt-5-mini/top5/perplexity_gpt-5-mini_top5.csv\n",
      "‚úÖ Case10363_top5: 55.56% accuracy (5/9)\n",
      "‚úÖ Case10917_top5: 66.67% accuracy (6/9)\n",
      "‚úÖ Case10451_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case4512_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case7376_top5: 55.56% accuracy (5/9)\n",
      "‚úÖ Case19321_top5: 88.89% accuracy (8/9)\n",
      "‚úÖ Case8051_top5: 88.89% accuracy (8/9)\n",
      "‚úÖ Case9349_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case11124_top5: 55.56% accuracy (5/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_gpt-5-mini/top5_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_gpt-5-mini/top5_overall.csv\n",
      "Total cases evaluated: 9\n",
      "QnA Accuracy (Mean %, Std): 71.60%, 12.95%\n",
      "Min accuracy: 55.56%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 77.78%\n",
      "üîÑ Evaluating gpt-5-mini_perplexity_top10\n",
      "üîÑ Evaluating 10 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top10/gpt-5-mini_perplexity_top10.csv\n",
      "‚úÖ Case11124_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case18257_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case19321_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case7376_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case4512_top10: 44.44% accuracy (4/9)\n",
      "‚úÖ Case8051_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case9349_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10363_top10: 66.67% accuracy (6/9)\n",
      "‚úÖ Case10451_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case10917_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_perplexity/top10_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_perplexity/top10_overall.csv\n",
      "Total cases evaluated: 10\n",
      "QnA Accuracy (Mean %, Std): 78.89%, 13.56%\n",
      "Min accuracy: 44.44%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 83.33%\n",
      "üîÑ Evaluating gpt-5-mini_perplexity_top1\n",
      "üîÑ Evaluating 2 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top1/gpt-5-mini_perplexity_top1.csv\n",
      "‚úÖ Case11124_top1: 77.78% accuracy (7/9)\n",
      "‚úÖ Case19321_top1: 88.89% accuracy (8/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_perplexity/top1_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_perplexity/top1_overall.csv\n",
      "Total cases evaluated: 2\n",
      "QnA Accuracy (Mean %, Std): 83.33%, 5.56%\n",
      "Min accuracy: 77.78%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 83.33%\n",
      "üîÑ Evaluating gpt-5-mini_perplexity_top3\n",
      "üîÑ Evaluating 6 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top3/gpt-5-mini_perplexity_top3.csv\n",
      "‚úÖ Case19321_top3: 88.89% accuracy (8/9)\n",
      "‚úÖ Case10451_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10363_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case8051_top3: 88.89% accuracy (8/9)\n",
      "‚úÖ Case7376_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case11124_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_perplexity/top3_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_perplexity/top3_overall.csv\n",
      "Total cases evaluated: 6\n",
      "QnA Accuracy (Mean %, Std): 81.48%, 5.24%\n",
      "Min accuracy: 77.78%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 77.78%\n",
      "üîÑ Evaluating gpt-5-mini_perplexity_top5\n",
      "üîÑ Evaluating 9 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/gpt-5-mini_perplexity/top5/gpt-5-mini_perplexity_top5.csv\n",
      "‚úÖ Case10363_top5: 66.67% accuracy (6/9)\n",
      "‚úÖ Case10917_top5: 88.89% accuracy (8/9)\n",
      "‚úÖ Case10451_top5: 66.67% accuracy (6/9)\n",
      "‚úÖ Case4512_top5: 55.56% accuracy (5/9)\n",
      "‚úÖ Case7376_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case19321_top5: 88.89% accuracy (8/9)\n",
      "‚úÖ Case8051_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case9349_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case11124_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_perplexity/top5_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/gpt-5-mini_perplexity/top5_overall.csv\n",
      "Total cases evaluated: 9\n",
      "QnA Accuracy (Mean %, Std): 75.31%, 10.18%\n",
      "Min accuracy: 55.56%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 77.78%\n",
      "üîÑ Evaluating perplexity_perplexity_top10\n",
      "üîÑ Evaluating 10 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top10/perplexity_perplexity_top10.csv\n",
      "‚úÖ Case11124_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case18257_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case19321_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case7376_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case4512_top10: 44.44% accuracy (4/9)\n",
      "‚úÖ Case8051_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case9349_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10363_top10: 66.67% accuracy (6/9)\n",
      "‚úÖ Case10451_top10: 77.78% accuracy (7/9)\n",
      "‚úÖ Case10917_top10: 88.89% accuracy (8/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_perplexity/top10_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_perplexity/top10_overall.csv\n",
      "Total cases evaluated: 10\n",
      "QnA Accuracy (Mean %, Std): 76.67%, 12.62%\n",
      "Min accuracy: 44.44%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 77.78%\n",
      "üîÑ Evaluating perplexity_perplexity_top1\n",
      "üîÑ Evaluating 2 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top1/perplexity_perplexity_top1.csv\n",
      "‚úÖ Case11124_top1: 77.78% accuracy (7/9)\n",
      "‚úÖ Case19321_top1: 88.89% accuracy (8/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_perplexity/top1_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_perplexity/top1_overall.csv\n",
      "Total cases evaluated: 2\n",
      "QnA Accuracy (Mean %, Std): 83.33%, 5.56%\n",
      "Min accuracy: 77.78%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 83.33%\n",
      "üîÑ Evaluating perplexity_perplexity_top3\n",
      "üîÑ Evaluating 6 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top3/perplexity_perplexity_top3.csv\n",
      "‚úÖ Case19321_top3: 88.89% accuracy (8/9)\n",
      "‚úÖ Case10451_top3: 66.67% accuracy (6/9)\n",
      "‚úÖ Case10363_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case8051_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case7376_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case11124_top3: 77.78% accuracy (7/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_perplexity/top3_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_perplexity/top3_overall.csv\n",
      "Total cases evaluated: 6\n",
      "QnA Accuracy (Mean %, Std): 77.78%, 6.42%\n",
      "Min accuracy: 66.67%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 77.78%\n",
      "üîÑ Evaluating perplexity_perplexity_top5\n",
      "üîÑ Evaluating 9 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/perplexity_perplexity/top5/perplexity_perplexity_top5.csv\n",
      "‚úÖ Case10363_top5: 66.67% accuracy (6/9)\n",
      "‚úÖ Case10917_top5: 88.89% accuracy (8/9)\n",
      "‚úÖ Case10451_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case4512_top5: 55.56% accuracy (5/9)\n",
      "‚úÖ Case7376_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case19321_top5: 88.89% accuracy (8/9)\n",
      "‚úÖ Case8051_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case9349_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case11124_top5: 77.78% accuracy (7/9)\n",
      "‚úÖ Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_perplexity/top5_case_level.csv\n",
      "‚úÖ Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/individual/perplexity_perplexity/top5_overall.csv\n",
      "Total cases evaluated: 9\n",
      "QnA Accuracy (Mean %, Std): 76.54%, 9.72%\n",
      "Min accuracy: 55.56%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 77.78%\n",
      "\n",
      "‚úÖ Summary saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/comparative/model_summary.csv\n",
      "              Model_Name Overall_Accuracy Case_Accuracy_Mean  \\\n",
      "0  gpt-5-mini_gpt-5-mini           73.66%             73.66%   \n",
      "1  perplexity_gpt-5-mini           73.25%             73.25%   \n",
      "2  gpt-5-mini_perplexity           78.60%             78.60%   \n",
      "3  perplexity_perplexity           77.37%             77.37%   \n",
      "\n",
      "  Case_Accuracy_Std  Total_Cases  Total_Questions  Total_Correct  \\\n",
      "0            14.23%           27              243            179   \n",
      "1            12.19%           27              243            178   \n",
      "2            10.87%           27              243            191   \n",
      "3            10.25%           27              243            188   \n",
      "\n",
      "  Policy_Match_Rate  Total_Attempted  Matched_Count  \n",
      "0            67.50%               40             27  \n",
      "1            67.50%               40             27  \n",
      "2            67.50%               40             27  \n",
      "3            67.50%               40             27  \n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample\"\n",
    "ground_truth_path = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/sample_ground_truth.json\"\n",
    "with open(ground_truth_path, \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "evaluate_all_folders_with_summary(\n",
    "    base_dir=base_dir,\n",
    "    gold_answers=ground_truth,\n",
    "    summary_output_csv=\"\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Collecting all raw results from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample\n",
      "‚úÖ Combined statistics saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/combined/all_models_combined_statistics.csv\n",
      "‚úÖ Matching details saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/combined/all_models_matching_details.csv\n",
      "\n",
      "üéâ Combined evaluation completed!\n",
      "Results saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/combined\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/combined/all_models_combined_statistics.csv',\n",
       " '/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/combined/all_models_matching_details.csv')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_all_models_combined(base_dir, gold_answers):\n",
    "    # Aggregate all model/topK CSVs into a single combined statistics file\n",
    "    print(f\"üîÑ Collecting all raw results from {base_dir}\")\n",
    "\n",
    "    # 1) Collect all QnA CSV files (excluding matching.csv)\n",
    "    csv_files_tuples = get_rag_combination_files(base_dir, include_matching=False)\n",
    "    if not csv_files_tuples:\n",
    "        print(\"‚ö†Ô∏è No RAG combination CSV files found\")\n",
    "        return None, None\n",
    "\n",
    "    # Accumulation variables for summation\n",
    "    total_correct_all = 0\n",
    "    total_questions_all = 0\n",
    "    case_accuracies_all = []   # Distribution of case-level accuracy (overall)\n",
    "\n",
    "    # 2) Iterate through all QnA CSVs and accumulate global accuracy by comparing answers for Q0~Q8\n",
    "    for csv_path, csv_file in csv_files_tuples:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read {csv_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if \"case_id\" not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è 'case_id' column not found in {csv_path}. Skipped.\")\n",
    "            continue\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            case_id = row[\"case_id\"]\n",
    "            clean_case_id = case_id.split(\"_top\")[0] if \"_top\" in case_id else case_id\n",
    "\n",
    "            gold = gold_answers.get(clean_case_id)\n",
    "            if gold is None:\n",
    "                continue\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for qid in [\"Q0\",\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\"]:\n",
    "                if qid not in gold:\n",
    "                    continue\n",
    "                pred = row.get(qid, \"\")\n",
    "                ans  = gold.get(qid, \"\")\n",
    "\n",
    "                # NaN/list normalization\n",
    "                if pred is None or (isinstance(pred, float) and np.isnan(pred)):\n",
    "                    pred = \"\"\n",
    "                if isinstance(pred, list):\n",
    "                    pred = \", \".join(map(str, pred))\n",
    "                if isinstance(ans, list):\n",
    "                    ans = \", \".join(map(str, ans))\n",
    "\n",
    "                pred = str(pred).strip()\n",
    "                ans  = str(ans).strip()\n",
    "\n",
    "                total += 1\n",
    "                if pred == ans:\n",
    "                    correct += 1\n",
    "\n",
    "            if total > 0:\n",
    "                acc = correct / total * 100.0\n",
    "                case_accuracies_all.append(acc)\n",
    "                total_correct_all += correct\n",
    "                total_questions_all += total\n",
    "\n",
    "    # 3) Aggregate global accuracy statistics\n",
    "    if total_questions_all > 0 and len(case_accuracies_all) > 0:\n",
    "        overall_accuracy = total_correct_all / total_questions_all * 100.0\n",
    "        overall_accuracy_se = math.sqrt((total_correct_all / total_questions_all) * (1.0 - (total_correct_all / total_questions_all)) / total_questions_all) * 100.0\n",
    "        stats_summary = {\n",
    "            \"total_cases_evaluated\": len(case_accuracies_all),\n",
    "            \"total_questions\": int(total_questions_all),\n",
    "            \"total_correct\": int(total_correct_all),\n",
    "            \"overall_accuracy\": round(overall_accuracy, 2),\n",
    "            \"overall_accuracy_se\": round(overall_accuracy_se, 2),\n",
    "        }\n",
    "    else:\n",
    "        stats_summary = {\n",
    "            \"total_cases_evaluated\": 0,\n",
    "            \"total_questions\": 0,\n",
    "            \"total_correct\": 0,\n",
    "            \"overall_accuracy\": \"N/A\",\n",
    "            \"overall_accuracy_se\": \"N/A\",\n",
    "        }\n",
    "\n",
    "    # 4) Aggregate global Match Rate (sum of all models√ótop-k)\n",
    "    all_matching_data = []\n",
    "    for _, csv_file in csv_files_tuples:\n",
    "        match_rate, total_attempted, matched_count = get_policy_match_rate(\"\", csv_file)\n",
    "        if match_rate != \"N/A\":\n",
    "            all_matching_data.append({\n",
    "                \"model_combination\": csv_file.replace(\".csv\",\"\"),\n",
    "                \"total_attempted\": total_attempted,\n",
    "                \"matched_count\": matched_count,\n",
    "                \"match_rate\": match_rate\n",
    "            })\n",
    "\n",
    "    if all_matching_data:\n",
    "        total_attempted_all = sum(item[\"total_attempted\"] for item in all_matching_data)\n",
    "        total_matched_all   = sum(item[\"matched_count\"]   for item in all_matching_data)\n",
    "        overall_match_rate  = (total_matched_all / total_attempted_all * 100.0) if total_attempted_all > 0 else 0.0\n",
    "        stats_summary.update({\n",
    "            \"overall_match_rate\": round(overall_match_rate, 2),\n",
    "            \"overall_match_total_attempted\": int(total_attempted_all),\n",
    "            \"overall_match_total_matched\": int(total_matched_all),\n",
    "        })\n",
    "    else:\n",
    "        stats_summary.update({\n",
    "            \"overall_match_rate\": \"N/A\",\n",
    "            \"overall_match_total_attempted\": \"N/A\",\n",
    "            \"overall_match_total_matched\": \"N/A\",\n",
    "        })\n",
    "\n",
    "    # 5) Save combined statistics (combined folder, only two files)\n",
    "    combined_dir = os.path.join(base_dir, \"evaluation\", \"combined\")\n",
    "    os.makedirs(combined_dir, exist_ok=True)\n",
    "\n",
    "    combined_stats_path = os.path.join(combined_dir, \"all_models_combined_statistics.csv\")\n",
    "    pd.DataFrame([stats_summary]).to_csv(combined_stats_path, index=False)\n",
    "    print(f\"‚úÖ Combined statistics saved: {combined_stats_path}\")\n",
    "\n",
    "    if all_matching_data:\n",
    "        matching_details_path = os.path.join(combined_dir, \"all_models_matching_details.csv\")\n",
    "        pd.DataFrame(all_matching_data).to_csv(matching_details_path, index=False)\n",
    "        print(f\"‚úÖ Matching details saved: {matching_details_path}\")\n",
    "    else:\n",
    "        matching_details_path = None\n",
    "        print(\"‚ö†Ô∏è No matching details to save\")\n",
    "\n",
    "    print(\"\\nüéâ Combined evaluation completed!\")\n",
    "    print(f\"Results saved to: {combined_dir}\")\n",
    "\n",
    "    return combined_stats_path, matching_details_path\n",
    "\n",
    "evaluate_all_models_combined(\n",
    "    base_dir=base_dir,\n",
    "    gold_answers=ground_truth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e060948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Complete RAG Question Analysis\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Running Overall Analysis (All RAG Combinations Combined)\n",
      "üìã Found RAG combinations: ['gpt-5-mini_gpt-5-mini_top10', 'gpt-5-mini_gpt-5-mini_top1', 'gpt-5-mini_gpt-5-mini_top3', 'gpt-5-mini_gpt-5-mini_top5', 'perplexity_gpt-5-mini_top10', 'perplexity_gpt-5-mini_top1', 'perplexity_gpt-5-mini_top3', 'perplexity_gpt-5-mini_top5', 'gpt-5-mini_perplexity_top10', 'gpt-5-mini_perplexity_top1', 'gpt-5-mini_perplexity_top3', 'gpt-5-mini_perplexity_top5', 'perplexity_perplexity_top10', 'perplexity_perplexity_top1', 'perplexity_perplexity_top3', 'perplexity_perplexity_top5']\n",
      "‚úÖ Loaded 10 cases from gpt-5-mini_gpt-5-mini_top10.csv\n",
      "‚úÖ Loaded 2 cases from gpt-5-mini_gpt-5-mini_top1.csv\n",
      "‚úÖ Loaded 6 cases from gpt-5-mini_gpt-5-mini_top3.csv\n",
      "‚úÖ Loaded 9 cases from gpt-5-mini_gpt-5-mini_top5.csv\n",
      "‚úÖ Loaded 10 cases from perplexity_gpt-5-mini_top10.csv\n",
      "‚úÖ Loaded 2 cases from perplexity_gpt-5-mini_top1.csv\n",
      "‚úÖ Loaded 6 cases from perplexity_gpt-5-mini_top3.csv\n",
      "‚úÖ Loaded 9 cases from perplexity_gpt-5-mini_top5.csv\n",
      "‚úÖ Loaded 10 cases from gpt-5-mini_perplexity_top10.csv\n",
      "‚úÖ Loaded 2 cases from gpt-5-mini_perplexity_top1.csv\n",
      "‚úÖ Loaded 6 cases from gpt-5-mini_perplexity_top3.csv\n",
      "‚úÖ Loaded 9 cases from gpt-5-mini_perplexity_top5.csv\n",
      "‚úÖ Loaded 10 cases from perplexity_perplexity_top10.csv\n",
      "‚úÖ Loaded 2 cases from perplexity_perplexity_top1.csv\n",
      "‚úÖ Loaded 6 cases from perplexity_perplexity_top3.csv\n",
      "‚úÖ Loaded 9 cases from perplexity_perplexity_top5.csv\n",
      "üìä Total combined cases: 108 across 16 RAG combinations\n",
      "\n",
      "============================================================\n",
      "üìä OVERALL RAG QUESTION ACCURACY (All Combinations Combined)\n",
      "============================================================\n",
      "Q0: 100.000% (108/108)\n",
      "Q1: 87.037% (94/108)\n",
      "Q2: 70.370% (76/108)\n",
      "Q3: 75.000% (81/108)\n",
      "Q4: 72.222% (78/108)\n",
      "Q5: 81.481% (88/108)\n",
      "Q6: 30.556% (33/108)\n",
      "Q7: 76.852% (83/108)\n",
      "Q8: 87.963% (95/108)\n",
      "\n",
      "============================================================\n",
      "üìä RAG QUESTION DIFFICULTY RANKING\n",
      "============================================================\n",
      "Question  Overall_Accuracy_pct Correct_Total Difficulty_Level\n",
      "      Q0            100.000000       108/108             Easy\n",
      "      Q8             87.962963        95/108             Easy\n",
      "      Q1             87.037037        94/108             Easy\n",
      "      Q5             81.481481        88/108             Easy\n",
      "      Q7             76.851852        83/108           Medium\n",
      "      Q3             75.000000        81/108           Medium\n",
      "      Q4             72.222222        78/108           Medium\n",
      "      Q2             70.370370        76/108           Medium\n",
      "      Q6             30.555556        33/108             Hard\n",
      "\n",
      "‚úÖ Overall question accuracy saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/combined/overall_question_accuracy.csv\n",
      "‚úÖ RAG difficulty ranking saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/combined/experiment_summary.csv\n",
      "\n",
      "2Ô∏è‚É£ Running Combination-wise Analysis\n",
      "‚úÖ Processed gpt-5-mini_gpt-5-mini_top10: 10 cases\n",
      "‚úÖ Processed gpt-5-mini_gpt-5-mini_top1: 2 cases\n",
      "‚úÖ Processed gpt-5-mini_gpt-5-mini_top3: 6 cases\n",
      "‚úÖ Processed gpt-5-mini_gpt-5-mini_top5: 9 cases\n",
      "‚úÖ Processed perplexity_gpt-5-mini_top10: 10 cases\n",
      "‚úÖ Processed perplexity_gpt-5-mini_top1: 2 cases\n",
      "‚úÖ Processed perplexity_gpt-5-mini_top3: 6 cases\n",
      "‚úÖ Processed perplexity_gpt-5-mini_top5: 9 cases\n",
      "‚úÖ Processed gpt-5-mini_perplexity_top10: 10 cases\n",
      "‚úÖ Processed gpt-5-mini_perplexity_top1: 2 cases\n",
      "‚úÖ Processed gpt-5-mini_perplexity_top3: 6 cases\n",
      "‚úÖ Processed gpt-5-mini_perplexity_top5: 9 cases\n",
      "‚úÖ Processed perplexity_perplexity_top10: 10 cases\n",
      "‚úÖ Processed perplexity_perplexity_top1: 2 cases\n",
      "‚úÖ Processed perplexity_perplexity_top3: 6 cases\n",
      "‚úÖ Processed perplexity_perplexity_top5: 9 cases\n",
      "\n",
      "============================================================\n",
      "üìä RAG COMBINATION-WISE QUESTION ACCURACY (%, higher is better)\n",
      "============================================================\n",
      "            RAG_Combination    Q0     Q1    Q2     Q3     Q4     Q5    Q6     Q7     Q8\n",
      " gpt-5-mini_gpt-5-mini_top1 100.0 100.00 50.00 100.00  50.00 100.00  0.00 100.00  50.00\n",
      "gpt-5-mini_gpt-5-mini_top10 100.0  80.00 90.00  70.00  70.00  80.00 40.00  70.00  90.00\n",
      " gpt-5-mini_gpt-5-mini_top3 100.0  83.33 66.67  50.00  66.67  66.67 16.67  83.33  66.67\n",
      " gpt-5-mini_gpt-5-mini_top5 100.0  88.89 77.78  66.67  55.56  88.89 33.33  88.89  77.78\n",
      " gpt-5-mini_perplexity_top1 100.0 100.00 50.00 100.00 100.00 100.00  0.00 100.00 100.00\n",
      "gpt-5-mini_perplexity_top10 100.0  90.00 60.00  80.00  70.00  90.00 50.00  70.00 100.00\n",
      " gpt-5-mini_perplexity_top3 100.0 100.00 66.67  83.33 100.00  83.33 16.67  83.33 100.00\n",
      " gpt-5-mini_perplexity_top5 100.0  88.89 55.56  77.78 100.00  77.78 33.33  66.67  77.78\n",
      " perplexity_gpt-5-mini_top1 100.0  50.00 50.00 100.00  50.00 100.00  0.00 100.00  50.00\n",
      "perplexity_gpt-5-mini_top10 100.0  90.00 90.00  70.00  50.00  80.00 40.00  80.00  90.00\n",
      " perplexity_gpt-5-mini_top3 100.0  83.33 83.33  66.67  66.67  66.67 16.67  83.33  83.33\n",
      " perplexity_gpt-5-mini_top5 100.0  66.67 88.89  66.67  44.44  88.89 33.33  66.67  88.89\n",
      " perplexity_perplexity_top1 100.0 100.00 50.00 100.00 100.00 100.00  0.00 100.00 100.00\n",
      "perplexity_perplexity_top10 100.0  90.00 60.00  80.00  70.00  80.00 40.00  70.00 100.00\n",
      " perplexity_perplexity_top3 100.0 100.00 66.67  83.33 100.00  66.67 16.67  83.33  83.33\n",
      " perplexity_perplexity_top5 100.0  88.89 55.56  77.78  88.89  77.78 33.33  66.67 100.00\n",
      "\n",
      "‚úÖ Model-wise question accuracy saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/comparative/question_accuracy_by_model.csv\n",
      "‚úÖ Detailed question analysis saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation/comparative/detailed_question_analysis.json\n",
      "\n",
      "üéâ Complete RAG Question Analysis Finished!\n",
      "üìÅ All results saved under: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/sample/evaluation\n"
     ]
    }
   ],
   "source": [
    "def analyze_rag_question_accuracy_combined(base_dir, ground_truth, output_dir):\n",
    "    # Analyze overall question accuracy across all RAG combinations (combined)\n",
    "    csv_files_tuples = get_rag_combination_files(base_dir, include_matching=False)\n",
    "    if not csv_files_tuples:\n",
    "        print(\"‚ö†Ô∏è No RAG combination CSV files found\")\n",
    "        return None\n",
    "\n",
    "    csv_files = [name for _, name in csv_files_tuples]\n",
    "    print(f\"üìã Found RAG combinations: {[f.replace('.csv', '') for f in csv_files]}\")\n",
    "\n",
    "    # Load and combine data\n",
    "    all_data = []\n",
    "    question_columns = ['Q0','Q1','Q2','Q3','Q4','Q5','Q6','Q7','Q8']\n",
    "    for full_path, csv_file in csv_files_tuples:\n",
    "        try:\n",
    "            df = pd.read_csv(full_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read {full_path}: {e}\")\n",
    "            continue\n",
    "        df['rag_combination'] = csv_file.replace('.csv', '')\n",
    "        all_data.append(df)\n",
    "        print(f\"‚úÖ Loaded {len(df)} cases from {csv_file}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"‚ö†Ô∏è No valid CSVs loaded\")\n",
    "        return None\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"üìä Total combined cases: {len(combined_df)} across {len(csv_files)} RAG combinations\")\n",
    "\n",
    "    # Aggregate question-level accuracy\n",
    "    overall_stats = {}\n",
    "    for q in question_columns:\n",
    "        if q not in combined_df.columns:\n",
    "            continue\n",
    "        correct, total = 0, 0\n",
    "        for _, row in combined_df.iterrows():\n",
    "            case_id = row.get('case_id', '')\n",
    "            clean_case_id = case_id.split(\"_top\")[0] if \"_top\" in str(case_id) else case_id\n",
    "            gold = ground_truth.get(clean_case_id)\n",
    "            if gold is None or q not in gold:\n",
    "                continue\n",
    "\n",
    "            pred = row.get(q, \"\")\n",
    "            ans  = gold.get(q, \"\")\n",
    "\n",
    "            if pred is None or (isinstance(pred, float) and np.isnan(pred)):\n",
    "                pred = \"\"\n",
    "            if isinstance(pred, list):\n",
    "                pred = \", \".join(map(str, pred))\n",
    "            if isinstance(ans, list):\n",
    "                ans = \", \".join(map(str, ans))\n",
    "            pred = str(pred).strip()\n",
    "            ans  = str(ans).strip()\n",
    "\n",
    "            total += 1\n",
    "            if pred == ans:\n",
    "                correct += 1\n",
    "\n",
    "        acc = (correct / total * 100.0) if total > 0 else 0.0\n",
    "        overall_stats[q] = {\"accuracy_pct\": acc, \"correct\": correct, \"total\": total}\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä OVERALL RAG QUESTION ACCURACY (All Combinations Combined)\")\n",
    "    print(\"=\"*60)\n",
    "    for q in question_columns:\n",
    "        if q in overall_stats:\n",
    "            s = overall_stats[q]\n",
    "            print(f\"{q}: {s['accuracy_pct']:.3f}% ({s['correct']}/{s['total']})\")\n",
    "\n",
    "    difficulty_ranking = sorted(overall_stats.items(), key=lambda x: x[1]['accuracy_pct'], reverse=True)\n",
    "    difficulty_df = pd.DataFrame([{\n",
    "        'Question': q,\n",
    "        'Overall_Accuracy_pct': stats['accuracy_pct'],\n",
    "        'Correct_Total': f\"{stats['correct']}/{stats['total']}\",\n",
    "        'Difficulty_Level': 'Easy' if stats['accuracy_pct'] > 80\n",
    "                             else 'Medium' if stats['accuracy_pct'] > 50 else 'Hard'\n",
    "    } for q, stats in difficulty_ranking])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä RAG QUESTION DIFFICULTY RANKING\")\n",
    "    print(\"=\"*60)\n",
    "    if not difficulty_df.empty:\n",
    "        print(difficulty_df.to_string(index=False))\n",
    "\n",
    "    combined_dir = os.path.join(base_dir, \"evaluation\", \"combined\")\n",
    "    os.makedirs(combined_dir, exist_ok=True)\n",
    "\n",
    "    overall_df = pd.DataFrame([{\n",
    "        'Question': q,\n",
    "        'Accuracy_pct': s['accuracy_pct'],\n",
    "        'Correct': s['correct'],\n",
    "        'Total': s['total']\n",
    "    } for q, s in overall_stats.items()])\n",
    "\n",
    "    overall_out = os.path.join(combined_dir, \"overall_question_accuracy.csv\")\n",
    "    overall_df.to_csv(overall_out, index=False)\n",
    "    print(f\"\\n‚úÖ Overall question accuracy saved: {overall_out}\")\n",
    "\n",
    "    difficulty_out = os.path.join(combined_dir, \"experiment_summary.csv\")\n",
    "    difficulty_df.to_csv(difficulty_out, index=False)\n",
    "    print(f\"‚úÖ RAG difficulty ranking saved: {difficulty_out}\")\n",
    "\n",
    "    return overall_stats, difficulty_df\n",
    "\n",
    "def analyze_rag_question_accuracy_by_combination(base_dir, ground_truth, output_dir):\n",
    "    # Analyze question accuracy for each RAG combination separately\n",
    "    csv_files_tuples = get_rag_combination_files(base_dir, include_matching=False)\n",
    "    if not csv_files_tuples:\n",
    "        print(\"‚ö†Ô∏è No RAG combination CSV files found\")\n",
    "        return None\n",
    "\n",
    "    all_results = {}\n",
    "    question_columns = ['Q0','Q1','Q2','Q3','Q4','Q5','Q6','Q7','Q8']\n",
    "\n",
    "    for full_path, csv_file in csv_files_tuples:\n",
    "        rag_comb = csv_file.replace('.csv', '')\n",
    "        try:\n",
    "            df = pd.read_csv(full_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read {full_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        combination_acc = {}\n",
    "        for q in question_columns:\n",
    "            if q not in df.columns:\n",
    "                continue\n",
    "            correct, total = 0, 0\n",
    "            for _, row in df.iterrows():\n",
    "                case_id = row.get('case_id', '')\n",
    "                clean_case_id = case_id.split(\"_top\")[0] if \"_top\" in str(case_id) else case_id\n",
    "                gold = ground_truth.get(clean_case_id)\n",
    "                if gold is None or q not in gold:\n",
    "                    continue\n",
    "\n",
    "                pred = row.get(q, \"\")\n",
    "                ans  = gold.get(q, \"\")\n",
    "\n",
    "                if pred is None or (isinstance(pred, float) and np.isnan(pred)):\n",
    "                    pred = \"\"\n",
    "                if isinstance(pred, list):\n",
    "                    pred = \", \".join(map(str, pred))\n",
    "                if isinstance(ans, list):\n",
    "                    ans = \", \".join(map(str, ans))\n",
    "                pred = str(pred).strip()\n",
    "                ans  = str(ans).strip()\n",
    "\n",
    "                total += 1\n",
    "                if pred == ans:\n",
    "                    correct += 1\n",
    "\n",
    "            acc = (correct / total * 100.0) if total > 0 else 0.0\n",
    "            combination_acc[q] = {\"accuracy_pct\": acc, \"correct\": correct, \"total\": total}\n",
    "\n",
    "        all_results[rag_comb] = combination_acc\n",
    "        print(f\"‚úÖ Processed {rag_comb}: {len(df)} cases\")\n",
    "\n",
    "    rows = []\n",
    "    for comb_name in sorted(all_results.keys()):\n",
    "        row = {'RAG_Combination': comb_name}\n",
    "        for q in question_columns:\n",
    "            row[q] = all_results[comb_name].get(q, {}).get('accuracy_pct', 0.0)\n",
    "        rows.append(row)\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä RAG COMBINATION-WISE QUESTION ACCURACY (%, higher is better)\")\n",
    "    print(\"=\"*60)\n",
    "    if not results_df.empty:\n",
    "        print(results_df.round(2).to_string(index=False))\n",
    "\n",
    "    comparative_dir = os.path.join(base_dir, \"evaluation\", \"comparative\")\n",
    "    os.makedirs(comparative_dir, exist_ok=True)\n",
    "\n",
    "    combination_out = os.path.join(comparative_dir, \"question_accuracy_by_model.csv\")\n",
    "    results_df.to_csv(combination_out, index=False)\n",
    "    print(f\"\\n‚úÖ Model-wise question accuracy saved: {combination_out}\")\n",
    "\n",
    "    detailed_out = os.path.join(comparative_dir, \"detailed_question_analysis.json\")\n",
    "    with open(detailed_out, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Detailed question analysis saved: {detailed_out}\")\n",
    "\n",
    "    return all_results, results_df\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "def run_complete_rag_question_analysis(base_dir, ground_truth, output_dir):\n",
    "    # Run complete RAG question analysis - both combined and by combination\n",
    "    print(\"üöÄ Starting Complete RAG Question Analysis\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1) Overall Analysis\n",
    "    print(\"\\n1Ô∏è‚É£ Running Overall Analysis (All RAG Combinations Combined)\")\n",
    "    overall_stats, difficulty_df = analyze_rag_question_accuracy_combined(\n",
    "        base_dir=base_dir,\n",
    "        ground_truth=ground_truth,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    # 2) Combination-wise Analysis\n",
    "    print(\"\\n2Ô∏è‚É£ Running Combination-wise Analysis\")\n",
    "    detailed_results, combination_df = analyze_rag_question_accuracy_by_combination(\n",
    "        base_dir=base_dir,\n",
    "        ground_truth=ground_truth,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    print(\"\\nüéâ Complete RAG Question Analysis Finished!\")\n",
    "    print(f\"üìÅ All results saved under: {os.path.join(base_dir, 'evaluation')}\")\n",
    "    return overall_stats, difficulty_df, detailed_results, combination_df\n",
    "\n",
    "# Run analysis\n",
    "overall_stats, difficulty_df, detailed_results, combination_df = run_complete_rag_question_analysis(\n",
    "    base_dir=base_dir,\n",
    "    ground_truth=ground_truth,\n",
    "    output_dir=\"\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
