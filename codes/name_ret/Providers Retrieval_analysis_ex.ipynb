{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ae8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6a035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIClient:\n",
    "    def __init__(self, api_key, model=\"gpt-4o\"):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1  # ÏùºÍ¥ÄÏÑ±ÏùÑ ÏúÑÌï¥ ÎÇÆÏùÄ temperature\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"OpenAI API error: {str(e)}\")\n",
    "\n",
    "# ÏÇ¨Ïö© Î∞©Î≤ï\n",
    "path = '/home/cptaswadu/RESCUE-n8n/insurance'\n",
    "load_dotenv(dotenv_path=os.path.join(path, \".env\"))\n",
    "openai_api_key = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "gpt_client = OpenAI(api_key=openai_api_key)\n",
    "model_client = OpenAIClient(api_key=openai_api_key, model=\"gpt-4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075f7d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Providers</th>\n",
       "      <th>Source</th>\n",
       "      <th>Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aarkansas Medicaid (FFS)</td>\n",
       "      <td>X</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aetna</td>\n",
       "      <td>html</td>\n",
       "      <td>https://www.aetna.com/cpb/medical/data/100_199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aetna | Texas Health</td>\n",
       "      <td>X</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aetna Better Health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aetna Better Health Illinois</td>\n",
       "      <td>X</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Providers Source  \\\n",
       "0      Aarkansas Medicaid (FFS)      X   \n",
       "1                         Aetna   html   \n",
       "2          Aetna | Texas Health      X   \n",
       "3           Aetna Better Health    NaN   \n",
       "4  Aetna Better Health Illinois      X   \n",
       "\n",
       "                                               Links  \n",
       "0                                                NaN  \n",
       "1  https://www.aetna.com/cpb/medical/data/100_199...  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/In-Network_Providers_Update.csv')\n",
    "ground_truth = df[\"Providers\"].dropna().str.strip().tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32380869",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_gpt1 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/ChatGPT_baseline_experiment1/provider.csv')\n",
    "baseline_gpt2 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/ChatGPT_baseline_experiment2/provider.csv')\n",
    "baseline_gpt3 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/ChatGPT_baseline_experiment3/provider.csv')\n",
    "explicit_gpt1 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/ChatGPT_explicit_source_experiment1/provider.csv')\n",
    "explicit_gpt2 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/ChatGPT_explicit_source_experiment2/provider.csv')\n",
    "explicit_gpt3 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/ChatGPT_explicit_source_experiment3/provider.csv')\n",
    "\n",
    "baseline_perplexity1 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/perplexity_baseline_experiment1/provider.csv')\n",
    "baseline_perplexity2 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/perplexity_baseline_experiment2/provider.csv')\n",
    "baseline_perplexity3 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/perplexity_baseline_experiment3/provider.csv')\n",
    "explicit_perplexity1 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/perplexity_explicit_source_experiment1/provider.csv')\n",
    "explicit_perplexity2 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/perplexity_explicit_source_experiment2/provider.csv')\n",
    "explicit_perplexity3 = pd.read_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/perplexity_explicit_source_experiment3/provider.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "107d7e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184.0 94.33333333333333 7.666666666666667 8.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([len(baseline_gpt1), len(baseline_gpt2), len(baseline_gpt3)]), np.mean([len(explicit_gpt1), len(explicit_gpt2), len(explicit_gpt3)]),np.mean([len(baseline_perplexity1), len(baseline_perplexity2), len(baseline_perplexity3)]), np.mean([len(explicit_perplexity1), len(explicit_perplexity2), len(explicit_perplexity3)]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded9d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_provider_name(name):\n",
    "    name = name.strip()\n",
    "    \n",
    "    # 1. ÌäπÏàòÎ¨∏Ïûê Ï†ïÍ∑úÌôî (LLM Î£∞ Î∞òÏòÅ)\n",
    "    name = re.sub(r'[‚Äì‚Äî]', '-', name)  # em-dash, en-dash ‚Üí hyphen\n",
    "    name = re.sub(r'\\s+', ' ', name)   # Îã§Ï§ë Í≥µÎ∞± ‚Üí Îã®Ïùº Í≥µÎ∞±\n",
    "    \n",
    "    # 2. Oscar Ï†ïÍ∑úÌôî (LLM Î£∞ Î∞òÏòÅ)\n",
    "    if name.lower().strip() == \"oscar\":\n",
    "        name = \"Oscar Health Insurance\"\n",
    "    \n",
    "    # 3. Fix garbled characters\n",
    "    name = re.sub(r\"\\?\\?\\s*Medicaid\", \"- Medicaid\", name, flags=re.IGNORECASE)\n",
    "\n",
    "    # 4. Kansas City ‚Üí Kansas\n",
    "    name = name.replace(\"Kansas City\", \"Kansas\")\n",
    "\n",
    "    # 5. Wellpoint (Amerigroup ...) ‚Üí Wellpoint\n",
    "    if re.search(r\"Wellpoint\\s*\\(Amerigroup.*?\\)\", name, flags=re.IGNORECASE):\n",
    "        name = \"Wellpoint\"\n",
    "\n",
    "    # 6. Wellpoint (XX) ‚Üí Wellpoint XX\n",
    "    match = re.match(r\"Wellpoint\\s*\\((\\w{2})\\)\", name)\n",
    "    if match:\n",
    "        name = f\"Wellpoint {match.group(1)}\"\n",
    "\n",
    "    # 7. State abbreviation mapping\n",
    "    state_abbrev = {\n",
    "        \"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\", \"California\": \"CA\",\n",
    "        \"Colorado\": \"CO\", \"Connecticut\": \"CT\", \"Delaware\": \"DE\", \"Florida\": \"FL\", \"Georgia\": \"GA\",\n",
    "        \"Hawaii\": \"HI\", \"Idaho\": \"ID\", \"Illinois\": \"IL\", \"Indiana\": \"IN\", \"Iowa\": \"IA\", \"Kansas\": \"KS\",\n",
    "        \"Kentucky\": \"KY\", \"Louisiana\": \"LA\", \"Maine\": \"ME\", \"Maryland\": \"MD\", \"Massachusetts\": \"MA\",\n",
    "        \"Michigan\": \"MI\", \"Minnesota\": \"MN\", \"Mississippi\": \"MS\", \"Missouri\": \"MO\", \"Montana\": \"MT\",\n",
    "        \"Nebraska\": \"NE\", \"Nevada\": \"NV\", \"New Hampshire\": \"NH\", \"New Jersey\": \"NJ\", \"New Mexico\": \"NM\",\n",
    "        \"New York\": \"NY\", \"North Carolina\": \"NC\", \"North Dakota\": \"ND\", \"Ohio\": \"OH\", \"Oklahoma\": \"OK\",\n",
    "        \"Oregon\": \"OR\", \"Pennsylvania\": \"PA\", \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\",\n",
    "        \"South Dakota\": \"SD\", \"Tennessee\": \"TN\", \"Texas\": \"TX\", \"Utah\": \"UT\", \"Vermont\": \"VT\",\n",
    "        \"Virginia\": \"VA\", \"Washington\": \"WA\", \"West Virginia\": \"WV\", \"Wisconsin\": \"WI\", \"Wyoming\": \"WY\"\n",
    "    }\n",
    "\n",
    "    # 8. These keep the full state name - check first\n",
    "    preserve_state_full = [\n",
    "        \"Amerigroup\", \"Anthem BCBS\", \"BCBS\", \"Blue Cross\", \"CareSource\", \"Healthy Blue\", \"Molina Healthcare\"\n",
    "    ]\n",
    "    \n",
    "    # Check if this should preserve full state name\n",
    "    should_preserve = False\n",
    "    for prefix in preserve_state_full:\n",
    "        if re.search(rf\"^{prefix}\\s+of\\s+[A-Za-z ]+\", name, flags=re.IGNORECASE):\n",
    "            should_preserve = True\n",
    "            break\n",
    "    \n",
    "    if not should_preserve:\n",
    "        # 9. AmeriHealth Caritas State ‚Üí AmeriHealth Caritas XX\n",
    "        for state, abbr in state_abbrev.items():\n",
    "            if re.fullmatch(rf\"AmeriHealth Caritas {state}\", name, flags=re.IGNORECASE):\n",
    "                name = f\"AmeriHealth Caritas {abbr}\"\n",
    "                break\n",
    "            if re.fullmatch(rf\"Wellpoint {state}\", name, flags=re.IGNORECASE):\n",
    "                name = f\"Wellpoint {abbr}\"\n",
    "                break\n",
    "\n",
    "        for state, abbr in state_abbrev.items():\n",
    "            # BCBS California ‚Üí BCBS CA\n",
    "            if re.fullmatch(rf\"BCBS {state}\", name, flags=re.IGNORECASE):\n",
    "                name = f\"BCBS {abbr}\"\n",
    "                break\n",
    "            # BCBS (CA) ‚Üí BCBS CA  \n",
    "            if re.fullmatch(rf\"BCBS \\({abbr}\\)\", name, flags=re.IGNORECASE):\n",
    "                name = f\"BCBS {abbr}\"\n",
    "                break\n",
    "        \n",
    "        # 11. X of State ‚Üí X (for non-preserved cases)\n",
    "        name = re.sub(r\"\\bof\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b\", \"\", name)\n",
    "\n",
    "    # 12. Clean-up\n",
    "    name = name.replace(\"&\", \"and\")\n",
    "    name = re.sub(r\"\\s+\", \" \", name)  # Îã§Ïãú ÌïúÎ≤à Í≥µÎ∞± Ï†ïÎ¶¨\n",
    "    name = re.sub(r\"\\s*\\(.*?\\)\", \"\", name)  # Í¥ÑÌò∏ ÎÇ¥Ïö© Ï†úÍ±∞\n",
    "\n",
    "    return name.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ace324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_provider_metrics_multi_level(predicted, ground_truth, llm_evaluator=None):\n",
    "    \"\"\"\n",
    "    Compute precision and recall at three levels:\n",
    "    1. Raw (no normalization)\n",
    "    2. Normalized\n",
    "    3. LLM-based evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    def _compute_metrics(pred_list, gt_list, level_name):\n",
    "        \"\"\"Helper function to compute metrics for a given level\"\"\"\n",
    "        if not pred_list or not gt_list:\n",
    "            return {\n",
    "                f\"{level_name}_ground_truth_count\": len(gt_list),\n",
    "                f\"{level_name}_llm_returned_count\": len(pred_list),\n",
    "                f\"{level_name}_common_count\": 0,\n",
    "                f\"{level_name}_missing_count\": len(gt_list),\n",
    "                f\"{level_name}_extra_count\": len(pred_list),\n",
    "                f\"{level_name}_Common_Providers\": [],\n",
    "                f\"{level_name}_Missing_Providers\": gt_list,\n",
    "                f\"{level_name}_Extra_Providers\": pred_list,\n",
    "                f\"{level_name}_Precision (%)\": 0.0,\n",
    "                f\"{level_name}_Recall (%)\": 0.0,\n",
    "            }\n",
    "        \n",
    "        pred_set = set(pred_list)\n",
    "        gt_set = set(gt_list)\n",
    "        common = pred_set & gt_set\n",
    "        missing = gt_set - pred_set\n",
    "        extra = pred_set - gt_set\n",
    "        \n",
    "        precision = len(common) / len(pred_set) * 100 if pred_set else 0\n",
    "        recall = len(common) / len(gt_set) * 100 if gt_set else 0\n",
    "        \n",
    "        return {\n",
    "            f\"{level_name}_ground_truth_count\": len(gt_set),\n",
    "            f\"{level_name}_llm_returned_count\": len(pred_set),\n",
    "            f\"{level_name}_common_count\": len(common),\n",
    "            f\"{level_name}_missing_count\": len(missing),\n",
    "            f\"{level_name}_extra_count\": len(extra),\n",
    "            f\"{level_name}_Common_Providers\": sorted(list(common)),\n",
    "            f\"{level_name}_Missing_Providers\": sorted(list(missing)),\n",
    "            f\"{level_name}_Extra_Providers\": sorted(list(extra)),\n",
    "            f\"{level_name}_Precision (%)\": round(precision, 2),\n",
    "            f\"{level_name}_Recall (%)\": round(recall, 2),\n",
    "        }\n",
    "    \n",
    "    def _compute_llm_metrics(predicted, ground_truth, llm_evaluator):\n",
    "        \"\"\"\n",
    "        Use LLM to evaluate provider lists\n",
    "        \"\"\"\n",
    "        if not predicted or not ground_truth:\n",
    "            \n",
    "            return {\n",
    "                \"LLM_ground_truth_count\": len(ground_truth),\n",
    "                \"LLM_llm_returned_count\": len(predicted),\n",
    "                \"LLM_common_count\": 0,\n",
    "                \"LLM_missing_count\": len(ground_truth),\n",
    "                \"LLM_extra_count\": len(predicted),\n",
    "                \"LLM_Common_Providers\": [],\n",
    "                \"LLM_Missing_Providers\": list(ground_truth),\n",
    "                \"LLM_Extra_Providers\": list(predicted),\n",
    "                \"LLM_Precision (%)\": 0.0,\n",
    "                \"LLM_Recall (%)\": 0.0,\n",
    "            }\n",
    "\n",
    "        llm_results = llm_evaluator.evaluate_provider_lists(predicted, ground_truth)\n",
    "        if \"LLM_Common_Providers\" not in llm_results:\n",
    "            llm_results[\"LLM_Common_Providers\"] = []\n",
    "            \n",
    "        return llm_results\n",
    "        \n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Raw analysis (no normalization)\n",
    "    raw_results = _compute_metrics(predicted, ground_truth, \"Raw\")\n",
    "    results.update(raw_results)\n",
    "    \n",
    "    # 2. Normalized analysis\n",
    "    normalized_predicted = [normalize_provider_name(x) for x in predicted]\n",
    "    normalized_ground_truth = [normalize_provider_name(x) for x in ground_truth]\n",
    "    normalized_results = _compute_metrics(normalized_predicted, normalized_ground_truth, \"Normalized\")\n",
    "    results.update(normalized_results)\n",
    "    \n",
    "    # 3. LLM-based evaluation\n",
    "    if llm_evaluator:\n",
    "        llm_results = _compute_llm_metrics(predicted, ground_truth, llm_evaluator)\n",
    "        results.update(llm_results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "836eed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMProviderEvaluator:\n",
    "    def __init__(self, model_client, log_csv_path=None):\n",
    "        self.model_client = model_client\n",
    "        self.log_csv_path = log_csv_path or f\"llm_evaluation_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        self._initialize_csv_log()\n",
    "\n",
    "    def _initialize_csv_log(self):\n",
    "        if not os.path.exists(self.log_csv_path):\n",
    "            with open(self.log_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\n",
    "                    'timestamp', 'ground_truth_count', 'predicted_count', \n",
    "                    'llm_raw_response', 'parsed_common_count', 'parsed_common_providers', 'parsed_missing_providers', \n",
    "                    'parsed_extra_providers', 'final_precision', 'final_recall', 'parsing_error'\n",
    "                ])\n",
    "\n",
    "    def _log_to_csv(self, ground_truth, predicted, llm_response, parsed_result, final_metrics, parsing_error=None):\n",
    "        try:\n",
    "            with open(self.log_csv_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    len(ground_truth or []), len(predicted or []),\n",
    "                    (llm_response or '').replace('\\n', '\\\\n').replace('\\r', '\\\\r'), \n",
    "                    parsed_result.get('common_count', 0),\n",
    "                    '|'.join(parsed_result.get('common', [])),\n",
    "                    '|'.join(parsed_result.get('missing', [])),\n",
    "                    '|'.join(parsed_result.get('extra', [])),\n",
    "                    final_metrics.get('LLM_Precision (%)', 0),\n",
    "                    final_metrics.get('LLM_Recall (%)', 0),\n",
    "                    str(parsing_error) if parsing_error else ''\n",
    "                ])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to log to CSV: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_provider_lists(self, predicted, ground_truth):\n",
    "        \"\"\"\n",
    "        LLM evaluates two provider lists and returns standard metrics\n",
    "        \"\"\"\n",
    "        if not predicted or not ground_truth:\n",
    "            \n",
    "            predicted = predicted or []\n",
    "            ground_truth = ground_truth or []\n",
    "            empty_result = {\n",
    "                \"LLM_ground_truth_count\": len(ground_truth),\n",
    "                \"LLM_llm_returned_count\": len(predicted),\n",
    "                \"LLM_common_count\": 0,\n",
    "                \"LLM_missing_count\": len(ground_truth),\n",
    "                \"LLM_extra_count\": len(predicted),\n",
    "                \"LLM_Common_Providers\": [],\n",
    "                \"LLM_Missing_Providers\": list(ground_truth),\n",
    "                \"LLM_Extra_Providers\": list(predicted),\n",
    "                \"LLM_Precision (%)\": 0.0,\n",
    "                \"LLM_Recall (%)\": 0.0,\n",
    "            }\n",
    "\n",
    "            self._log_to_csv(ground_truth, predicted, \"EMPTY_LISTS\", {}, empty_result)\n",
    "            return empty_result\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert evaluator tasked with comparing two healthcare provider lists to calculate precision and recall metrics.\n",
    "\n",
    "Your job is to:\n",
    "1. Compare the Predicted list (experimental results) against the Ground Truth list\n",
    "2. Identify which providers represent the same healthcare insurance entity\n",
    "3. Calculate standard evaluation metrics\n",
    "\n",
    "Matching Rules:\n",
    "- Medicaid suffix is a KEY DIFFERENTIATOR: \"Dean Health Plan\" ‚â† \"Dean Health Plan ‚Äì Medicaid\" (different entities)\n",
    "- Special characters/spacing/capitalization differences are IGNORED: \"AmeriHealth\" = \"Amerihealth\", \"Anthem ‚Äì Medicaid\" = \"Anthem - Medicaid\"\n",
    "- Company name extensions are considered same entity: \"Oscar\" = \"Oscar Health Insurance\"\n",
    "- United Healthcare plans are ALL DIFFERENT entities: \"United Healthcare\" ‚â† \"United Healthcare Community Plan\" ‚â† \"United Healthcare Community Plan (DC)\"\n",
    "- BCBS regional plans are different entities: \"BCBS Texas\" ‚â† \"BCBS Minnesota\"\n",
    "\n",
    "Examples:\n",
    "- MATCH: \"BCBS California\" ‚Üî \"BCBS CA\" ‚Üî \"BCBS (CA)\" (same region, different abbreviations)\n",
    "- MATCH: \"Oscar\" ‚Üî \"Oscar Health Insurance\"\n",
    "- MATCH: \"Anthem ‚Äì Medicaid\" ‚Üî \"Anthem - Medicaid\" (dash vs em-dash)\n",
    "- NO MATCH: \"Dean Health Plan\" ‚Üî \"Dean Health Plan ‚Äì Medicaid\" (different entities)\n",
    "- NO MATCH: \"BCBS Texas\" ‚Üî \"BCBS Minnesota\" (different regions)\n",
    "\n",
    "Step-by-step process:\n",
    "1. First, identify all exact matches between the two lists following the rules above\n",
    "2. Count matched providers (common_count)\n",
    "3. List unmatched Ground Truth providers (missing)\n",
    "4. List unmatched Predicted providers (extra)\n",
    "\n",
    "Ground Truth ({len(ground_truth)} providers):\n",
    "{chr(10).join([f\"{i+1}. {gt}\" for i, gt in enumerate(ground_truth)])}\n",
    "\n",
    "Predicted ({len(predicted)} providers):\n",
    "{chr(10).join([f\"{i+1}. {pred}\" for i, pred in enumerate(predicted)])}\n",
    "\n",
    "Please provide your analysis and results in the following format:\n",
    "Common Count: [number]\n",
    "Common Providers: [list]\n",
    "Missing Providers: [list]\n",
    "Extra Providers: [list]\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model_client.generate(prompt)\n",
    "            parsed_result = self._parse_llm_response(response)\n",
    "            \n",
    "            # Convert to standard metric format\n",
    "            precision = parsed_result['common_count'] / len(predicted) * 100 if predicted else 0\n",
    "            recall = parsed_result['common_count'] / len(ground_truth) * 100 if ground_truth else 0\n",
    "\n",
    "            final_metrics = {\n",
    "                \"LLM_ground_truth_count\": len(ground_truth),\n",
    "                \"LLM_llm_returned_count\": len(predicted),\n",
    "                \"LLM_common_count\": parsed_result['common_count'],\n",
    "                \"LLM_missing_count\": len(parsed_result['missing']),\n",
    "                \"LLM_extra_count\": len(parsed_result['extra']),\n",
    "                \"LLM_Common_Providers\": sorted(parsed_result['common']),\n",
    "                \"LLM_Missing_Providers\": sorted(parsed_result['missing']),\n",
    "                \"LLM_Extra_Providers\": sorted(parsed_result['extra']),\n",
    "                \"LLM_Precision (%)\": round(precision, 2),\n",
    "                \"LLM_Recall (%)\": round(recall, 2),\n",
    "            }\n",
    "            self._log_to_csv(ground_truth, predicted, response, parsed_result, final_metrics)\n",
    "            return final_metrics\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Fallback in case of LLM error\n",
    "            error_result = {\n",
    "                \"LLM_ground_truth_count\": len(ground_truth),\n",
    "                \"LLM_llm_returned_count\": len(predicted),\n",
    "                \"LLM_common_count\": 0,\n",
    "                \"LLM_missing_count\": len(ground_truth),\n",
    "                \"LLM_extra_count\": len(predicted),\n",
    "                \"LLM_Common_Providers\": [],\n",
    "                \"LLM_Missing_Providers\": list(ground_truth),\n",
    "                \"LLM_Extra_Providers\": list(predicted),\n",
    "                \"LLM_Precision (%)\": 0.0,\n",
    "                \"LLM_Recall (%)\": 0.0,\n",
    "                \"LLM_Error\": str(e),\n",
    "            }\n",
    "        \n",
    "            self._log_to_csv(ground_truth, predicted, f\"ERROR: {str(e)}\", {}, error_result, parsing_error=str(e))\n",
    "        return error_result\n",
    "    \n",
    "    def _parse_llm_response(self, response):\n",
    "        \"\"\"\n",
    "        Parse LLM response to extract structured data\n",
    "        \"\"\"     \n",
    "        try:\n",
    "            # Extract common count\n",
    "            common_match = re.search(r'Common Count:\\s*(\\d+)', response, re.IGNORECASE)\n",
    "            common_count = int(common_match.group(1)) if common_match else 0\n",
    "\n",
    "            # Extract common providers - Ï∂îÍ∞Ä\n",
    "            common_providers_match = re.search(r'Common Providers:\\s*\\[(.*?)\\]', response, re.DOTALL | re.IGNORECASE)\n",
    "            if common_providers_match:\n",
    "                common_text = common_providers_match.group(1).strip()\n",
    "                common_providers = [item.strip().strip('\"\\'') for item in common_text.split(',') if item.strip() and item.strip() != 'None']\n",
    "            else:\n",
    "                common_providers = []\n",
    "            \n",
    "            # Extract missing providers\n",
    "            missing_match = re.search(r'Missing Providers:\\s*\\[(.*?)\\]', response, re.DOTALL | re.IGNORECASE)\n",
    "            if missing_match:\n",
    "                missing_text = missing_match.group(1).strip()\n",
    "                missing = [item.strip().strip('\"\\'') for item in missing_text.split(',') if item.strip() and item.strip() != 'None']\n",
    "            else:\n",
    "                missing = []\n",
    "            \n",
    "            # Extract extra providers\n",
    "            extra_match = re.search(r'Extra Providers:\\s*\\[(.*?)\\]', response, re.DOTALL | re.IGNORECASE)\n",
    "            if extra_match:\n",
    "                extra_text = extra_match.group(1).strip()\n",
    "                extra = [item.strip().strip('\"\\'') for item in extra_text.split(',') if item.strip() and item.strip() != 'None']\n",
    "            else:\n",
    "                extra = []\n",
    "            \n",
    "            return {\n",
    "                'common_count': common_count,\n",
    "                'common': common_providers,\n",
    "                'missing': missing,\n",
    "                'extra': extra\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If parsing fails, return empty structure\n",
    "            return {\n",
    "                'common_count': 0,\n",
    "                'common': [],\n",
    "                'missing': [],\n",
    "                'extra': []\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33aede0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"baseline_gpt1\": baseline_gpt1,\n",
    "    \"baseline_gpt2\": baseline_gpt2,\n",
    "    \"baseline_gpt3\": baseline_gpt3,\n",
    "    \"explicit_gpt1\": explicit_gpt1,\n",
    "    \"explicit_gpt2\": explicit_gpt2,\n",
    "    \"explicit_gpt3\": explicit_gpt3,\n",
    "    \"baseline_perplexity1\": baseline_perplexity1,\n",
    "    \"baseline_perplexity2\": baseline_perplexity2,\n",
    "    \"baseline_perplexity3\": baseline_perplexity3,\n",
    "    \"explicit_perplexity1\": explicit_perplexity1,\n",
    "    \"explicit_perplexity2\": explicit_perplexity2,\n",
    "    \"explicit_perplexity3\": explicit_perplexity3,\n",
    "}\n",
    "\n",
    "experiments_gpt = {k: v for k, v in experiments.items() if 'gpt' in k.lower()}\n",
    "experiments_perplexity = {k: v for k, v in experiments.items() if 'perplexity' in k.lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d23b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM evaluator ÏÉùÏÑ±\n",
    "llm_evaluator = LLMProviderEvaluator(model_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a188ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_union_metrics(row, ground_truth):\n",
    "    \"\"\"Calculate union precision/recall from three methodologies\"\"\"\n",
    "    \n",
    "    # Extract common providers from each method (handle string/list conversion)\n",
    "    raw_common = set(row.get('Raw_Common_Providers', []) or [])\n",
    "    normalized_common = set(row.get('Normalized_Common_Providers', []) or [])\n",
    "    llm_common = set(row.get('LLM_Common_Providers', []) or [])\n",
    "    \n",
    "    # Extract extra providers from each method\n",
    "    raw_extra = set(row.get('Raw_Extra_Providers', []) or [])\n",
    "    normalized_extra = set(row.get('Normalized_Extra_Providers', []) or [])\n",
    "    llm_extra = set(row.get('LLM_Extra_Providers', []) or [])\n",
    "    \n",
    "    # Union of all common providers\n",
    "    union_common = raw_common | normalized_common | llm_common\n",
    "    \n",
    "    # Calculate union metrics\n",
    "    ground_truth_set = set(ground_truth)\n",
    "    predicted_total = (raw_common | raw_extra) | (normalized_common | normalized_extra) | (llm_common | llm_extra)\n",
    "    \n",
    "    union_precision = len(union_common) / len(predicted_total) * 100 if predicted_total else 0\n",
    "    union_recall = len(union_common) / len(ground_truth_set) * 100 if ground_truth_set else 0\n",
    "    \n",
    "    return {\n",
    "        'Union_Common_Count': len(union_common),\n",
    "        'Union_Common_Providers': sorted(list(union_common)),\n",
    "        'Union_Precision (%)': round(union_precision, 2),\n",
    "        'Union_Recall (%)': round(union_recall, 2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8944f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Raw_ground_truth_count  Raw_llm_returned_count  Raw_common_count  \\\n",
      "0                     239                     165                45   \n",
      "1                     239                     191               148   \n",
      "2                     239                     196               153   \n",
      "3                     239                     160               125   \n",
      "4                     239                      85                78   \n",
      "\n",
      "   Raw_missing_count  Raw_extra_count  \\\n",
      "0                194              120   \n",
      "1                 91               43   \n",
      "2                 86               43   \n",
      "3                114               35   \n",
      "4                161                7   \n",
      "\n",
      "                                Raw_Common_Providers  \\\n",
      "0  [Aetna, Aetna Better Health, Aetna Better Heal...   \n",
      "1  [Aetna, Aetna Better Health, Aetna | Texas Hea...   \n",
      "2  [Aetna, Aetna Better Health, Aetna | Texas Hea...   \n",
      "3  [Aetna, Allina Health | Aetna, AmeriHealth Car...   \n",
      "4  [Aetna, American Indian Health Program (AIHP),...   \n",
      "\n",
      "                               Raw_Missing_Providers  \\\n",
      "0  [Aarkansas Medicaid (FFS), Aetna Better Health...   \n",
      "1  [Aarkansas Medicaid (FFS), Aetna Better Health...   \n",
      "2  [Aarkansas Medicaid (FFS), Aetna Better Health...   \n",
      "3  [Aarkansas Medicaid (FFS), Aetna Better Health...   \n",
      "4  [Aarkansas Medicaid (FFS), Aetna Better Health...   \n",
      "\n",
      "                                 Raw_Extra_Providers  Raw_Precision (%)  \\\n",
      "0  [Absolute Total Care, Aetna Better Health Mary...              27.27   \n",
      "1  [Alameda Alliance for Health, AmeriHealth, Ame...              77.49   \n",
      "2  [Alameda Alliance for Health, AmeriHealth, Ame...              78.06   \n",
      "3  [Alameda Alliance for Health, AmeriHealth, Ant...              78.12   \n",
      "4  [Alameda Alliance for Health, Anthem ‚Äì Medicai...              91.76   \n",
      "\n",
      "   Raw_Recall (%)  ...                               LLM_Common_Providers  \\\n",
      "0           18.83  ...                                                 []   \n",
      "1           61.92  ...  [Aetna, Aetna | Texas Health, AmeriHealth Cari...   \n",
      "2           64.02  ...  [Aetna, Aetna | Texas Health, AmeriHealth Cari...   \n",
      "3           52.30  ...  [Aetna, Alameda Alliance for Health, Allina He...   \n",
      "4           32.64  ...  [Aetna, Alameda Alliance for Health, American ...   \n",
      "\n",
      "                               LLM_Missing_Providers  \\\n",
      "0                                                 []   \n",
      "1  [Aetna Better Health Illinois, Aetna Better He...   \n",
      "2  [Aetna Better Health Illinois, Aetna Better He...   \n",
      "3  [Aarkansas Medicaid (FFS), Aetna Better Health...   \n",
      "4                                    [168 providers]   \n",
      "\n",
      "                    LLM_Extra_Providers  LLM_Precision (%)  LLM_Recall (%)  \\\n",
      "0                                    []               0.00            0.00   \n",
      "1                                    []             100.00           79.92   \n",
      "2  [Amerihealth, Caresource ‚Äì Medicaid]             100.00           82.01   \n",
      "3   [BCBS Virginia, BCBS West Virginia]             100.00           66.95   \n",
      "4                        [14 providers]              83.53           29.71   \n",
      "\n",
      "      experiment Union_Common_Count  \\\n",
      "0  baseline_gpt1                 59   \n",
      "1  baseline_gpt2                277   \n",
      "2  baseline_gpt3                289   \n",
      "3  explicit_gpt1                230   \n",
      "4  explicit_gpt2                134   \n",
      "\n",
      "                              Union_Common_Providers  Union_Precision (%)  \\\n",
      "0  [Aetna, Aetna Better Health, Aetna Better Heal...                31.55   \n",
      "1  [Aetna, Aetna Better Health, Aetna | Texas Hea...                95.52   \n",
      "2  [Aetna, Aetna Better Health, Aetna | Texas Hea...                96.33   \n",
      "3  [Aetna, Alameda Alliance for Health, Allina He...                97.05   \n",
      "4  [Aetna, Alameda Alliance for Health, American ...                95.04   \n",
      "\n",
      "   Union_Recall (%)  \n",
      "0             24.69  \n",
      "1            115.90  \n",
      "2            120.92  \n",
      "3             96.23  \n",
      "4             56.07  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "‚úÖ GPT Experiments Metrics (Multi-level):\n",
      "      experiment  Raw_Precision (%)  Raw_Recall (%)  Normalized_Precision (%)  \\\n",
      "0  baseline_gpt1              27.27           18.83                     26.03   \n",
      "1  baseline_gpt2              77.49           61.92                     96.30   \n",
      "2  baseline_gpt3              78.06           64.02                     96.39   \n",
      "3  explicit_gpt1              78.12           52.30                     94.97   \n",
      "4  explicit_gpt2              91.76           32.64                     95.24   \n",
      "5  explicit_gpt3              89.47           14.23                     97.30   \n",
      "\n",
      "   Normalized_Recall (%)  LLM_Precision (%)  LLM_Recall (%)  \\\n",
      "0                  17.84               0.00            0.00   \n",
      "1                  85.45             100.00           79.92   \n",
      "2                  87.79             100.00           82.01   \n",
      "3                  70.89             100.00           66.95   \n",
      "4                  37.56              83.53           29.71   \n",
      "5                  16.90             100.00           15.90   \n",
      "\n",
      "   Union_Precision (%)  Union_Recall (%)  \n",
      "0                31.55             24.69  \n",
      "1                95.52            115.90  \n",
      "2                96.33            120.92  \n",
      "3                97.05             96.23  \n",
      "4                95.04             56.07  \n",
      "5               100.00             20.50  \n",
      "\n",
      "‚úÖ Perplexity Experiments Metrics (Multi-level):\n",
      "             experiment  Raw_Precision (%)  Raw_Recall (%)  \\\n",
      "0  baseline_perplexity1               50.0            0.84   \n",
      "1  baseline_perplexity2               40.0            2.51   \n",
      "2  baseline_perplexity3               50.0            0.84   \n",
      "3  explicit_perplexity1              100.0            3.35   \n",
      "4  explicit_perplexity2              100.0            3.35   \n",
      "5  explicit_perplexity3              100.0            3.35   \n",
      "\n",
      "   Normalized_Precision (%)  Normalized_Recall (%)  LLM_Precision (%)  \\\n",
      "0                      50.0                   0.94               50.0   \n",
      "1                      40.0                   2.82               40.0   \n",
      "2                      50.0                   0.94               50.0   \n",
      "3                     100.0                   3.76              100.0   \n",
      "4                     100.0                   3.76              100.0   \n",
      "5                     100.0                   3.76              100.0   \n",
      "\n",
      "   LLM_Recall (%)  Union_Precision (%)  Union_Recall (%)  \n",
      "0            0.84                50.00              0.84  \n",
      "1            2.51                31.58              2.51  \n",
      "2            0.84                50.00              0.84  \n",
      "3            3.35               100.00              3.77  \n",
      "4            3.35               100.00              3.77  \n",
      "5            3.35                90.00              3.77  \n"
     ]
    }
   ],
   "source": [
    "# GPT Ïã§Ìóò Í≤∞Í≥º Î∂ÑÏÑù (Multi-level)\n",
    "gpt_rows = []\n",
    "for name, df_pred in experiments_gpt.items():\n",
    "    pred_list = (\n",
    "        df_pred[\"Providers\"]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .tolist()  # normalize Ï†úÍ±∞ - multi-levelÏóêÏÑú Ï≤òÎ¶¨\n",
    "    )\n",
    "    metrics = compute_provider_metrics_multi_level(pred_list, ground_truth, llm_evaluator)\n",
    "    metrics[\"experiment\"] = name\n",
    "    gpt_rows.append(metrics)\n",
    "\n",
    "metrics_gpt_df = pd.DataFrame(gpt_rows)\n",
    "\n",
    "# Perplexity Ïã§Ìóò Í≤∞Í≥º Î∂ÑÏÑù (Multi-level)\n",
    "perp_rows = []\n",
    "for name, df_pred in experiments_perplexity.items():\n",
    "    pred_list = (\n",
    "        df_pred[\"Providers\"]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .tolist()  # normalize Ï†úÍ±∞ - multi-levelÏóêÏÑú Ï≤òÎ¶¨\n",
    "    )\n",
    "    metrics = compute_provider_metrics_multi_level(pred_list, ground_truth, llm_evaluator)\n",
    "    metrics[\"experiment\"] = name\n",
    "    perp_rows.append(metrics)\n",
    "\n",
    "metrics_perplexity_df = pd.DataFrame(perp_rows)\n",
    "\n",
    "# Union Î©îÌä∏Î¶≠Ïä§ Í≥ÑÏÇ∞ (GPT)\n",
    "union_metrics_gpt = []\n",
    "for _, row in metrics_gpt_df.iterrows():\n",
    "    union_metrics = calculate_union_metrics(row, ground_truth)\n",
    "    union_metrics_gpt.append(union_metrics)\n",
    "\n",
    "union_gpt_df = pd.DataFrame(union_metrics_gpt)\n",
    "metrics_gpt_df = pd.concat([metrics_gpt_df, union_gpt_df], axis=1)\n",
    "\n",
    "# Union Î©îÌä∏Î¶≠Ïä§ Í≥ÑÏÇ∞ (Perplexity)\n",
    "union_metrics_perp = []\n",
    "for _, row in metrics_perplexity_df.iterrows():\n",
    "    union_metrics = calculate_union_metrics(row, ground_truth)\n",
    "    union_metrics_perp.append(union_metrics)\n",
    "\n",
    "union_perp_df = pd.DataFrame(union_metrics_perp)\n",
    "metrics_perplexity_df = pd.concat([metrics_perplexity_df, union_perp_df], axis=1)\n",
    "\n",
    "# CSV ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "metrics_gpt_df.to_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/gpt_experiments_metrics.csv', index=False)\n",
    "metrics_perplexity_df.to_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/perplexity_experiments_metrics.csv', index=False)\n",
    "print(metrics_gpt_df.head())\n",
    "\n",
    "print(\"‚úÖ GPT Experiments Metrics (Multi-level):\")\n",
    "print(metrics_gpt_df[['experiment', 'Raw_Precision (%)', 'Raw_Recall (%)', \n",
    "                     'Normalized_Precision (%)', 'Normalized_Recall (%)',\n",
    "                     'LLM_Precision (%)', 'LLM_Recall (%)',\n",
    "                     'Union_Precision (%)', 'Union_Recall (%)']])\n",
    "\n",
    "print(\"\\n‚úÖ Perplexity Experiments Metrics (Multi-level):\")\n",
    "print(metrics_perplexity_df[['experiment', 'Raw_Precision (%)', 'Raw_Recall (%)', \n",
    "                            'Normalized_Precision (%)', 'Normalized_Recall (%)',\n",
    "                            'LLM_Precision (%)', 'LLM_Recall (%)',\n",
    "                            'Union_Precision (%)', 'Union_Recall (%)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83d00364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Î™®Îç∏Î≥Ñ ÌèâÍ∑† Precision / Recall (Multi-level + Union)\n",
      "                 group  Raw_Precision (%)  Raw_Recall (%)  \\\n",
      "0         baseline_gpt              60.94           48.26   \n",
      "1  baseline_perplexity              46.67            1.40   \n",
      "2         explicit_gpt              86.45           33.06   \n",
      "3  explicit_perplexity             100.00            3.35   \n",
      "\n",
      "   Normalized_Precision (%)  Normalized_Recall (%)  LLM_Precision (%)  \\\n",
      "0                     72.91                  63.69              66.67   \n",
      "1                     46.67                   1.57              46.67   \n",
      "2                     95.84                  41.78              94.51   \n",
      "3                    100.00                   3.76             100.00   \n",
      "\n",
      "   LLM_Recall (%)  Union_Precision (%)  Union_Recall (%)  \n",
      "0           53.98                74.47             87.17  \n",
      "1            1.40                43.86              1.40  \n",
      "2           37.52                97.36             57.60  \n",
      "3            3.35                96.67              3.77  \n"
     ]
    }
   ],
   "source": [
    "metrics_all_df = pd.concat([metrics_gpt_df, metrics_perplexity_df], ignore_index=True)\n",
    "metrics_all_df.to_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/all_experiments_metrics.csv', index=False)\n",
    "\n",
    "# Í∑∏Î£π Ïù¥Î¶Ñ ÎΩëÍ∏∞ (experiment Ïù¥Î¶ÑÏóêÏÑú Ïà´Ïûê Ï†úÍ±∞)\n",
    "metrics_all_df[\"group\"] = metrics_all_df[\"experiment\"].str.replace(r\"\\d+$\", \"\", regex=True)\n",
    "\n",
    "# Í∑∏Î£πÎ≥Ñ ÌèâÍ∑† Í≥ÑÏÇ∞ (4Í∞ÄÏßÄ Î†àÎ≤® - Union Ï∂îÍ∞Ä)\n",
    "grouped_means = (\n",
    "    metrics_all_df.groupby(\"group\")[[\"Raw_Precision (%)\", \"Raw_Recall (%)\",\n",
    "                                    \"Normalized_Precision (%)\", \"Normalized_Recall (%)\",\n",
    "                                    \"LLM_Precision (%)\", \"LLM_Recall (%)\",\n",
    "                                    \"Union_Precision (%)\", \"Union_Recall (%)\"]]  # Union Ï∂îÍ∞Ä\n",
    "    .mean()\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Ï∂úÎ†• Î∞è Ï†ÄÏû•\n",
    "print(\"\\nüìä Î™®Îç∏Î≥Ñ ÌèâÍ∑† Precision / Recall (Multi-level + Union)\")\n",
    "print(grouped_means)\n",
    "\n",
    "# Í∑∏Î£πÎ≥Ñ ÌèâÍ∑† Í≤∞Í≥ºÎèÑ Ï†ÄÏû•\n",
    "grouped_means.to_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/grouped_means_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39ea5513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä GPT vs Perplexity ÌèâÍ∑† ÏÑ±Îä• ÎπÑÍµê (Multi-level)\n",
      "        model  Raw_Precision (%)  Raw_Recall (%)  Normalized_Precision (%)  \\\n",
      "0         gpt              73.70           40.66                     84.37   \n",
      "1  perplexity              73.33            2.37                     73.33   \n",
      "\n",
      "   Normalized_Recall (%)  LLM_Precision (%)  LLM_Recall (%)  \\\n",
      "0                  52.74              80.59           45.75   \n",
      "1                   2.66              73.33            2.37   \n",
      "\n",
      "   Union_Precision (%)  Union_Recall (%)  \n",
      "0                85.92             72.39  \n",
      "1                70.26              2.58  \n"
     ]
    }
   ],
   "source": [
    "# Î™®Îç∏Î≥Ñ ÌèâÍ∑† Í≥ÑÏÇ∞\n",
    "metrics_all_df[\"model\"] = metrics_all_df[\"experiment\"].apply(\n",
    "    lambda x: \"gpt\" if \"gpt\" in x.lower() else \"perplexity\"\n",
    ")\n",
    "\n",
    "model_means = (\n",
    "    metrics_all_df.groupby(\"model\")[[\"Raw_Precision (%)\", \"Raw_Recall (%)\",\n",
    "                                    \"Normalized_Precision (%)\", \"Normalized_Recall (%)\",\n",
    "                                    \"LLM_Precision (%)\", \"LLM_Recall (%)\",\n",
    "                                    \"Union_Precision (%)\", \"Union_Recall (%)\"]]  # Union Ï∂îÍ∞Ä\n",
    "    .mean()\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Ï∂úÎ†• Î∞è Ï†ÄÏû•\n",
    "print(\"\\nüìä GPT vs Perplexity ÌèâÍ∑† ÏÑ±Îä• ÎπÑÍµê (Multi-level)\")\n",
    "print(model_means)\n",
    "\n",
    "model_means.to_csv('/home/cptaswadu/RESCUE-n8n/insurance/results/Providers Retrieval/model_means_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50921b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Î†àÎ≤®Î≥Ñ ÏÑ±Îä• Í∞úÏÑ† Î∂ÑÏÑù:\n",
      "\n",
      "GPT Î™®Îç∏:\n",
      "  Raw ‚Üí Normalized: Precision 73.7% ‚Üí 84.37%\n",
      "  Raw ‚Üí Normalized: Recall 40.66% ‚Üí 52.74%\n",
      "  Normalized ‚Üí LLM: Precision 84.37% ‚Üí 80.59%\n",
      "  Normalized ‚Üí LLM: Recall 52.74% ‚Üí 45.75%\n",
      "\n",
      "PERPLEXITY Î™®Îç∏:\n",
      "  Raw ‚Üí Normalized: Precision 73.33% ‚Üí 73.33%\n",
      "  Raw ‚Üí Normalized: Recall 2.37% ‚Üí 2.66%\n",
      "  Normalized ‚Üí LLM: Precision 73.33% ‚Üí 73.33%\n",
      "  Normalized ‚Üí LLM: Recall 2.66% ‚Üí 2.37%\n"
     ]
    }
   ],
   "source": [
    "# Î†àÎ≤®Î≥Ñ ÏÑ±Îä• ÎπÑÍµêÎ•º ÏúÑÌïú Ï∂îÍ∞Ä Î∂ÑÏÑù\n",
    "print(\"\\nüìà Î†àÎ≤®Î≥Ñ ÏÑ±Îä• Í∞úÏÑ† Î∂ÑÏÑù:\")\n",
    "for model in [\"gpt\", \"perplexity\"]:\n",
    "    model_data = model_means[model_means[\"model\"] == model].iloc[0]\n",
    "    print(f\"\\n{model.upper()} Î™®Îç∏:\")\n",
    "    print(f\"  Raw ‚Üí Normalized: Precision {model_data['Raw_Precision (%)']}% ‚Üí {model_data['Normalized_Precision (%)']}%\")\n",
    "    print(f\"  Raw ‚Üí Normalized: Recall {model_data['Raw_Recall (%)']}% ‚Üí {model_data['Normalized_Recall (%)']}%\")\n",
    "    print(f\"  Normalized ‚Üí LLM: Precision {model_data['Normalized_Precision (%)']}% ‚Üí {model_data['LLM_Precision (%)']}%\")\n",
    "    print(f\"  Normalized ‚Üí LLM: Recall {model_data['Normalized_Recall (%)']}% ‚Üí {model_data['LLM_Recall (%)']}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
