{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d0dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.contingency_tables import cochrans_q, mcnemar\n",
    "from itertools import combinations\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99397601",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae09f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>mode</th>\n",
       "      <th>case_type</th>\n",
       "      <th>iteration</th>\n",
       "      <th>Q0_answer</th>\n",
       "      <th>Q0_reasoning</th>\n",
       "      <th>Q1_answer</th>\n",
       "      <th>Q1_reasoning</th>\n",
       "      <th>Q2_answer</th>\n",
       "      <th>Q2_reasoning</th>\n",
       "      <th>...</th>\n",
       "      <th>adjusted_correct</th>\n",
       "      <th>adjusted_total</th>\n",
       "      <th>adjusted_accuracy</th>\n",
       "      <th>adjusted_wrong_questions</th>\n",
       "      <th>adjusted_wrong_predictions</th>\n",
       "      <th>adjusted_ground_truth</th>\n",
       "      <th>unclear_count</th>\n",
       "      <th>unclear_questions</th>\n",
       "      <th>not_answerable_count</th>\n",
       "      <th>not_answerable_questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case3223</td>\n",
       "      <td>baseline</td>\n",
       "      <td>without_policy</td>\n",
       "      <td>iter2</td>\n",
       "      <td>BRCA1/2</td>\n",
       "      <td>Request was specifically for BRCA1 and BRCA2 t...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>BCBS_FEP BRCA coverage criteria are based on p...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>BCBS_FEP policies specify requirements around ...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>Q3</td>\n",
       "      <td>Q3:Yes</td>\n",
       "      <td>Q3:No</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case14226</td>\n",
       "      <td>baseline</td>\n",
       "      <td>without_policy</td>\n",
       "      <td>iter2</td>\n",
       "      <td>BRCA1/2</td>\n",
       "      <td>The clinical note explicitly states germline B...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Cigna BRCA testing policy does not impose a sp...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>Cigna typically requires an appropriate orderi...</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>Q7</td>\n",
       "      <td>Q7:81277</td>\n",
       "      <td>Q7:81162</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case10451</td>\n",
       "      <td>baseline</td>\n",
       "      <td>without_policy</td>\n",
       "      <td>iter2</td>\n",
       "      <td>WES</td>\n",
       "      <td>The clinical note explicitly states the neurol...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>UHC coverage for WES includes pediatric patien...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>UHC policies generally require an appropriatel...</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Q4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case4196</td>\n",
       "      <td>baseline</td>\n",
       "      <td>without_policy</td>\n",
       "      <td>iter2</td>\n",
       "      <td>WES</td>\n",
       "      <td>The clinical note explicitly states whole exom...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>BCBS FEP WES coverage criteria do not impose a...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>BCBS FEP criteria typically require appropriat...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>Q3,Q4</td>\n",
       "      <td>Q3:Yes|Q4:No</td>\n",
       "      <td>Q3:No|Q4:Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case18281</td>\n",
       "      <td>baseline</td>\n",
       "      <td>without_policy</td>\n",
       "      <td>iter2</td>\n",
       "      <td>WGS</td>\n",
       "      <td>The clinical note explicitly states the cardio...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>UHC sequencing policies do not impose a strict...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>UHC policies generally require an appropriate ...</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>Q4</td>\n",
       "      <td>Q4:No</td>\n",
       "      <td>Q4:Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     case_id      mode       case_type iteration Q0_answer  \\\n",
       "0   Case3223  baseline  without_policy     iter2   BRCA1/2   \n",
       "1  Case14226  baseline  without_policy     iter2   BRCA1/2   \n",
       "2  Case10451  baseline  without_policy     iter2       WES   \n",
       "3   Case4196  baseline  without_policy     iter2       WES   \n",
       "4  Case18281  baseline  without_policy     iter2       WGS   \n",
       "\n",
       "                                        Q0_reasoning      Q1_answer  \\\n",
       "0  Request was specifically for BRCA1 and BRCA2 t...  Not Specified   \n",
       "1  The clinical note explicitly states germline B...  Not Specified   \n",
       "2  The clinical note explicitly states the neurol...            Yes   \n",
       "3  The clinical note explicitly states whole exom...  Not Specified   \n",
       "4  The clinical note explicitly states the cardio...  Not Specified   \n",
       "\n",
       "                                        Q1_reasoning      Q2_answer  \\\n",
       "0  BCBS_FEP BRCA coverage criteria are based on p...  Not Specified   \n",
       "1  Cigna BRCA testing policy does not impose a sp...  Not Specified   \n",
       "2  UHC coverage for WES includes pediatric patien...  Not Specified   \n",
       "3  BCBS FEP WES coverage criteria do not impose a...  Not Specified   \n",
       "4  UHC sequencing policies do not impose a strict...  Not Specified   \n",
       "\n",
       "                                        Q2_reasoning  ... adjusted_correct  \\\n",
       "0  BCBS_FEP policies specify requirements around ...  ...                4   \n",
       "1  Cigna typically requires an appropriate orderi...  ...                5   \n",
       "2  UHC policies generally require an appropriatel...  ...                5   \n",
       "3  BCBS FEP criteria typically require appropriat...  ...                4   \n",
       "4  UHC policies generally require an appropriate ...  ...                5   \n",
       "\n",
       "  adjusted_total adjusted_accuracy adjusted_wrong_questions  \\\n",
       "0              5         80.000000                       Q3   \n",
       "1              6         83.333333                       Q7   \n",
       "2              5        100.000000                      NaN   \n",
       "3              6         66.666667                    Q3,Q4   \n",
       "4              6         83.333333                       Q4   \n",
       "\n",
       "  adjusted_wrong_predictions adjusted_ground_truth unclear_count  \\\n",
       "0                     Q3:Yes                 Q3:No             0   \n",
       "1                   Q7:81277              Q7:81162             0   \n",
       "2                        NaN                   NaN             0   \n",
       "3               Q3:Yes|Q4:No          Q3:No|Q4:Yes             0   \n",
       "4                      Q4:No                Q4:Yes             0   \n",
       "\n",
       "  unclear_questions not_answerable_count not_answerable_questions  \n",
       "0               NaN                    0                      NaN  \n",
       "1               NaN                    0                      NaN  \n",
       "2               NaN                    1                       Q4  \n",
       "3               NaN                    0                      NaN  \n",
       "4               NaN                    0                      NaN  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "df = pd.read_csv('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/final/final_qna_results/final_results_with_accuracy_updated.csv')\n",
    "df_match = pd.read_csv('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/final/top3_30retrieve_gpt_5_mini_header_openai_small/retrieval/gpt_5_mini/top3/matching_summary.csv')\n",
    "df_match_simple = df_match[['case_id', 'matched']]\n",
    "df_merged = df.merge(df_match_simple, on='case_id', how='left')\n",
    "cols = df_merged.columns.tolist()\n",
    "case_id_idx = cols.index('case_id')\n",
    "cols.remove('matched')\n",
    "cols.insert(case_id_idx + 1, 'matched')\n",
    "df_merged = df_merged[cols]\n",
    "df_merged.to_csv('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/final/final_qna_results/final_file_updated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbc8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 11개 케이스 저장 완료: Q3_special_cases_updated.csv\n",
      "총 케이스: 11개\n",
      "\n",
      "답변 분포:\n",
      "Q3_true\n",
      "Yes    9\n",
      "No     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "without_policy가 맞춘 답:\n",
      "wp_answer\n",
      "Yes    9\n",
      "No     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "all_correct가 틀린 답:\n",
      "ac_answer\n",
      "No                9\n",
      "Not Answerable    1\n",
      "Yes               1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/final_ground_truth.json') as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "with open('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/filtered_llm_samples.json') as f:\n",
    "    patient_data = json.load(f)\n",
    "\n",
    "patient_info_dict = {item['id']: item['patient_info'] for item in patient_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc476170",
   "metadata": {},
   "source": [
    "# Data for Final decision reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bc416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 20개 케이스 저장 완료: Q8_special_cases_updated.csv\n",
      "총 케이스: 20개\n",
      "\n",
      "답변 분포:\n",
      "Q8_true\n",
      "No     14\n",
      "Yes     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "without_policy가 맞춘 답:\n",
      "wp_answer\n",
      "No     14\n",
      "Yes     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "all_correct가 틀린 답:\n",
      "ac_answer\n",
      "Yes    14\n",
      "No      6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "case_results = {}\n",
    "\n",
    "for _, row in df_merged.iterrows():\n",
    "    case_id = row['case_id']\n",
    "    case_type = row['case_type']\n",
    "    q3_answer = str(row['Q8_answer']).strip()\n",
    "    \n",
    "    if case_id not in ground_truth:\n",
    "        continue\n",
    "    \n",
    "    true_answer = str(ground_truth[case_id]['Q8']).strip()\n",
    "    is_correct = (q3_answer == true_answer)\n",
    "    \n",
    "    if case_id not in case_results:\n",
    "        case_results[case_id] = {}\n",
    "    \n",
    "    case_results[case_id][case_type] = {\n",
    "        'answer': q3_answer,\n",
    "        'is_correct': is_correct,\n",
    "        'reasoning': row['Q8_reasoning']\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "for case_id, types in case_results.items():\n",
    "    if 'without_policy' not in types or 'all_correct' not in types:\n",
    "        continue\n",
    "    \n",
    "    wp = types['without_policy'] \n",
    "    ac = types['all_correct']   \n",
    "\n",
    "    if wp['is_correct'] and not ac['is_correct']:\n",
    "        true_answer = str(ground_truth[case_id]['Q8']).strip()\n",
    "        \n",
    "        results.append({\n",
    "            'case_id': case_id,\n",
    "            'patient_info': patient_info_dict.get(case_id, 'N/A'),\n",
    "            'Q8_true': true_answer,\n",
    "            'wp_answer': wp['answer'],\n",
    "            'ac_answer': ac['answer'],\n",
    "            'wp_reasoning': wp['reasoning'],\n",
    "            'ac_reasoning': ac['reasoning']\n",
    "        })\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv('Q8_special_cases_updated.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e96571",
   "metadata": {},
   "source": [
    "# Final decesion Yes-Intermediate No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_questions(row):\n",
    "    no_qs = [f'Q{i}' for i in range(1, 8) if row[f'Q{i}_answer'] == 'No']\n",
    "    return ', '.join(no_qs) if no_qs else 'None'\n",
    "\n",
    "\n",
    "def has_no_in_q1_to_q7(row):\n",
    "    return any(row[f'Q{i}_answer'] == 'No' for i in range(1, 8))\n",
    "\n",
    "q8_yes_cases = [case_id for case_id, values in ground_truth.items() if values.get('Q8') == 'Yes']\n",
    "df_ground_truth_yes = df_merged[\n",
    "    (df_merged['case_id'].isin(q8_yes_cases)) & \n",
    "    (df_merged['Q8_answer'] == 'Yes')\n",
    "].copy()\n",
    "df_ground_truth_yes['has_no_in_pred'] = df_ground_truth_yes.apply(has_no_in_q1_to_q7, axis=1)\n",
    "\n",
    "df_pred_yes = df_merged[df_merged['Q8_answer'] == 'Yes'].copy()\n",
    "df_pred_yes['has_no_in_pred'] = df_pred_yes.apply(has_no_in_q1_to_q7, axis=1)\n",
    "\n",
    "\n",
    "df_pred_yes['no_questions'] = df_pred_yes.apply(get_no_questions, axis=1)\n",
    "\n",
    "df_with_no = df_pred_yes[df_pred_yes['has_no_in_pred'] == True]\n",
    "\n",
    "for case_type in ['all_correct', 'all_incorrect', 'without_policy']:\n",
    "    subset = df_with_no[df_with_no['case_type'] == case_type]\n",
    "    if len(subset) > 0:\n",
    "        subset[['case_id', 'mode', 'iteration', 'no_questions', 'Q8_reasoning']].to_csv(\n",
    "            f'/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/final/final_qna_results/q8_pred_yes_{case_type}_with_no_updated.csv',\n",
    "            index=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59cb44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Count  Accuracy\n",
      "case_type      Q8_answer                 \n",
      "all_correct    No           177    0.7571\n",
      "               Yes          138    0.5362\n",
      "all_incorrect  No           239    0.6778\n",
      "               Yes           75    0.5333\n",
      "without_policy No           218    0.7156\n",
      "               Yes           97    0.5670\n",
      "2. Q8 예측='Yes'인 모든 케이스\n",
      "                Count  Q1-Q7 No 개수  Q1-Q7 No 비율\n",
      "case_type                                      \n",
      "all_correct       138           29       0.2101\n",
      "all_incorrect      75           14       0.1867\n",
      "without_policy     97            8       0.0825\n"
     ]
    }
   ],
   "source": [
    "df_merged['Q8_ground_truth'] = df_merged['case_id'].apply(lambda x: ground_truth.get(x, {}).get('Q8'))\n",
    "df_merged['Q8_correct'] = df_merged['Q8_answer'] == df_merged['Q8_ground_truth']\n",
    "\n",
    "q8_dist = df_merged.groupby(['case_type', 'Q8_answer']).agg({\n",
    "    'case_id': 'count',\n",
    "    'Q8_correct': 'mean'\n",
    "}).round(4)\n",
    "q8_dist.columns = ['Count', 'Accuracy']\n",
    "print(q8_dist)\n",
    "\n",
    "print(\"2. Q8 Predict='Yes'\")\n",
    "summary2 = df_pred_yes.groupby(['case_type']).agg({\n",
    "    'case_id': 'count',\n",
    "    'has_no_in_pred': ['sum', 'mean']\n",
    "}).round(4)\n",
    "summary2.columns = ['Count', 'Q1-Q7 No #', 'Q1-Q7 No Ratio']\n",
    "print(summary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_merged[df_merged['not_answerable_count'] > 0].copy()\n",
    "\n",
    "df_na['questions_list'] = df_na['not_answerable_questions'].str.split(',')\n",
    "df_exploded = df_na.explode('questions_list')\n",
    "df_exploded['questions_list'] = df_exploded['questions_list'].str.strip()\n",
    "\n",
    "summary = df_exploded.groupby(['case_type', 'questions_list']).size().unstack(fill_value=0)\n",
    "\n",
    "print(\"Case Type Not Answerable Question distribution\")\n",
    "print(\"=\" * 70)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65cdce",
   "metadata": {},
   "source": [
    "# Consistency across 3 iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840ebbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 92.35%\n",
      "질문별 평균 일치율:\n",
      "Q0: 100.00%\n",
      "Q1: 82.86%\n",
      "Q2: 87.30%\n",
      "Q3: 97.78%\n",
      "Q4: 93.97%\n",
      "Q5: 88.25%\n",
      "Q6: 96.51%\n",
      "Q7: 87.94%\n",
      "Q8: 96.51%\n",
      "\n",
      "케이스별 평균 일치율:\n",
      "Case3223: 92.59%\n",
      "Case14226: 92.59%\n",
      "Case10451: 81.48%\n",
      "Case4196: 92.59%\n",
      "Case18281: 96.30%\n",
      "Case17196: 92.59%\n",
      "Case8288: 92.59%\n",
      "Case3422: 85.19%\n",
      "Case5885: 81.48%\n",
      "Case14195: 92.59%\n",
      "Case2847: 92.59%\n",
      "Case11855: 88.89%\n",
      "Case3465: 96.30%\n",
      "Case15557: 96.30%\n",
      "Case4982: 92.59%\n",
      "Case14371: 88.89%\n",
      "Case15202: 88.89%\n",
      "Case8621: 92.59%\n",
      "Case14141: 100.00%\n",
      "Case6747: 96.30%\n",
      "Case811: 92.59%\n",
      "Case7604: 92.59%\n",
      "Case950: 100.00%\n",
      "Case17822: 92.59%\n",
      "Case9408: 92.59%\n",
      "Case3592: 92.59%\n",
      "Case17929: 81.48%\n",
      "Case12834: 85.19%\n",
      "Case4604: 96.30%\n",
      "Case8661: 88.89%\n",
      "Case6451: 92.59%\n",
      "Case19255: 88.89%\n",
      "Case2211: 88.89%\n",
      "Case18882: 96.30%\n",
      "Case14408: 85.19%\n",
      "Case4262: 88.89%\n",
      "Case10363: 88.89%\n",
      "Case9349: 85.19%\n",
      "Case1428: 96.30%\n",
      "Case3359: 96.30%\n",
      "Case9978: 92.59%\n",
      "Case22: 92.59%\n",
      "Case5605: 88.89%\n",
      "Case19162: 88.89%\n",
      "Case15834: 92.59%\n",
      "Case13: 96.30%\n",
      "Case16487: 88.89%\n",
      "Case3677: 88.89%\n",
      "Case3474: 96.30%\n",
      "Case15938: 96.30%\n",
      "Case4067: 96.30%\n",
      "Case11514: 85.19%\n",
      "Case11981: 88.89%\n",
      "Case7682: 96.30%\n",
      "Case3494: 96.30%\n",
      "Case7014: 96.30%\n",
      "Case13517: 96.30%\n",
      "Case14127: 100.00%\n",
      "Case15820: 85.19%\n",
      "Case13675: 96.30%\n",
      "Case2162: 96.30%\n",
      "Case7376: 88.89%\n",
      "Case11124: 96.30%\n",
      "Case8051: 100.00%\n",
      "Case19905: 88.89%\n",
      "Case10850: 88.89%\n",
      "Case19321: 96.30%\n",
      "Case2496: 96.30%\n",
      "Case6867: 81.48%\n",
      "Case1130: 85.19%\n",
      "Case5303: 88.89%\n",
      "Case7447: 85.19%\n",
      "Case16126: 92.59%\n",
      "Case4512: 100.00%\n",
      "Case1778: 92.59%\n",
      "Case14155: 85.19%\n",
      "Case13406: 96.30%\n",
      "Case3581: 96.30%\n",
      "Case10529: 96.30%\n",
      "Case18821: 96.30%\n",
      "Case5230: 100.00%\n",
      "Case6478: 92.59%\n",
      "Case13613: 100.00%\n",
      "Case5380: 92.59%\n",
      "Case15570: 92.59%\n",
      "Case5834: 96.30%\n",
      "Case1501: 88.89%\n",
      "Case10427: 96.30%\n",
      "Case8485: 92.59%\n",
      "Case11795: 96.30%\n",
      "Case2007: 92.59%\n",
      "Case7297: 88.89%\n",
      "Case2666: 92.59%\n",
      "Case13983: 85.19%\n",
      "Case15458: 85.19%\n",
      "Case18868: 88.89%\n",
      "Case10917: 100.00%\n",
      "Case12792: 96.30%\n",
      "Case8710: 96.30%\n",
      "Case16246: 96.30%\n",
      "Case14017: 100.00%\n",
      "Case5674: 92.59%\n",
      "Case9773: 96.30%\n",
      "Case18257: 88.89%\n",
      "Case10075: 85.19%\n"
     ]
    }
   ],
   "source": [
    "def calc_consistency(df):\n",
    "    \"\"\"3 iteration answer consistency\"\"\"\n",
    "    \n",
    "    questions = [col.replace('_answer', '') for col in df.columns if col.endswith('_answer')]\n",
    "    \n",
    "    case_q_consistency = {}\n",
    "    \n",
    "    for case_id in df['case_id'].unique():\n",
    "        case_q_consistency[case_id] = {}\n",
    "        \n",
    "        for q in questions:\n",
    "            answers = df[df['case_id'] == case_id][f'{q}_answer'].tolist()\n",
    "            if len(answers) != 3:\n",
    "                continue\n",
    "            \n",
    "            max_count = Counter(answers).most_common(1)[0][1]\n",
    "            case_q_consistency[case_id][q] = (max_count / 3) * 100\n",
    "\n",
    "    q_avg = {q: np.mean([case_q_consistency[c][q] for c in case_q_consistency \n",
    "                         if q in case_q_consistency[c]]) \n",
    "             for q in questions}\n",
    "\n",
    "    case_avg = {c: np.mean(list(case_q_consistency[c].values())) \n",
    "                for c in case_q_consistency if case_q_consistency[c]}\n",
    "    \n",
    "    overall = np.mean(list(case_avg.values()))\n",
    "    \n",
    "    return overall, q_avg, case_avg\n",
    "\n",
    "baseline = df_merged[df_merged['case_type'] == 'without_policy']\n",
    "overall, q_avg, case_avg = calc_consistency(baseline)\n",
    "print(f\"Baseline: {overall:.2f}%\")\n",
    "print(f\"Question-wise average consistency:\")\n",
    "for q, avg in q_avg.items():\n",
    "    print(f\"{q}: {avg:.2f}%\")\n",
    "print(f\"\\nCase-wise average consistency:\")\n",
    "for case_id, avg in case_avg.items():\n",
    "    print(f\"{case_id}: {avg:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60827a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all correct: 92.77%\n",
      "질문별 평균 일치율:\n",
      "Q0: 100.00%\n",
      "Q1: 89.21%\n",
      "Q2: 88.89%\n",
      "Q3: 92.38%\n",
      "Q4: 91.75%\n",
      "Q5: 93.02%\n",
      "Q6: 90.16%\n",
      "Q7: 95.56%\n",
      "Q8: 93.97%\n",
      "\n",
      "케이스별 평균 일치율:\n",
      "Case3223: 96.30%\n",
      "Case14226: 88.89%\n",
      "Case10451: 100.00%\n",
      "Case4196: 74.07%\n",
      "Case18281: 92.59%\n",
      "Case17196: 100.00%\n",
      "Case8288: 88.89%\n",
      "Case3422: 88.89%\n",
      "Case5885: 96.30%\n",
      "Case14195: 92.59%\n",
      "Case2847: 96.30%\n",
      "Case11855: 96.30%\n",
      "Case3465: 88.89%\n",
      "Case15557: 92.59%\n",
      "Case4982: 92.59%\n",
      "Case14371: 92.59%\n",
      "Case15202: 100.00%\n",
      "Case8621: 88.89%\n",
      "Case14141: 88.89%\n",
      "Case6747: 88.89%\n",
      "Case811: 92.59%\n",
      "Case7604: 92.59%\n",
      "Case950: 96.30%\n",
      "Case17822: 92.59%\n",
      "Case9408: 96.30%\n",
      "Case3592: 92.59%\n",
      "Case17929: 88.89%\n",
      "Case12834: 100.00%\n",
      "Case4604: 88.89%\n",
      "Case8661: 92.59%\n",
      "Case6451: 81.48%\n",
      "Case19255: 96.30%\n",
      "Case2211: 92.59%\n",
      "Case18882: 96.30%\n",
      "Case14408: 100.00%\n",
      "Case4262: 85.19%\n",
      "Case10363: 100.00%\n",
      "Case9349: 77.78%\n",
      "Case1428: 100.00%\n",
      "Case3359: 100.00%\n",
      "Case9978: 88.89%\n",
      "Case22: 100.00%\n",
      "Case5605: 92.59%\n",
      "Case19162: 88.89%\n",
      "Case15834: 92.59%\n",
      "Case13: 85.19%\n",
      "Case16487: 92.59%\n",
      "Case3677: 96.30%\n",
      "Case3474: 85.19%\n",
      "Case15938: 88.89%\n",
      "Case4067: 92.59%\n",
      "Case11514: 96.30%\n",
      "Case11981: 77.78%\n",
      "Case7682: 96.30%\n",
      "Case3494: 88.89%\n",
      "Case7014: 96.30%\n",
      "Case13517: 92.59%\n",
      "Case14127: 100.00%\n",
      "Case15820: 96.30%\n",
      "Case13675: 100.00%\n",
      "Case2162: 88.89%\n",
      "Case7376: 92.59%\n",
      "Case11124: 92.59%\n",
      "Case8051: 85.19%\n",
      "Case19905: 96.30%\n",
      "Case10850: 96.30%\n",
      "Case19321: 85.19%\n",
      "Case2496: 96.30%\n",
      "Case6867: 100.00%\n",
      "Case1130: 96.30%\n",
      "Case5303: 96.30%\n",
      "Case7447: 92.59%\n",
      "Case16126: 92.59%\n",
      "Case4512: 85.19%\n",
      "Case1778: 88.89%\n",
      "Case14155: 88.89%\n",
      "Case13406: 96.30%\n",
      "Case3581: 92.59%\n",
      "Case10529: 96.30%\n",
      "Case18821: 88.89%\n",
      "Case5230: 88.89%\n",
      "Case6478: 100.00%\n",
      "Case13613: 96.30%\n",
      "Case5380: 92.59%\n",
      "Case15570: 96.30%\n",
      "Case5834: 100.00%\n",
      "Case1501: 96.30%\n",
      "Case10427: 96.30%\n",
      "Case8485: 88.89%\n",
      "Case11795: 85.19%\n",
      "Case2007: 96.30%\n",
      "Case7297: 92.59%\n",
      "Case2666: 92.59%\n",
      "Case13983: 88.89%\n",
      "Case15458: 96.30%\n",
      "Case18868: 81.48%\n",
      "Case10917: 92.59%\n",
      "Case12792: 96.30%\n",
      "Case8710: 88.89%\n",
      "Case16246: 100.00%\n",
      "Case14017: 88.89%\n",
      "Case5674: 96.30%\n",
      "Case9773: 92.59%\n",
      "Case18257: 96.30%\n",
      "Case10075: 96.30%\n"
     ]
    }
   ],
   "source": [
    "correct = df_merged[df_merged['case_type'] == 'all_correct']\n",
    "overall, q_avg, case_avg = calc_consistency(correct)\n",
    "print(f\"all correct: {overall:.2f}%\")\n",
    "print(f\"Question-wise average consistency:\")\n",
    "for q, avg in q_avg.items():\n",
    "    print(f\"{q}: {avg:.2f}%\")\n",
    "print(f\"\\nCase-wise average consistency:\")\n",
    "for case_id, avg in case_avg.items():\n",
    "    print(f\"{case_id}: {avg:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d32f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all incorrect: 86.95%\n",
      "질문별 평균 일치율:\n",
      "Q0: 100.00%\n",
      "Q1: 85.71%\n",
      "Q2: 86.67%\n",
      "Q3: 83.17%\n",
      "Q4: 78.41%\n",
      "Q5: 86.67%\n",
      "Q6: 87.30%\n",
      "Q7: 81.27%\n",
      "Q8: 93.33%\n",
      "\n",
      "케이스별 평균 일치율:\n",
      "Case3223: 96.30%\n",
      "Case14226: 85.19%\n",
      "Case10451: 92.59%\n",
      "Case4196: 85.19%\n",
      "Case18281: 77.78%\n",
      "Case17196: 85.19%\n",
      "Case8288: 85.19%\n",
      "Case3422: 77.78%\n",
      "Case5885: 70.37%\n",
      "Case14195: 88.89%\n",
      "Case2847: 81.48%\n",
      "Case11855: 96.30%\n",
      "Case3465: 77.78%\n",
      "Case15557: 85.19%\n",
      "Case4982: 92.59%\n",
      "Case14371: 81.48%\n",
      "Case15202: 96.30%\n",
      "Case8621: 92.59%\n",
      "Case14141: 88.89%\n",
      "Case6747: 70.37%\n",
      "Case811: 88.89%\n",
      "Case7604: 92.59%\n",
      "Case950: 85.19%\n",
      "Case17822: 85.19%\n",
      "Case9408: 92.59%\n",
      "Case3592: 96.30%\n",
      "Case17929: 100.00%\n",
      "Case12834: 92.59%\n",
      "Case4604: 92.59%\n",
      "Case8661: 81.48%\n",
      "Case6451: 85.19%\n",
      "Case19255: 85.19%\n",
      "Case2211: 96.30%\n",
      "Case18882: 81.48%\n",
      "Case14408: 81.48%\n",
      "Case4262: 85.19%\n",
      "Case10363: 88.89%\n",
      "Case9349: 81.48%\n",
      "Case1428: 81.48%\n",
      "Case3359: 100.00%\n",
      "Case9978: 96.30%\n",
      "Case22: 100.00%\n",
      "Case5605: 81.48%\n",
      "Case19162: 74.07%\n",
      "Case15834: 88.89%\n",
      "Case13: 88.89%\n",
      "Case16487: 85.19%\n",
      "Case3677: 92.59%\n",
      "Case3474: 70.37%\n",
      "Case15938: 88.89%\n",
      "Case4067: 88.89%\n",
      "Case11514: 88.89%\n",
      "Case11981: 81.48%\n",
      "Case7682: 88.89%\n",
      "Case3494: 81.48%\n",
      "Case7014: 92.59%\n",
      "Case13517: 96.30%\n",
      "Case14127: 88.89%\n",
      "Case15820: 96.30%\n",
      "Case13675: 85.19%\n",
      "Case2162: 74.07%\n",
      "Case7376: 85.19%\n",
      "Case11124: 96.30%\n",
      "Case8051: 77.78%\n",
      "Case19905: 85.19%\n",
      "Case10850: 77.78%\n",
      "Case19321: 74.07%\n",
      "Case2496: 88.89%\n",
      "Case6867: 100.00%\n",
      "Case1130: 96.30%\n",
      "Case5303: 74.07%\n",
      "Case7447: 77.78%\n",
      "Case16126: 92.59%\n",
      "Case4512: 85.19%\n",
      "Case1778: 81.48%\n",
      "Case14155: 88.89%\n",
      "Case13406: 85.19%\n",
      "Case3581: 88.89%\n",
      "Case10529: 81.48%\n",
      "Case18821: 92.59%\n",
      "Case5230: 96.30%\n",
      "Case6478: 88.89%\n",
      "Case13613: 92.59%\n",
      "Case5380: 81.48%\n",
      "Case15570: 88.89%\n",
      "Case5834: 96.30%\n",
      "Case1501: 77.78%\n",
      "Case10427: 85.19%\n",
      "Case8485: 74.07%\n",
      "Case11795: 81.48%\n",
      "Case2007: 96.30%\n",
      "Case7297: 88.89%\n",
      "Case2666: 81.48%\n",
      "Case13983: 85.19%\n",
      "Case15458: 100.00%\n",
      "Case18868: 70.37%\n",
      "Case10917: 85.19%\n",
      "Case12792: 92.59%\n",
      "Case8710: 88.89%\n",
      "Case16246: 88.89%\n",
      "Case14017: 92.59%\n",
      "Case5674: 85.19%\n",
      "Case9773: 92.59%\n",
      "Case18257: 81.48%\n",
      "Case10075: 96.30%\n"
     ]
    }
   ],
   "source": [
    "incorrect = df_merged[df_merged['case_type'] == 'all_incorrect']\n",
    "overall, q_avg, case_avg = calc_consistency(incorrect)\n",
    "print(f\"all incorrect: {overall:.2f}%\")\n",
    "print(f\"Question-wise average consistency:\")\n",
    "for q, avg in q_avg.items():\n",
    "    print(f\"{q}: {avg:.2f}%\")\n",
    "print(f\"\\nCase-wise average consistency:\")\n",
    "for case_id, avg in case_avg.items():\n",
    "    print(f\"{case_id}: {avg:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e9ff4",
   "metadata": {},
   "source": [
    "# Question-wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23dc11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_question_accuracy(df, ground_truth, questions):\n",
    "    iterations = df['iteration'].unique()\n",
    "    \n",
    "    basic_mean = {}\n",
    "    basic_se = {}\n",
    "    adj_mean = {}\n",
    "    adj_se = {}\n",
    "    \n",
    "    for q in questions:\n",
    "        answer_col = f'{q}_answer'\n",
    "        \n",
    "        basic_by_iter = []\n",
    "        adj_by_iter = []\n",
    "\n",
    "        for iteration in iterations:\n",
    "            iter_df = df[df['iteration'] == iteration]\n",
    "        \n",
    "            # Basic accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "        \n",
    "            # Adjusted accuracy\n",
    "            adj_correct = 0\n",
    "            adj_total = 0\n",
    "        \n",
    "            excluded = [\"Not Specified\", \"Question Unclear\", \"Not Answerable\"]\n",
    "        \n",
    "            for _, row in iter_df.iterrows():\n",
    "                case_id = row['case_id']\n",
    "                if case_id not in ground_truth:\n",
    "                    continue\n",
    "            \n",
    "                pred = str(row[answer_col]).strip()\n",
    "                true = str(ground_truth[case_id][q]).strip()\n",
    "            \n",
    "                # Basic\n",
    "                total += 1\n",
    "                if pred == true:\n",
    "                    correct += 1\n",
    "            \n",
    "                # Adjusted\n",
    "                if true not in excluded and pred not in excluded:\n",
    "                    adj_total += 1\n",
    "                    if pred == true:\n",
    "                        adj_correct += 1\n",
    "        \n",
    "            basic_by_iter.append((correct / total * 100) if total > 0 else 0)\n",
    "            adj_by_iter.append((adj_correct / adj_total * 100) if adj_total > 0 else 0)\n",
    "\n",
    "        basic_mean[q] = np.mean(basic_by_iter)\n",
    "        basic_se[q] = np.std(basic_by_iter, ddof=1) / np.sqrt(len(basic_by_iter))\n",
    "        \n",
    "        adj_mean[q] = np.mean(adj_by_iter)\n",
    "        adj_se[q] = np.std(adj_by_iter, ddof=1) / np.sqrt(len(adj_by_iter))\n",
    "    \n",
    "    return basic_mean, basic_se, adj_mean, adj_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d45a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_vote(df, ground_truth, questions, matched_val, mode_val):\n",
    "    \"\"\" majority vote results (≥2/3 correct = 1)\"\"\"\n",
    "    case_votes = {}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if row['matched'] != matched_val or row['mode'] != mode_val:\n",
    "            continue\n",
    "        \n",
    "        case_id = row['case_id']\n",
    "        if case_id not in ground_truth:\n",
    "            continue\n",
    "        if not all(q in ground_truth[case_id] for q in questions):\n",
    "            continue\n",
    "        \n",
    "        if case_id not in case_votes:\n",
    "            case_votes[case_id] = {q: [] for q in questions}\n",
    "        \n",
    "        for q in questions:\n",
    "            pred = str(row[f'{q}_answer']).strip()\n",
    "            true = str(ground_truth[case_id][q]).strip()\n",
    "            case_votes[case_id][q].append(1 if pred == true else 0)\n",
    "    \n",
    "    # Majority vote\n",
    "    binary_data = []\n",
    "    case_ids = []\n",
    "    for case_id, votes in case_votes.items():\n",
    "        case_result = [1 if sum(votes[q]) >= 2 else 0 for q in questions]\n",
    "        binary_data.append(case_result)\n",
    "        case_ids.append(case_id)\n",
    "    \n",
    "    return pd.DataFrame(binary_data, columns=questions, index=case_ids)\n",
    "\n",
    "def test_question_difficulty(df, ground_truth, questions, matched_val, mode_val, name):\n",
    "    \"\"\"Are questions significantly different in difficulty within a condition?\"\"\"\n",
    "    \n",
    "    binary_df = get_majority_vote(df, ground_truth, questions, matched_val, mode_val)\n",
    "    \n",
    "    if len(binary_df) == 0:\n",
    "        print(\"No complete cases!\")\n",
    "        return\n",
    "    \n",
    "    result = cochrans_q(binary_df)\n",
    "    q_stat = result.statistic\n",
    "    p_value = result.pvalue\n",
    "    print(f\"Cochran's Q={q_stat:.3f}, p={p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"→ Questions have significantly different difficulty levels\")\n",
    "    else:\n",
    "        print(\"→ No significant difference between questions\")\n",
    "\n",
    "def compare_conditions(df, ground_truth, questions, matched_val, \n",
    "                       mode1, mode2, name1, name2):\n",
    "    \"\"\"Compare two conditions for the same questions (e.g., Baseline vs RAG)\"\"\"\n",
    "    \n",
    "    df1 = get_majority_vote(df, ground_truth, questions, matched_val, mode1)\n",
    "    df2 = get_majority_vote(df, ground_truth, questions, matched_val, mode2)\n",
    "    \n",
    "    common_cases = df1.index.intersection(df2.index)\n",
    "    \n",
    "    if len(common_cases) == 0:\n",
    "        print(\"No common cases!\")\n",
    "        return\n",
    "    \n",
    "    df1 = df1.loc[common_cases]\n",
    "    df2 = df2.loc[common_cases]\n",
    "    \n",
    "    print(f\"N={len(common_cases)} cases\")\n",
    "    \n",
    "    n_tests = len(questions)\n",
    "    alpha_corrected = 0.05 / n_tests\n",
    "    \n",
    "    for q in questions:\n",
    "        b = ((df1[q] == 1) & (df2[q] == 0)).sum()  \n",
    "        c = ((df1[q] == 0) & (df2[q] == 1)).sum() \n",
    "        \n",
    "        rate1 = df1[q].mean() * 100\n",
    "        rate2 = df2[q].mean() * 100\n",
    "        \n",
    "        if b + c == 0: \n",
    "            print(f\"{q}: {name1}={rate1:.1f}% vs {name2}={rate2:.1f}%, p=N/A (no discordant pairs)\")\n",
    "            continue\n",
    "        \n",
    "        result = mcnemar([[0, b], [c, 0]], exact=True)    \n",
    "        \n",
    "        if result.pvalue < alpha_corrected:\n",
    "            print(f\"{q}: {name1}={rate1:.1f}% vs {name2}={rate2:.1f}%, p={result.pvalue:.4f} * (Bonferroni)\")\n",
    "        else:\n",
    "            print(f\"{q}: {name1}={rate1:.1f}% vs {name2}={rate2:.1f}%, p={result.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ad2f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cochran's Q=90.082, p=0.0000\n",
      "→ Questions have significantly different difficulty levels\n",
      "Cochran's Q=61.990, p=0.0000\n",
      "→ Questions have significantly different difficulty levels\n",
      "Cochran's Q=30.318, p=0.0002\n",
      "→ Questions have significantly different difficulty levels\n",
      "Cochran's Q=12.485, p=0.1308\n",
      "→ No significant difference between questions\n",
      "N=86 cases\n",
      "Q0: Baseline=100.0% vs RAG=100.0%, p=N/A (no discordant pairs)\n",
      "Q1: Baseline=50.0% vs RAG=69.8%, p=0.0186\n",
      "Q2: Baseline=58.1% vs RAG=84.9%, p=0.0002 * (Bonferroni)\n",
      "Q3: Baseline=67.4% vs RAG=84.9%, p=0.0007 * (Bonferroni)\n",
      "Q4: Baseline=43.0% vs RAG=66.3%, p=0.0002 * (Bonferroni)\n",
      "Q5: Baseline=73.3% vs RAG=87.2%, p=0.0075\n",
      "Q6: Baseline=41.9% vs RAG=86.0%, p=0.0000 * (Bonferroni)\n",
      "Q7: Baseline=59.3% vs RAG=94.2%, p=0.0000 * (Bonferroni)\n",
      "Q8: Baseline=67.4% vs RAG=88.4%, p=0.0003 * (Bonferroni)\n",
      "N=19 cases\n",
      "Q0: Baseline=100.0% vs RAG=100.0%, p=N/A (no discordant pairs)\n",
      "Q1: Baseline=42.1% vs RAG=89.5%, p=0.0117\n",
      "Q2: Baseline=78.9% vs RAG=94.7%, p=0.3750\n",
      "Q3: Baseline=68.4% vs RAG=78.9%, p=0.6250\n",
      "Q4: Baseline=68.4% vs RAG=84.2%, p=0.2500\n",
      "Q5: Baseline=57.9% vs RAG=94.7%, p=0.0156\n",
      "Q6: Baseline=21.1% vs RAG=78.9%, p=0.0010 * (Bonferroni)\n",
      "Q7: Baseline=47.4% vs RAG=100.0%, p=0.0020 * (Bonferroni)\n",
      "Q8: Baseline=63.2% vs RAG=78.9%, p=0.2500\n"
     ]
    }
   ],
   "source": [
    "questions = [f'Q{i}' for i in range(9)]\n",
    "\n",
    "test_question_difficulty(df_merged, ground_truth, questions, 1, 'baseline', \n",
    "                         \"Matched-Baseline\")\n",
    "test_question_difficulty(df_merged, ground_truth, questions, 1, 'rag', \n",
    "                         \"Matched-RAG\")\n",
    "test_question_difficulty(df_merged, ground_truth, questions, 0, 'baseline', \n",
    "                         \"Unmatched-Baseline\")\n",
    "test_question_difficulty(df_merged, ground_truth, questions, 0, 'rag', \n",
    "                         \"Unmatched-RAG\")\n",
    "\n",
    "compare_conditions(df_merged, ground_truth, questions, 1, \n",
    "                   'baseline', 'rag', \"Baseline\", \"RAG\")\n",
    "\n",
    "compare_conditions(df_merged, ground_truth, questions, 0, \n",
    "                   'baseline', 'rag', \"Baseline\", \"RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa7e21",
   "metadata": {},
   "source": [
    "# Case-wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db04da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_type  all_correct  all_incorrect  without_policy\n",
      "case_id                                              \n",
      "Case10075    50.000000      50.000000       50.000000\n",
      "Case10363   100.000000      95.238095       61.111111\n",
      "Case10427    71.428571      56.666667       68.253968\n",
      "Case10451    85.714286      82.142857       94.444444\n",
      "Case10529   100.000000      79.365079       71.428571\n",
      "case_type  all_correct  all_incorrect  without_policy\n",
      "case_id                                              \n",
      "Case10075    74.074074      85.185185       59.259259\n",
      "Case10363   100.000000      70.370370       48.148148\n",
      "Case10427    62.962963      33.333333       48.148148\n",
      "Case10451    77.777778      66.666667       55.555556\n",
      "Case10529    74.074074      51.851852       59.259259\n"
     ]
    }
   ],
   "source": [
    "pivot_data1 = df_merged.pivot_table(\n",
    "    index='case_id',\n",
    "    columns='case_type',\n",
    "    values='adjusted_accuracy',\n",
    "    aggfunc='mean'  \n",
    ")\n",
    "\n",
    "print( pivot_data1.head() )\n",
    "\n",
    "pivot_data2 = df_merged.pivot_table(\n",
    "    index='case_id',\n",
    "    columns='case_type',\n",
    "    values='accuracy',\n",
    "    aggfunc='mean'  \n",
    ")\n",
    "\n",
    "print( pivot_data2.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbee4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se(x):\n",
    "    return x.std() / np.sqrt(len(x))\n",
    "\n",
    "summary = df_merged.groupby(['mode', 'case_type', 'iteration']).agg({\n",
    "    'case_id': 'count',\n",
    "    'accuracy': ['mean', se],\n",
    "    'adjusted_accuracy': ['mean', se],\n",
    "    'adjusted_total': ['mean', se],\n",
    "    'not_answerable_count': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "summary.columns = ['Count', 'Avg Accuracy (%)', 'Accuracy SE', \n",
    "                   'Avg Adjusted Accuracy (%)', 'Adjusted Accuracy SE', \n",
    "                   'Adjusted Total', 'Adjusted Total SE',\n",
    "                   'Total Not Answerable']\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_means = df_merged.groupby(['mode', 'case_type', 'iteration']).agg({\n",
    "    'case_id': 'count',\n",
    "    'accuracy': 'mean',\n",
    "    'adjusted_accuracy': 'mean',\n",
    "    'adjusted_total': 'mean',\n",
    "    'not_answerable_count': 'sum'\n",
    "})\n",
    "summary_iter = iter_means.groupby(['mode', 'case_type']).agg({\n",
    "    'case_id': 'sum',\n",
    "    'accuracy': ['mean', se],\n",
    "    'adjusted_accuracy': ['mean', se],\n",
    "    'adjusted_total': ['mean', se],\n",
    "    'not_answerable_count': 'sum'\n",
    "}).round(2)\n",
    "summary_iter.columns = ['Count', 'Avg Accuracy (%)', 'Accuracy SE',\n",
    "                        'Avg Adjusted Accuracy (%)', 'Adjusted Accuracy SE',\n",
    "                        'Adjusted Total', 'Adjusted Total SE',\n",
    "                        'Total Not Answerable']\n",
    "print(summary_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf56665",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['mode_case'] = df_merged.apply(lambda row: \n",
    "    'baseline' if row['mode'] == 'baseline' \n",
    "    else f\"rag_{row['case_type']}\", axis=1)\n",
    "\n",
    "iter_means = df_merged.groupby(['matched', 'mode_case', 'iteration']).agg({\n",
    "    'case_id': 'count',\n",
    "    'accuracy': 'mean',\n",
    "    'adjusted_accuracy': 'mean',\n",
    "    'adjusted_total': 'mean',\n",
    "    'not_answerable_count': 'sum'\n",
    "})\n",
    "\n",
    "summary_iter = iter_means.groupby(['matched', 'mode_case']).agg({\n",
    "    'case_id': 'sum',\n",
    "    'accuracy': ['mean', se],\n",
    "    'adjusted_accuracy': ['mean', se],\n",
    "    'adjusted_total': ['mean', se],\n",
    "    'not_answerable_count': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "summary_iter.columns = ['Count', 'Avg Accuracy (%)', 'Accuracy SE',\n",
    "                        'Avg Adjusted Accuracy (%)', 'Adjusted Accuracy SE',\n",
    "                        'Adjusted Total', 'Adjusted Total SE',\n",
    "                        'Total Not Answerable']\n",
    "print(summary_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_merged[\n",
    "    ((df_merged['mode'] == 'rag') & (df_merged['case_type'] == 'all_correct') & (df_merged['matched'] == 1)) |\n",
    "    ((df_merged['mode'] == 'rag') & (df_merged['case_type'] == 'all_incorrect') & (df_merged['matched'] == 0)) |\n",
    "    (df_merged['mode'] == 'baseline')\n",
    "]\n",
    "\n",
    "iter_means2 = filtered_df.groupby(['mode', 'matched', 'iteration']).agg({\n",
    "    'case_id': 'count',\n",
    "    'accuracy': 'mean',\n",
    "    'adjusted_accuracy': 'mean',\n",
    "    'adjusted_total': 'mean',\n",
    "    'not_answerable_count': 'sum'\n",
    "})\n",
    "\n",
    "summary2_iter = iter_means2.groupby(['mode', 'matched']).agg({\n",
    "    'case_id': 'sum',\n",
    "    'accuracy': ['mean', se],\n",
    "    'adjusted_accuracy': ['mean', se],\n",
    "    'adjusted_total': ['mean', se],\n",
    "    'not_answerable_count': 'sum'\n",
    "}).round(2)\n",
    "summary2_iter.columns = ['Count', 'Avg Accuracy (%)', 'Accuracy SE',\n",
    "                         'Avg Adjusted Accuracy (%)', 'Adjusted Accuracy SE',\n",
    "                         'Adjusted Total', 'Adjusted Total SE',\n",
    "                         'Total Not Answerable']\n",
    "print(summary2_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3909ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case_type  all_correct  all_incorrect  without_policy\n",
      "case_id                                              \n",
      "Case10075    74.074074      85.185185       59.259259\n",
      "Case10363   100.000000      70.370370       48.148148\n",
      "Case10427    62.962963      33.333333       48.148148\n",
      "Case10451    77.777778      66.666667       55.555556\n",
      "Case10529    74.074074      51.851852       59.259259\n",
      "case_type  all_correct  all_incorrect  without_policy\n",
      "case_id                                              \n",
      "Case11855    51.851852      62.962963       62.962963\n",
      "Case12834   100.000000      59.259259       62.962963\n",
      "Case14155    81.481481      51.851852       62.962963\n",
      "Case15820    55.555556      62.962963       59.259259\n",
      "Case16246   100.000000      70.370370       55.555556\n"
     ]
    }
   ],
   "source": [
    "df_matched = df_merged[df_merged['matched'] == 1].copy()\n",
    "pivot_data_matched = df_matched.pivot_table(\n",
    "    index='case_id',\n",
    "    columns='case_type',\n",
    "    values='accuracy',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print( pivot_data_matched.head() )\n",
    "\n",
    "df_unmatched = df_merged[df_merged['matched'] == 0].copy()\n",
    "pivot_data_unmatched = df_unmatched.pivot_table(\n",
    "    index='case_id',\n",
    "    columns='case_type',\n",
    "    values='accuracy',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print( pivot_data_unmatched.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdabed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test statistic: 28.118644067796662, p-value: 7.83635354461258e-07\n",
      "Kendall's W: 0.133898305084746\n",
      "Friedman test statistic: 91.35038363171361, p-value: 1.4571901754039593e-20\n",
      "Kendall's W: 0.43500182681768385\n"
     ]
    }
   ],
   "source": [
    "conditions = ['without_policy', 'all_correct', 'all_incorrect'] \n",
    "data_for_friedman = [pivot_data1[case_type].values for case_type in conditions]\n",
    "\n",
    "friedman_stat, friedman_p = stats.friedmanchisquare(*data_for_friedman)\n",
    "\n",
    "# Effect size: Kendall's W\n",
    "n_subjects = len(pivot_data1)\n",
    "k_conditions = len(conditions)\n",
    "kendall_w = friedman_stat / (n_subjects * (k_conditions - 1))\n",
    "\n",
    "print(f\"Friedman test statistic: {friedman_stat}, p-value: {friedman_p}\")\n",
    "print(f\"Kendall's W: {kendall_w}\")\n",
    "\n",
    "data_for_friedman = [pivot_data2[case_type].values for case_type in conditions]\n",
    "\n",
    "friedman_stat, friedman_p = stats.friedmanchisquare(*data_for_friedman)\n",
    "\n",
    "# Effect size: Kendall's W\n",
    "n_subjects = len(pivot_data2)\n",
    "k_conditions = len(conditions)\n",
    "kendall_w = friedman_stat / (n_subjects * (k_conditions - 1))\n",
    "\n",
    "print(f\"Friedman test statistic: {friedman_stat}, p-value: {friedman_p}\")\n",
    "print(f\"Kendall's W: {kendall_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cec92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U_adj: U=860, p=0.721985\n",
      "Mann-Whitney U: U=812, p=0.973297\n",
      "Matched: 78.17% (SE: 1.65)\n",
      "Unmatched: 75.76% (SE: 4.03)\n",
      "Difference: +2.41%\n",
      "Matched: 62.32% (SE: 1.39)\n",
      "Unmatched: 61.01% (SE: 2.01)\n",
      "Difference: +1.30%\n",
      "\n",
      "Baseline Matched: 62.32% (SE: 1.39, 95% CI: [59.60%, 65.03%])\n",
      "Baseline Unmatched: 61.01% (SE: 2.01, 95% CI: [57.12%, 64.91%])\n"
     ]
    }
   ],
   "source": [
    "# Baseline filter\n",
    "df_baseline = df_merged[df_merged['mode'] == 'baseline'].copy()\n",
    "\n",
    "# Matched and Unmatched data\n",
    "df_baseline_matched = df_baseline[df_baseline['matched'] == 1]\n",
    "df_baseline_unmatched = df_baseline[df_baseline['matched'] == 0]\n",
    "\n",
    "# Case-wise average calculation\n",
    "baseline_matched_adj = df_baseline_matched.groupby('case_id')['adjusted_accuracy'].mean().values\n",
    "baseline_unmatched_adj = df_baseline_unmatched.groupby('case_id')['adjusted_accuracy'].mean().values\n",
    "baseline_matched = df_baseline_matched.groupby('case_id')['accuracy'].mean().values\n",
    "baseline_unmatched = df_baseline_unmatched.groupby('case_id')['accuracy'].mean().values\n",
    "\n",
    "\n",
    "# Mann-Whitney U test\n",
    "stat_adj, p_adj = mannwhitneyu(baseline_matched_adj, baseline_unmatched_adj, alternative='two-sided')\n",
    "print(f\"Mann-Whitney U_adj: U={stat_adj:.0f}, p={p_adj:.6f}\")\n",
    "stat, p = mannwhitneyu(baseline_matched, baseline_unmatched, alternative='two-sided')\n",
    "print(f\"Mann-Whitney U: U={stat:.0f}, p={p:.6f}\")\n",
    "\n",
    "\n",
    "# Mean and SE\n",
    "mean_m_adj = np.mean(baseline_matched_adj)\n",
    "se_m_adj = np.std(baseline_matched_adj, ddof=1) / np.sqrt(len(baseline_matched_adj))\n",
    "mean_u_adj = np.mean(baseline_unmatched_adj)\n",
    "se_u_adj = np.std(baseline_unmatched_adj, ddof=1) / np.sqrt(len(baseline_unmatched_adj))\n",
    "\n",
    "print(f\"Matched: {mean_m_adj:.2f}% (SE: {se_m_adj:.2f})\")\n",
    "print(f\"Unmatched: {mean_u_adj:.2f}% (SE: {se_u_adj:.2f})\")\n",
    "print(f\"Difference: {mean_m_adj - mean_u_adj:+.2f}%\")\n",
    "\n",
    "mean_m = np.mean(baseline_matched)\n",
    "se_m = np.std(baseline_matched, ddof=1) / np.sqrt(len(baseline_matched))\n",
    "mean_u = np.mean(baseline_unmatched)\n",
    "se_u = np.std(baseline_unmatched, ddof=1) / np.sqrt(len(baseline_unmatched))\n",
    "\n",
    "print(f\"Matched: {mean_m:.2f}% (SE: {se_m:.2f})\")\n",
    "print(f\"Unmatched: {mean_u:.2f}% (SE: {se_u:.2f})\")\n",
    "print(f\"Difference: {mean_m - mean_u:+.2f}%\")\n",
    "\n",
    "# Bootstrap CI for difference (independent samples)\n",
    "# Since the samples are independent, bootstrap each group separately and then calculate the difference\n",
    "n_bootstrap = 100000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Matched group CI\n",
    "bootstrap_matched = np.zeros(n_bootstrap)\n",
    "for i in range(n_bootstrap):\n",
    "    boot_sample = np.random.choice(baseline_matched, size=len(baseline_matched), replace=True)\n",
    "    bootstrap_matched[i] = np.mean(boot_sample)\n",
    "\n",
    "lower_ci_matched = np.percentile(bootstrap_matched, 2.5)\n",
    "upper_ci_matched = np.percentile(bootstrap_matched, 97.5)\n",
    "\n",
    "# Unmatched group CI\n",
    "bootstrap_unmatched = np.zeros(n_bootstrap)\n",
    "for i in range(n_bootstrap):\n",
    "    boot_sample = np.random.choice(baseline_unmatched, size=len(baseline_unmatched), replace=True)\n",
    "    bootstrap_unmatched[i] = np.mean(boot_sample)\n",
    "\n",
    "lower_ci_unmatched = np.percentile(bootstrap_unmatched, 2.5)\n",
    "upper_ci_unmatched = np.percentile(bootstrap_unmatched, 97.5)\n",
    "\n",
    "print(f\"\\nBaseline Matched: {mean_m:.2f}% (SE: {se_m:.2f}, 95% CI: [{lower_ci_matched:.2f}%, {upper_ci_matched:.2f}%])\")\n",
    "print(f\"Baseline Unmatched: {mean_u:.2f}% (SE: {se_u:.2f}, 95% CI: [{lower_ci_unmatched:.2f}%, {upper_ci_unmatched:.2f}%])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb2599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched cases: 86\n",
      "Unmatched cases: 19\n",
      "Mann-Whitney U_adj: U=1065, p=0.018148\n"
     ]
    }
   ],
   "source": [
    "# RAG matched = all_correct\n",
    "df_rag_matched = df_merged[(df_merged['mode'] == 'rag') & (df_merged['matched'] == 1) & (df_merged['case_type'] == 'all_correct')]\n",
    "# RAG unmatched = all_incorrect\n",
    "df_rag_unmatched = df_merged[(df_merged['mode'] == 'rag') & (df_merged['matched'] == 0) & (df_merged['case_type'] == 'all_incorrect')]\n",
    "\n",
    "# Case-wise average calculation\n",
    "rag_matched_adj = df_rag_matched.groupby('case_id')['adjusted_accuracy'].mean().values\n",
    "rag_unmatched_adj = df_rag_unmatched.groupby('case_id')['adjusted_accuracy'].mean().values\n",
    "\n",
    "print(f\"Matched cases: {len(rag_matched_adj)}\")\n",
    "print(f\"Unmatched cases: {len(rag_unmatched_adj)}\")\n",
    "\n",
    "# Mann-Whitney U test\n",
    "stat_rag_adj, p_rag_adj = mannwhitneyu(rag_matched_adj, rag_unmatched_adj, alternative='greater')\n",
    "print(f\"Mann-Whitney U_adj: U={stat_rag_adj:.0f}, p={p_rag_adj:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b521d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched cases: 86\n",
      "Unmatched cases: 19\n",
      "Mann-Whitney U: U=1324, p=0.000012\n"
     ]
    }
   ],
   "source": [
    "# Case-wise average calculation for accuracy\n",
    "rag_matched = df_rag_matched.groupby('case_id')['accuracy'].mean().values\n",
    "rag_unmatched = df_rag_unmatched.groupby('case_id')['accuracy'].mean().values\n",
    "\n",
    "print(f\"Matched cases: {len(rag_matched)}\")\n",
    "print(f\"Unmatched cases: {len(rag_unmatched)}\")\n",
    "\n",
    "# Mann-Whitney U test\n",
    "stat_rag, p_rag = mannwhitneyu(rag_matched, rag_unmatched, alternative='greater')\n",
    "print(f\"Mann-Whitney U: U={stat_rag:.0f}, p={p_rag:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 73.30% (SE: 1.48)\n",
      "Unmatched: 57.50% (SE: 2.52)\n",
      "Difference: +15.79%\n",
      "Matched (Adjusted): 85.60% (SE: 1.49)\n",
      "Unmatched (Adjusted): 75.20% (SE: 4.55)\n",
      "Difference (Adjusted): +10.40%\n",
      "\n",
      "RAG Matched: 73.30% (SE: 1.48, 95% CI: [70.46%, 76.18%])\n",
      "RAG Unmatched: 57.50% (SE: 2.52, 95% CI: [52.83%, 62.38%])\n"
     ]
    }
   ],
   "source": [
    "# Mean and SE\n",
    "mean_m_rag = np.mean(rag_matched)\n",
    "se_m_rag = np.std(rag_matched, ddof=1) / np.sqrt(len(rag_matched))\n",
    "mean_u_rag = np.mean(rag_unmatched)\n",
    "se_u_rag = np.std(rag_unmatched, ddof=1) / np.sqrt(len(rag_unmatched))\n",
    "    \n",
    "mean_m_rag_adj = np.mean(rag_matched_adj)\n",
    "se_m_rag_adj = np.std(rag_matched_adj, ddof=1) / np.sqrt(len(rag_matched_adj))\n",
    "mean_u_rag_adj = np.mean(rag_unmatched_adj)\n",
    "se_u_rag_adj = np.std(rag_unmatched_adj, ddof=1) / np.sqrt(len(rag_unmatched_adj))\n",
    "\n",
    "print(f\"Matched: {mean_m_rag:.2f}% (SE: {se_m_rag:.2f})\")\n",
    "print(f\"Unmatched: {mean_u_rag:.2f}% (SE: {se_u_rag:.2f})\")\n",
    "print(f\"Difference: {mean_m_rag - mean_u_rag:+.2f}%\")\n",
    "\n",
    "print(f\"Matched (Adjusted): {mean_m_rag_adj:.2f}% (SE: {se_m_rag_adj:.2f})\")\n",
    "print(f\"Unmatched (Adjusted): {mean_u_rag_adj:.2f}% (SE: {se_u_rag_adj:.2f})\")\n",
    "print(f\"Difference (Adjusted): {mean_m_rag_adj - mean_u_rag_adj:+.2f}%\")\n",
    "\n",
    "bootstrap_matched_rag = np.zeros(n_bootstrap)\n",
    "for i in range(n_bootstrap):\n",
    "    boot_sample = np.random.choice(rag_matched, size=len(rag_matched), replace=True)\n",
    "    bootstrap_matched_rag[i] = np.mean(boot_sample)\n",
    "\n",
    "lower_ci_matched_rag = np.percentile(bootstrap_matched_rag, 2.5)\n",
    "upper_ci_matched_rag = np.percentile(bootstrap_matched_rag, 97.5)\n",
    "\n",
    "# RAG - Unmatched group CI\n",
    "bootstrap_unmatched_rag = np.zeros(n_bootstrap)\n",
    "for i in range(n_bootstrap):\n",
    "    boot_sample = np.random.choice(rag_unmatched, size=len(rag_unmatched), replace=True)\n",
    "    bootstrap_unmatched_rag[i] = np.mean(boot_sample)\n",
    "\n",
    "lower_ci_unmatched_rag = np.percentile(bootstrap_unmatched_rag, 2.5)\n",
    "upper_ci_unmatched_rag = np.percentile(bootstrap_unmatched_rag, 97.5)\n",
    "\n",
    "print(f\"\\nRAG Matched: {mean_m_rag:.2f}% (SE: {se_m_rag:.2f}, 95% CI: [{lower_ci_matched_rag:.2f}%, {upper_ci_matched_rag:.2f}%])\")\n",
    "print(f\"RAG Unmatched: {mean_u_rag:.2f}% (SE: {se_u_rag:.2f}, 95% CI: [{lower_ci_unmatched_rag:.2f}%, {upper_ci_unmatched_rag:.2f}%])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "979fafb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Comparison  V_statistic       p_value Significant  \\\n",
      "0    without_policy vs all_correct        757.5  2.747721e-06         Yes   \n",
      "1  without_policy vs all_incorrect       1919.5  3.927017e-01          No   \n",
      "2     all_correct vs all_incorrect        667.5  6.232523e-07         Yes   \n",
      "\n",
      "   Mean_diff  Median_diff           cond1          cond2  \n",
      "0  -8.277400    -4.761905  without_policy    all_correct  \n",
      "1   1.836735     0.000000  without_policy  all_incorrect  \n",
      "2  10.114135     5.555556     all_correct  all_incorrect  \n"
     ]
    }
   ],
   "source": [
    "n_comparisons = len(list(combinations(conditions, 2)))\n",
    "alpha = 0.05\n",
    "bonferroni_alpha = alpha / n_comparisons\n",
    "\n",
    "posthoc_results = []\n",
    "from itertools import combinations\n",
    "\n",
    "for cond1, cond2 in combinations(conditions, 2):\n",
    "    data1 = pivot_data1[cond1].values\n",
    "    data2 = pivot_data1[cond2].values\n",
    "    \n",
    "    stat, p_value = wilcoxon(data1, data2)\n",
    "    diff = data1 - data2\n",
    "    mean_diff = np.mean(diff)\n",
    "    median_diff = np.median(diff)\n",
    "\n",
    "    result = {\n",
    "        'Comparison': f'{cond1} vs {cond2}',\n",
    "        'V_statistic': stat,\n",
    "        'p_value': p_value,\n",
    "        'Significant': 'Yes' if p_value < bonferroni_alpha else 'No',\n",
    "        'Mean_diff': mean_diff,\n",
    "        'Median_diff': median_diff,\n",
    "        'cond1': cond1,\n",
    "        'cond2': cond2\n",
    "    }\n",
    "    posthoc_results.append(result)\n",
    "    \n",
    "posthoc_df = pd.DataFrame(posthoc_results)\n",
    "print(posthoc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c061ee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Comparison  V_statistic       p_value Significant  \\\n",
      "0    without_policy vs all_correct        602.5  1.033488e-10         Yes   \n",
      "1  without_policy vs all_incorrect        754.0  1.140699e-07         Yes   \n",
      "2     all_correct vs all_incorrect        142.5  3.882047e-16         Yes   \n",
      "\n",
      "   Mean_diff  Median_diff           cond1          cond2  \n",
      "0 -11.887125   -11.111111  without_policy    all_correct  \n",
      "1   7.936508     7.407407  without_policy  all_incorrect  \n",
      "2  19.823633    18.518519     all_correct  all_incorrect  \n"
     ]
    }
   ],
   "source": [
    "n_comparisons = len(list(combinations(conditions, 2)))\n",
    "alpha = 0.05\n",
    "bonferroni_alpha = alpha / n_comparisons\n",
    "\n",
    "posthoc_results2 = []\n",
    "from itertools import combinations\n",
    "\n",
    "for cond1, cond2 in combinations(conditions, 2):\n",
    "    data1 = pivot_data2[cond1].values\n",
    "    data2 = pivot_data2[cond2].values\n",
    "    \n",
    "    stat, p_value = wilcoxon(data1, data2)\n",
    "    diff = data1 - data2\n",
    "    mean_diff = np.mean(diff)\n",
    "    median_diff = np.median(diff)\n",
    "\n",
    "    result = {\n",
    "        'Comparison': f'{cond1} vs {cond2}',\n",
    "        'V_statistic': stat,\n",
    "        'p_value': p_value,\n",
    "        'Significant': 'Yes' if p_value < bonferroni_alpha else 'No',\n",
    "        'Mean_diff': mean_diff,\n",
    "        'Median_diff': median_diff,\n",
    "        'cond1': cond1,\n",
    "        'cond2': cond2\n",
    "    }\n",
    "    posthoc_results2.append(result)\n",
    "    \n",
    "posthoc_df2 = pd.DataFrame(posthoc_results2)\n",
    "print(posthoc_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6bb6490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test statistic (matched): 75.17868338557983, p-value: 4.7332160057243466e-17\n",
      "Kendall's W (matched): 0.437085368520813\n",
      "Friedman test statistic (unmatched): 17.333333333333314, p-value: 0.0001722322559608116\n",
      "Kendall's W (unmatched): 0.45614035087719246\n"
     ]
    }
   ],
   "source": [
    "data_for_friedman = [pivot_data_matched[case_type].values for case_type in conditions]\n",
    "\n",
    "friedman_stat, friedman_p = stats.friedmanchisquare(*data_for_friedman)\n",
    "\n",
    "# Effect size: Kendall's W\n",
    "n_subjects = len(pivot_data_matched)\n",
    "k_conditions = len(conditions)\n",
    "kendall_w = friedman_stat / (n_subjects * (k_conditions - 1))\n",
    "\n",
    "print(f\"Friedman test statistic (matched): {friedman_stat}, p-value: {friedman_p}\")\n",
    "print(f\"Kendall's W (matched): {kendall_w}\")\n",
    "\n",
    "data_for_friedman = [pivot_data_unmatched[case_type].values for case_type in conditions]\n",
    "friedman_stat, friedman_p = stats.friedmanchisquare(*data_for_friedman)\n",
    "\n",
    "# Effect size: Kendall's W\n",
    "n_subjects = len(pivot_data_unmatched)\n",
    "k_conditions = len(conditions)\n",
    "kendall_w = friedman_stat / (n_subjects * (k_conditions - 1))\n",
    "\n",
    "print(f\"Friedman test statistic (unmatched): {friedman_stat}, p-value: {friedman_p}\")\n",
    "print(f\"Kendall's W (unmatched): {kendall_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca0c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bootstrap 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_single_condition_ci(data, n_bootstrap=50000, ci_level=0.95, random_state=42):\n",
    "    \"\"\"   \n",
    "    Returns: observed_mean, lower_ci, upper_ci, bootstrap_samples\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n = len(data)\n",
    "    bootstrap_means = np.zeros(n_bootstrap)\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        indices = np.random.choice(n, size=n, replace=True)\n",
    "        bootstrap_means[i] = np.mean(data[indices])\n",
    "    \n",
    "    observed_mean = np.mean(data)\n",
    "    alpha = 1 - ci_level\n",
    "    lower_ci = np.percentile(bootstrap_means, 100 * alpha / 2)\n",
    "    upper_ci = np.percentile(bootstrap_means, 100 * (1 - alpha / 2))\n",
    "    \n",
    "    return observed_mean, lower_ci, upper_ci, bootstrap_means\n",
    "\n",
    "\n",
    "def bootstrap_paired_difference_ci(data1, data2, n_bootstrap=50000, ci_level=0.95, random_state=42):\n",
    "    \"\"\"    \n",
    "    Returns: observed_diff, lower_ci, upper_ci, bootstrap_samples\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n = len(data1)\n",
    "    bootstrap_diffs = np.zeros(n_bootstrap)\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        indices = np.random.choice(n, size=n, replace=True)\n",
    "        boot_data1 = data1[indices]\n",
    "        boot_data2 = data2[indices]\n",
    "        bootstrap_diffs[i] = np.mean(boot_data1 - boot_data2)\n",
    "    \n",
    "    observed_diff = np.mean(data1 - data2)\n",
    "    alpha = 1 - ci_level\n",
    "    lower_ci = np.percentile(bootstrap_diffs, 100 * alpha / 2)\n",
    "    upper_ci = np.percentile(bootstrap_diffs, 100 * (1 - alpha / 2))\n",
    "    \n",
    "    return observed_diff, lower_ci, upper_ci, bootstrap_diffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de99eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "각 조건의 평균 정확도 (95% Bootstrap CI, n=100,000)\n",
      "======================================================================\n",
      "without_policy      :  62.08% (95% CI:  59.79%– 64.41%)\n",
      "all_correct         :  73.97% (95% CI:  71.29%– 76.68%)\n",
      "all_incorrect       :  54.14% (95% CI:  51.22%– 57.14%)\n",
      "        Condition       Mean   Lower_CI   Upper_CI  CI_Width\n",
      "0  without_policy  62.081129  59.788360  64.409171  4.620811\n",
      "1     all_correct  73.968254  71.287478  76.684303  5.396825\n",
      "2   all_incorrect  54.144621  51.216931  57.142857  5.925926\n"
     ]
    }
   ],
   "source": [
    "n_bootstrap = 100000  \n",
    "print(f\"Each case accuracy (95% Bootstrap CI, n={n_bootstrap:,})\")\n",
    "\n",
    "single_results = []\n",
    "\n",
    "for condition in conditions:\n",
    "    data = pivot_data2[condition].values\n",
    "    mean_val, lower, upper, samples = bootstrap_single_condition_ci(\n",
    "        data, \n",
    "        n_bootstrap=n_bootstrap\n",
    "    )\n",
    "    \n",
    "    single_results.append({\n",
    "        'Condition': condition,\n",
    "        'Mean': mean_val,\n",
    "        'Lower_CI': lower,\n",
    "        'Upper_CI': upper,\n",
    "        'CI_Width': upper - lower\n",
    "    })\n",
    "    \n",
    "    print(f\"{condition:20s}: {mean_val:6.2f}% (95% CI: {lower:6.2f}%–{upper:6.2f}%)\")  \n",
    "\n",
    "single_df = pd.DataFrame(single_results)\n",
    "print(single_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c23f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Comparison  Observed_Mean_Diff  Bootstrap_Lower_CI  \\\n",
      "0    without_policy vs all_correct          -11.887125          -14.885362   \n",
      "1  without_policy vs all_incorrect            7.936508            5.220459   \n",
      "2     all_correct vs all_incorrect           19.823633           16.649030   \n",
      "\n",
      "   Bootstrap_Upper_CI CI_Excludes_Zero  \n",
      "0           -8.924162              Yes  \n",
      "1           10.652557              Yes  \n",
      "2           23.033510              Yes  \n"
     ]
    }
   ],
   "source": [
    "n_bootstrap = 100000\n",
    "ci_level = 0.95\n",
    "bootstrap_results = []\n",
    "\n",
    "for idx, row in posthoc_df.iterrows():\n",
    "    cond1 = row['cond1']\n",
    "    cond2 = row['cond2']\n",
    "    \n",
    "    data1 = pivot_data2[cond1].values\n",
    "    data2 = pivot_data2[cond2].values\n",
    "\n",
    "    observed_diff, lower_ci, upper_ci, boot_diffs = bootstrap_paired_difference_ci(\n",
    "        data1, data2, n_bootstrap=n_bootstrap, ci_level=ci_level\n",
    "    )\n",
    "    \n",
    "    ci_excludes_zero = not (lower_ci <= 0 <= upper_ci)\n",
    "    \n",
    "    result = {\n",
    "        'Comparison': row['Comparison'],\n",
    "        'Observed_Mean_Diff': observed_diff,  \n",
    "        'Bootstrap_Lower_CI': lower_ci,\n",
    "        'Bootstrap_Upper_CI': upper_ci,\n",
    "        'CI_Excludes_Zero': 'Yes' if ci_excludes_zero else 'No',\n",
    "        'bootstrap_samples': boot_diffs\n",
    "    }\n",
    "    bootstrap_results.append(result)\n",
    "\n",
    "bootstrap_df = pd.DataFrame(bootstrap_results)\n",
    "\n",
    "display_cols = ['Comparison', 'Observed_Mean_Diff', 'Bootstrap_Lower_CI', \n",
    "                'Bootstrap_Upper_CI', 'CI_Excludes_Zero']\n",
    "print(bootstrap_df[display_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9aa7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "각 조건의 평균 정확도 (95% Bootstrap CI, n=100,000)\n",
      "======================================================================\n",
      "without_policy      :  62.32% (95% CI:  59.60%– 65.03%)\n",
      "all_correct         :  73.30% (95% CI:  70.46%– 76.23%)\n",
      "all_incorrect       :  53.40% (95% CI:  50.00%– 56.89%)\n",
      "        Condition       Mean   Lower_CI   Upper_CI  CI_Width\n",
      "0  without_policy  62.316968  59.603790  65.030146  5.426357\n",
      "1     all_correct  73.298880  70.456503  76.227390  5.770887\n",
      "2   all_incorrect  53.402239  50.000000  56.890612  6.890612\n"
     ]
    }
   ],
   "source": [
    "n_bootstrap = 100000 \n",
    "\n",
    "print(f\"Matched: Case Mean Accuracy (95% Bootstrap CI, n={n_bootstrap:,})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "single_results = []\n",
    "\n",
    "for condition in conditions:\n",
    "    data = pivot_data_matched[condition].values\n",
    "    mean_val, lower, upper, samples = bootstrap_single_condition_ci(\n",
    "        data, \n",
    "        n_bootstrap=n_bootstrap\n",
    "    )\n",
    "    \n",
    "    single_results.append({\n",
    "        'Condition': condition,\n",
    "        'Mean': mean_val,\n",
    "        'Lower_CI': lower,\n",
    "        'Upper_CI': upper,\n",
    "        'CI_Width': upper - lower\n",
    "    })\n",
    "    \n",
    "    print(f\"{condition:20s}: {mean_val:6.2f}% (95% CI: {lower:6.2f}%–{upper:6.2f}%)\")  \n",
    "\n",
    "single_df = pd.DataFrame(single_results)\n",
    "print(single_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f9988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "각 조건의 평균 정확도 (95% Bootstrap CI, n=100,000)\n",
      "======================================================================\n",
      "without_policy      :  61.01% (95% CI:  57.12%– 64.91%)\n",
      "all_correct         :  77.00% (95% CI:  69.79%– 84.21%)\n",
      "all_incorrect       :  57.50% (95% CI:  52.83%– 62.38%)\n",
      "        Condition       Mean   Lower_CI   Upper_CI   CI_Width\n",
      "0  without_policy  61.013645  57.115010  64.912281   7.797271\n",
      "1     all_correct  76.998051  69.785575  84.210526  14.424951\n",
      "2   all_incorrect  57.504873  52.826511  62.378168   9.551657\n"
     ]
    }
   ],
   "source": [
    "n_bootstrap = 100000 \n",
    "\n",
    "print(f\"Unmatched: Case Mean Accuracy (95% Bootstrap CI, n={n_bootstrap:,})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "single_results = []\n",
    "\n",
    "for condition in conditions:\n",
    "    data = pivot_data_unmatched[condition].values\n",
    "    mean_val, lower, upper, samples = bootstrap_single_condition_ci(\n",
    "        data, \n",
    "        n_bootstrap=n_bootstrap\n",
    "    )\n",
    "    \n",
    "    single_results.append({\n",
    "        'Condition': condition,\n",
    "        'Mean': mean_val,\n",
    "        'Lower_CI': lower,\n",
    "        'Upper_CI': upper,\n",
    "        'CI_Width': upper - lower\n",
    "    })\n",
    "    \n",
    "    print(f\"{condition:20s}: {mean_val:6.2f}% (95% CI: {lower:6.2f}%–{upper:6.2f}%)\")  \n",
    "\n",
    "single_df = pd.DataFrame(single_results)\n",
    "print(single_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b65c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Count  Accuracy\n",
      "case_type      Q8_answer                 \n",
      "all_correct    No           177    0.7571\n",
      "               Yes          138    0.5362\n",
      "all_incorrect  No           239    0.6778\n",
      "               Yes           75    0.5333\n",
      "without_policy No           218    0.7156\n",
      "               Yes           97    0.5670\n",
      "======================================================================\n",
      "1. Q8 정답='Yes' AND Q8 예측='Yes'\n",
      "======================================================================\n",
      "                Count  Q1-Q7 No 개수  Q1-Q7 No 비율\n",
      "case_type                                      \n",
      "all_correct        74            8       0.1081\n",
      "all_incorrect      40            4       0.1000\n",
      "without_policy     55            4       0.0727\n",
      "\n",
      "======================================================================\n",
      "2. Q8 예측='Yes'인 모든 케이스\n",
      "======================================================================\n",
      "                Count  Q1-Q7 No 개수  Q1-Q7 No 비율\n",
      "case_type                                      \n",
      "all_correct       138           29       0.2101\n",
      "all_incorrect      75           14       0.1867\n",
      "without_policy     97            8       0.0825\n"
     ]
    }
   ],
   "source": [
    "df_merged['Q8_ground_truth'] = df_merged['case_id'].apply(lambda x: ground_truth.get(x, {}).get('Q8'))\n",
    "df_merged['Q8_correct'] = df_merged['Q8_answer'] == df_merged['Q8_ground_truth']\n",
    "\n",
    "# Q8 답변 분포 + 정답률\n",
    "q8_dist = df_merged.groupby(['case_type', 'Q8_answer']).agg({\n",
    "    'case_id': 'count',\n",
    "    'Q8_correct': 'mean'\n",
    "}).round(4)\n",
    "q8_dist.columns = ['Count', 'Accuracy']\n",
    "print(q8_dist)\n",
    "\n",
    "def has_no_in_q1_to_q7(row):\n",
    "    return any(row[f'Q{i}_answer'] == 'No' for i in range(1, 8))\n",
    "\n",
    "q8_yes_cases = [case_id for case_id, values in ground_truth.items() if values.get('Q8') == 'Yes']\n",
    "df_ground_truth_yes = df_merged[\n",
    "    (df_merged['case_id'].isin(q8_yes_cases)) & \n",
    "    (df_merged['Q8_answer'] == 'Yes')\n",
    "].copy()\n",
    "df_ground_truth_yes['has_no_in_pred'] = df_ground_truth_yes.apply(has_no_in_q1_to_q7, axis=1)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Q8 정답='Yes' AND Q8 예측='Yes'\")\n",
    "print(\"=\" * 70)\n",
    "summary1 = df_ground_truth_yes.groupby(['case_type']).agg({\n",
    "    'case_id': 'count',\n",
    "    'has_no_in_pred': ['sum', 'mean']\n",
    "}).round(4)\n",
    "summary1.columns = ['Count', 'Q1-Q7 No 개수', 'Q1-Q7 No 비율']\n",
    "print(summary1)\n",
    "\n",
    "df_pred_yes = df_merged[df_merged['Q8_answer'] == 'Yes'].copy()\n",
    "df_pred_yes['has_no_in_pred'] = df_pred_yes.apply(has_no_in_q1_to_q7, axis=1)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"2. Q8 예측='Yes'인 모든 케이스\")\n",
    "print(\"=\" * 70)\n",
    "summary2 = df_pred_yes.groupby(['case_type']).agg({\n",
    "    'case_id': 'count',\n",
    "    'has_no_in_pred': ['sum', 'mean']\n",
    "}).round(4)\n",
    "summary2.columns = ['Count', 'Q1-Q7 No 개수', 'Q1-Q7 No 비율']\n",
    "print(summary2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
