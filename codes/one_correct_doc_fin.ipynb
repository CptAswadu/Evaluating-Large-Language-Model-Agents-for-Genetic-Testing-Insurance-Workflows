{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ff6421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cptaswadu/new-rescue/RESCUE-n8n/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import glob\n",
    "import PyPDF2 \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hashlib\n",
    "from difflib import SequenceMatcher\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78ea977",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/cptaswadu/new-rescue/RESCUE-n8n'\n",
    "load_dotenv(dotenv_path=os.path.join(path, \".env\"))\n",
    "openai_api_key = os.getenv(\"OPEN_AI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a94444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCBS_FEP_20402 Germline Genetic Testing for.pdf => c5c2b854957d06467835e88a963d0c82\n",
      "Cigna_MOL.TS_.150A_CMA_for_Developmental_Disorders_and_Prenatal_Diagnosis_eff01.01.2025_pub09.10.2024_1.pdf => dd74cd39fca15b7b0888b16ce1da2014\n",
      "Cigna_MOL.TS_.344.A_Chromosomal_Microarray_Solid_Tumors_eff01.01.2025_pub09.11.2024_0.pdf => 49b51c0399e5563339f32a9f24f20641\n",
      "BCBS_FEP_20459 Genetic Testing for Developmental.pdf => 8340e5b0ce4959eccfb2cb295edb47f3\n",
      "Cigna_MOL.TS_.306.C_Whole_Genome_Sequencing_Cigna_eff01.01.2025_pub09.10.2024.pdf => 36bb5264dda1b2027dcdfdd32a714204\n",
      "BCBS_FEP_204102 Whole Exome and.pdf => d5e9701c13de1dca302ad0ce45524039\n",
      "Cigna_MOL.TS_.238.A_BRCA_Analysis_eff01.01.2025_pub09.10.2024_1.pdf => 626eac4d60df057ea93ece78f8cc3dfc\n",
      "United Healthcare_genetic-testing-hereditary-cancer.pdf => c69485372670ce1d12aa8f61d83a06fd\n",
      "United Healthcare_whole-exome-and-whole-genome-sequencing.pdf => 4fadf6b3ca9d4d08131cb31365e3aa7d\n",
      "Cigna_MOL.TS_.235.C_Whole_Exome_Sequencing_Cigna_eff01.01.2025_pub09.20.2024.pdf => ad2eb3a750b767e32ff847032f0e8e03\n",
      "United Healthcare_chromosome-microarray-testing.pdf => 8a7d5f974648c666b635eae9e03277e7\n"
     ]
    }
   ],
   "source": [
    "answer_policy_dir = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/policy_answer_real\"\n",
    "for filename in os.listdir(answer_policy_dir):\n",
    "    file_path = os.path.join(answer_policy_dir, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            content = f.read()\n",
    "            md5_hash = hashlib.md5(content).hexdigest()\n",
    "        print(f\"{filename} => {md5_hash}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QnAExecutorWithPDF:\n",
    "    uploaded_files_cache = {} # Class-level cache: MD5 â†’ uploaded OpenAI file_id\n",
    "    def __init__(self, questions_list, llm_model=\"chatgpt\", openai_api_key=None):\n",
    "        # Initialize with questions, model name, and API key\n",
    "        self.questions_list = questions_list\n",
    "        self.formatted_questions = self.format_questions()\n",
    "        self.llm_model = llm_model\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.assistant_id = None\n",
    "        if openai_api_key:\n",
    "            self.openai_client = OpenAI(api_key=openai_api_key)\n",
    "        else:\n",
    "            self.openai_client = None\n",
    "        if self.openai_client:\n",
    "            self.assistant_id = self._create_reusable_assistant()\n",
    "\n",
    "        self.pdf_base_path = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/policy_answer_real\"\n",
    "\n",
    "        self.md5_to_pdf = {\n",
    "            \"c5c2b854957d06467835e88a963d0c82\": \"BCBS_FEP_20402 Germline Genetic Testing for.pdf\",\n",
    "            \"8340e5b0ce4959eccfb2cb295edb47f3\": \"BCBS_FEP_20459 Genetic Testing for Developmental.pdf\",\n",
    "            \"d5e9701c13de1dca302ad0ce45524039\": \"BCBS_FEP_204102 Whole Exome and.pdf\",\n",
    "            \"36bb5264dda1b2027dcdfdd32a714204\": \"Cigna_MOL.TS_.306.C_Whole_Genome_Sequencing_Cigna_eff01.01.2025_pub09.10.2024.pdf\",\n",
    "            \"626eac4d60df057ea93ece78f8cc3dfc\": \"Cigna_MOL.TS_.238.A_BRCA_Analysis_eff01.01.2025_pub09.10.2024_1.pdf\",\n",
    "            \"c69485372670ce1d12aa8f61d83a06fd\": \"United Healthcare_genetic-testing-hereditary-cancer.pdf\",\n",
    "            \"8a7d5f974648c666b635eae9e03277e7\": \"United Healthcare_chromosome-microarray-testing.pdf\",\n",
    "            \"dd74cd39fca15b7b0888b16ce1da2014\": \"Cigna_MOL.TS_.150A_CMA_for_Developmental_Disorders_and_Prenatal_Diagnosis_eff01.01.2025_pub09.10.2024_1.pdf\",\n",
    "            \"4fadf6b3ca9d4d08131cb31365e3aa7d\": \"United Healthcare_whole-exome-and-whole-genome-sequencing.pdf\",\n",
    "            \"ad2eb3a750b767e32ff847032f0e8e03\": \"Cigna_MOL.TS_.235.C_Whole_Exome_Sequencing_Cigna_eff01.01.2025_pub09.20.2024.pdf\",\n",
    "            \"49b51c0399e5563339f32a9f24f20641\": \"Cigna_MOL.TS_.344.A_Chromosomal_Microarray_Solid_Tumors_eff01.01.2025_pub09.11.2024_0.pdf\"\n",
    "        }\n",
    "\n",
    "    def _create_reusable_assistant(self):\n",
    "        \"\"\"Create assistant once and reuse for all cases\"\"\"\n",
    "        json_example = \"\"\"{\n",
    "  \"Q0\": \"WES\",\n",
    "  \"Q1\": \"Yes\",\n",
    "  \"Q2\": \"Not Specified\",\n",
    "  \"Q3\": \"Not Specified\",\n",
    "  \"Q4\": \"No\",\n",
    "  \"Q5\": \"No\", \n",
    "  \"Q6\": \"Not Specified\",\n",
    "  \"Q7\": \"81415\",\n",
    "  \"Q8\": \"No\"\n",
    "}\"\"\"\n",
    "        assistant = self.openai_client.beta.assistants.create(\n",
    "            name=\"Insurance Policy Analyzer\",\n",
    "            instructions=f\"\"\"\n",
    "You are a clinical insurance assistant specializing in genetic testing coverage policies.\n",
    "You MUST answer in JSON format only.\n",
    "\n",
    "You will be given:\n",
    "\n",
    "1. Patient clinical information for answering questions.\n",
    "2. Official insurance policy document text (strictly use this policy content for answering questions).\n",
    "\n",
    "Instructions:\n",
    "- Answer all questions strictly based on the insurance policy document provided.\n",
    "- Do NOT refer to general guidelines or policies from other insurance providers.\n",
    "- If policy document does not clearly specify rules, you MAY use patient's clinical information to infer answers carefully.\n",
    "- Do NOT assume coverage criteria from other insurers or general clinical guidelines unless explicitly stated in the policy.\n",
    "- Output answers in JSON format ONLY.\n",
    "\n",
    "Focus on sections for uploaded policy document:\n",
    "- **Age criteria**\n",
    "- **Medical necessity criteria**\n",
    "- **Prior test criteria**\n",
    "- **Family history information** \n",
    "- **Related CPT codes**\n",
    "- **Coverage criteria**\n",
    "- **Counseling / Provider criteria**\n",
    "\n",
    "Based on the uploaded policy document and patient information, answer these questions:\n",
    "{self.formatted_questions}\n",
    "\n",
    "Output your answers in JSON format only, with no explanation.\n",
    "Your response must follow this exact structure:\n",
    "{json_example}\n",
    "\n",
    "Answer options for each question:\n",
    "- Q0: [\"WES\", \"WGS\", \"BRCA1/2\", \"CMA\"]\n",
    "- Q1: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q2: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q3: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q4: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q5: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q6: [\"Yes\", \"No\", \"Not Specified\"]\n",
    "- Q7: [\"81162\", \"81277\", \"81228\", \"81415\", \"81425\", \"Not Specified\"]\n",
    "- Q8: [\"Yes\", \"No\"]\n",
    "\"\"\",\n",
    "            model=\"gpt-4o\",\n",
    "            tools=[{\"type\": \"file_search\"}]\n",
    "        )\n",
    "        return assistant.id\n",
    "\n",
    "    def get_pdf_path_from_case(self, case_data):\n",
    "        '''Resolve the expected PDF path from case's MD5'''\n",
    "        if 'expected_md5' not in case_data:\n",
    "            raise ValueError(f\"Case {case_data.get('id', 'Unknown')} does not have 'expected_md5' field\")\n",
    "\n",
    "        md5_hash = case_data['expected_md5']\n",
    "\n",
    "        # Just in case md5 Unknown cases\n",
    "        if md5_hash == \"UNKNOWN\":\n",
    "            print(f\"âš ï¸ {case_data.get('id', 'Unknown')}: UNKNOWN MD5 - proceeding without PDF\")\n",
    "            return None, md5_hash\n",
    "\n",
    "        if md5_hash not in self.md5_to_pdf:\n",
    "            print(f\"âš ï¸ MD5 hash {md5_hash} not found in mapping - proceeding without PDF\")\n",
    "            return None, md5_hash\n",
    "\n",
    "        pdf_filename = self.md5_to_pdf[md5_hash]\n",
    "        pdf_path = os.path.join(self.pdf_base_path, pdf_filename)\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"âš ï¸ PDF file does not exist: {pdf_path} - proceeding without PDF\")\n",
    "            return None, md5_hash\n",
    "\n",
    "        print(f\"âœ… {case_data['id']}: Found PDF - {pdf_filename} - {md5_hash}\")\n",
    "        return pdf_path, md5_hash\n",
    "\n",
    "    def upload_pdf_to_openai(self, pdf_path, md5_hash):\n",
    "        '''Upload PDF to OpenAI; reuse cached file if available'''\n",
    "        if md5_hash in self.uploaded_files_cache:\n",
    "            cached_file_id = self.uploaded_files_cache[md5_hash]\n",
    "            print(f\"âœ… Utilizing cached file: {cached_file_id} (MD5: {md5_hash})\")\n",
    "            return cached_file_id\n",
    "        \n",
    "        try:\n",
    "            with open(pdf_path, \"rb\") as file:\n",
    "                uploaded_file = self.openai_client.files.create(\n",
    "                    file=file,\n",
    "                    purpose=\"assistants\"\n",
    "                )\n",
    "            self.uploaded_files_cache[md5_hash] = uploaded_file.id\n",
    "            print(f\"âœ… OpenAI upload successed: {uploaded_file.id} (MD5: {md5_hash})\")\n",
    "            return uploaded_file.id\n",
    "        except Exception as e:\n",
    "            print(f\"â— OpenAI upload failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def format_question_block(self, q, indent=2):\n",
    "        '''Format a single question with its options'''\n",
    "        indent_str = \" \" * indent\n",
    "        question_line = f\"{q['question']}\"\n",
    "        question_line += f\"\\n{indent_str}Options: {', '.join(q['options'])}\"\n",
    "        return question_line\n",
    "\n",
    "    def format_questions(self):\n",
    "        '''Join all formatted questions with IDs and spacing'''\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"{q['id']}. {self.format_question_block(q)}\"\n",
    "            for q in self.questions_list\n",
    "        ])\n",
    "\n",
    "    def clean_json_response(self, response_text):\n",
    "        \"\"\"Return a parsed JSON object from an LLM response, stripping code fences and extra text.\"\"\"\n",
    "    # Clean and extract JSON from the response text\n",
    "        original = response_text.strip()\n",
    "\n",
    "    # Step 0: Check for hallucinated greeting (Perplexity fallback)\n",
    "        if \"how can I assist you\" in original.lower() or \"insurance-related questions\" in original.lower():\n",
    "            raise ValueError(\"Perplexity returned generic assistant response instead of JSON.\")\n",
    "\n",
    "    # Step 1: Try direct parsing\n",
    "        try:\n",
    "            return json.loads(original)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Step 2: Remove code block wrappers\n",
    "        cleaned = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", original, flags=re.IGNORECASE).strip()\n",
    "        try:\n",
    "            return json.loads(cleaned)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Step 3: Try to extract the first {...} JSON-like block\n",
    "        match = re.search(r\"(\\{[\\s\\S]*?\\})\", original)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "        raise ValueError(\"No valid JSON found in the response.\")\n",
    "    \n",
    "    def run_qna_with_pdf(self, case_data, qna_model=\"gpt4o\"):\n",
    "        \"\"\"run QnA with PDF document using OpenAI API.\"\"\"\n",
    "        case_id = case_data['id']\n",
    "        patient_info = case_data['patient_info']\n",
    "\n",
    "        try:\n",
    "            pdf_path, md5_hash = self.get_pdf_path_from_case(case_data)\n",
    "            if not pdf_path:\n",
    "                return None\n",
    "            file_id = self.upload_pdf_to_openai(pdf_path, md5_hash)\n",
    "\n",
    "            thread = self.openai_client.beta.threads.create()\n",
    "            self.openai_client.beta.threads.messages.create(\n",
    "                thread_id=thread.id,\n",
    "                role=\"user\",\n",
    "                content=f\"Patient Information:\\n{patient_info}\\n\\nPlease analyze the uploaded policy document and answer the questions.\",\n",
    "                attachments=[{\"file_id\": file_id, \"tools\": [{\"type\": \"file_search\"}]}]\n",
    "            )\n",
    "\n",
    "            run = self.openai_client.beta.threads.runs.create(\n",
    "                thread_id=thread.id,\n",
    "                assistant_id=self.assistant_id\n",
    "            )\n",
    "        \n",
    "            while run.status in [\"queued\", \"in_progress\"]:\n",
    "                time.sleep(1)\n",
    "                run = self.openai_client.beta.threads.runs.retrieve(\n",
    "                    thread_id=thread.id,\n",
    "                    run_id=run.id\n",
    "                )\n",
    "            \n",
    "            messages = self.openai_client.beta.threads.messages.list(thread_id=thread.id)\n",
    "            result_content = messages.data[0].content[0].text.value\n",
    "\n",
    "            result_json = self.clean_json_response(result_content)\n",
    "            final_result = result_json.copy()\n",
    "            final_result[\"md5_hash\"] = md5_hash\n",
    "\n",
    "            save_dir = f\"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/{qna_model}\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            filename = os.path.join(save_dir, f\"{case_id}_{qna_model}_qna_result_sample.json\")\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(final_result, f, indent=2)\n",
    "            print(f\"âœ… QnA result saved to {filename}\")\n",
    "            return final_result\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"â— {case_id} Error during QnA: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inputs(case_ex_path, ground_truth_path):\n",
    "    '''Load case examples and ground-truth JSON files'''\n",
    "    with open(case_ex_path, \"r\") as f:\n",
    "        case_ex = json.load(f)\n",
    "\n",
    "    with open(ground_truth_path, \"r\") as f:\n",
    "        ground_truth = json.load(f)\n",
    "\n",
    "    return case_ex, ground_truth\n",
    "\n",
    "case_path = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/sample_qna_free_text.json\"\n",
    "truth_path = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/sample_ground_truth.json\"\n",
    "\n",
    "case_ex, ground_truth = load_inputs(case_path, truth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59cabc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_file_path = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/Insurance_Genetic_Testing_QA.json\"\n",
    "\n",
    "with open(questions_file_path, \"r\") as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "questions_list = questions_data[\"questions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfb182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Starting evaluation with model: gpt-5-mini\n",
      "âœ… Case10917: Found PDF - United Healthcare_whole-exome-and-whole-genome-sequencing.pdf - 4fadf6b3ca9d4d08131cb31365e3aa7d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI upload successed: file-Ns9qCLYBoEpWX4xoj2sieN (MD5: 4fadf6b3ca9d4d08131cb31365e3aa7d)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_868/3058399.py:190: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  thread = self.openai_client.beta.threads.create()\n",
      "/tmp/ipykernel_868/3058399.py:191: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  self.openai_client.beta.threads.messages.create(\n",
      "/tmp/ipykernel_868/3058399.py:196: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  run = self.openai_client.beta.threads.runs.create(\n",
      "/tmp/ipykernel_868/3058399.py:202: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  run = self.openai_client.beta.threads.runs.retrieve(\n",
      "/tmp/ipykernel_868/3058399.py:206: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  messages = self.openai_client.beta.threads.messages.list(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Case10917 LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Q0\": \"WES\",\n",
      "  \"Q1\": \"Not Specified\",\n",
      "  \"Q2\": \"Yes\",\n",
      "  \"Q3\": \"Yes\",\n",
      "  \"Q4\": \"Not Specified\",\n",
      "  \"Q5\": \"Yes\", \n",
      "  \"Q6\": \"Yes\",\n",
      "  \"Q7\": \"81415\",\n",
      "  \"Q8\": \"Yes\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "âœ… QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/Case10917_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Case10917 Successfully executed QnA with gpt-5-mini.\n",
      "âœ… Case8051: Found PDF - BCBS_FEP_204102 Whole Exome and.pdf - d5e9701c13de1dca302ad0ce45524039\n",
      "âœ… OpenAI upload successed: file-JJhCRjwgjoKGNbeCvrns3N (MD5: d5e9701c13de1dca302ad0ce45524039)\n",
      "\n",
      "=== Case8051 LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Q0\": \"WES\",\n",
      "  \"Q1\": \"Not Specified\",\n",
      "  \"Q2\": \"Not Specified\",\n",
      "  \"Q3\": \"Yes\",\n",
      "  \"Q4\": \"Yes\",\n",
      "  \"Q5\": \"No\",\n",
      "  \"Q6\": \"Yes\",\n",
      "  \"Q7\": \"81415\",\n",
      "  \"Q8\": \"Yes\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "âœ… QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/Case8051_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Case8051 Successfully executed QnA with gpt-5-mini.\n",
      "âœ… Case11124: Found PDF - United Healthcare_whole-exome-and-whole-genome-sequencing.pdf - 4fadf6b3ca9d4d08131cb31365e3aa7d\n",
      "âœ… Utilizing cached file: file-Ns9qCLYBoEpWX4xoj2sieN (MD5: 4fadf6b3ca9d4d08131cb31365e3aa7d)\n",
      "\n",
      "=== Case11124 LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Q0\": \"WES\",\n",
      "  \"Q1\": \"Not Specified\",\n",
      "  \"Q2\": \"Yes\",\n",
      "  \"Q3\": \"Yes\",\n",
      "  \"Q4\": \"Yes\",\n",
      "  \"Q5\": \"Yes\", \n",
      "  \"Q6\": \"Yes\",\n",
      "  \"Q7\": \"81415\",\n",
      "  \"Q8\": \"Yes\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "âœ… QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/Case11124_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Case11124 Successfully executed QnA with gpt-5-mini.\n",
      "âœ… Case7376: Found PDF - United Healthcare_whole-exome-and-whole-genome-sequencing.pdf - 4fadf6b3ca9d4d08131cb31365e3aa7d\n",
      "âœ… Utilizing cached file: file-Ns9qCLYBoEpWX4xoj2sieN (MD5: 4fadf6b3ca9d4d08131cb31365e3aa7d)\n",
      "\n",
      "=== Case7376 LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Q0\": \"WES\",\n",
      "  \"Q1\": \"Not Specified\",\n",
      "  \"Q2\": \"Yes\",\n",
      "  \"Q3\": \"Not Specified\",\n",
      "  \"Q4\": \"No\",\n",
      "  \"Q5\": \"No\",\n",
      "  \"Q6\": \"Yes\",\n",
      "  \"Q7\": \"81415\",\n",
      "  \"Q8\": \"No\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "âœ… QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/Case7376_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Case7376 Successfully executed QnA with gpt-5-mini.\n",
      "âœ… Case10451: Found PDF - United Healthcare_whole-exome-and-whole-genome-sequencing.pdf - 4fadf6b3ca9d4d08131cb31365e3aa7d\n",
      "âœ… Utilizing cached file: file-Ns9qCLYBoEpWX4xoj2sieN (MD5: 4fadf6b3ca9d4d08131cb31365e3aa7d)\n",
      "\n",
      "=== Case10451 LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Q0\": \"WES\",\n",
      "  \"Q1\": \"Yes\",\n",
      "  \"Q2\": \"Yes\",\n",
      "  \"Q3\": \"Yes\",\n",
      "  \"Q4\": \"Yes\",\n",
      "  \"Q5\": \"Not Specified\",\n",
      "  \"Q6\": \"No\",\n",
      "  \"Q7\": \"81415\",\n",
      "  \"Q8\": \"Yes\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "âœ… QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/Case10451_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Case10451 Successfully executed QnA with gpt-5-mini.\n",
      "âœ… Case19321: Found PDF - United Healthcare_whole-exome-and-whole-genome-sequencing.pdf - 4fadf6b3ca9d4d08131cb31365e3aa7d\n",
      "âœ… Utilizing cached file: file-Ns9qCLYBoEpWX4xoj2sieN (MD5: 4fadf6b3ca9d4d08131cb31365e3aa7d)\n",
      "\n",
      "=== Case19321 LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Q0\": \"WES\",\n",
      "  \"Q1\": \"Yes\",\n",
      "  \"Q2\": \"Yes\",\n",
      "  \"Q3\": \"Yes\",\n",
      "  \"Q4\": \"Yes\",\n",
      "  \"Q5\": \"Yes\", \n",
      "  \"Q6\": \"Yes\",\n",
      "  \"Q7\": \"81415\",\n",
      "  \"Q8\": \"Yes\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "âœ… QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/Case19321_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Case19321 Successfully executed QnA with gpt-5-mini.\n",
      "âœ… Case9349: Found PDF - BCBS_FEP_204102 Whole Exome and.pdf - d5e9701c13de1dca302ad0ce45524039\n",
      "âœ… Utilizing cached file: file-JJhCRjwgjoKGNbeCvrns3N (MD5: d5e9701c13de1dca302ad0ce45524039)\n",
      "\n",
      "=== Case9349 LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Q0\": \"WES\",\n",
      "  \"Q1\": \"Not Specified\",\n",
      "  \"Q2\": \"Yes\",\n",
      "  \"Q3\": \"Yes\",\n",
      "  \"Q4\": \"Yes\",\n",
      "  \"Q5\": \"Yes\",\n",
      "  \"Q6\": \"Yes\",\n",
      "  \"Q7\": \"81415\",\n",
      "  \"Q8\": \"Yes\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "âœ… QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/Case9349_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Case9349 Successfully executed QnA with gpt-5-mini.\n",
      "âœ… Case10363: Found PDF - United Healthcare_whole-exome-and-whole-genome-sequencing.pdf - 4fadf6b3ca9d4d08131cb31365e3aa7d\n",
      "âœ… Utilizing cached file: file-Ns9qCLYBoEpWX4xoj2sieN (MD5: 4fadf6b3ca9d4d08131cb31365e3aa7d)\n",
      "\n",
      "=== Case10363 LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Q0\": \"WGS\",\n",
      "  \"Q1\": \"Not Specified\",\n",
      "  \"Q2\": \"Not Specified\",\n",
      "  \"Q3\": \"Yes\",\n",
      "  \"Q4\": \"Yes\",\n",
      "  \"Q5\": \"No\", \n",
      "  \"Q6\": \"Yes\",\n",
      "  \"Q7\": \"81425\",\n",
      "  \"Q8\": \"Yes\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "âœ… QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/Case10363_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Case10363 Successfully executed QnA with gpt-5-mini.\n",
      "âœ… Case4512: Found PDF - BCBS_FEP_204102 Whole Exome and.pdf - d5e9701c13de1dca302ad0ce45524039\n",
      "âœ… Utilizing cached file: file-JJhCRjwgjoKGNbeCvrns3N (MD5: d5e9701c13de1dca302ad0ce45524039)\n",
      "\n",
      "=== Case4512 LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Q0\": \"WGS\",\n",
      "  \"Q1\": \"Not Specified\",\n",
      "  \"Q2\": \"Not Specified\",\n",
      "  \"Q3\": \"Yes\",\n",
      "  \"Q4\": \"Yes\",\n",
      "  \"Q5\": \"Yes\", \n",
      "  \"Q6\": \"Yes\",\n",
      "  \"Q7\": \"Not Specified\",\n",
      "  \"Q8\": \"No\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "âœ… QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/Case4512_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Case4512 Successfully executed QnA with gpt-5-mini.\n",
      "âœ… Case18257: Found PDF - Cigna_MOL.TS_.238.A_BRCA_Analysis_eff01.01.2025_pub09.10.2024_1.pdf - 626eac4d60df057ea93ece78f8cc3dfc\n",
      "âœ… OpenAI upload successed: file-9YZWdVPZ6KxRvCrKqKPMo6 (MD5: 626eac4d60df057ea93ece78f8cc3dfc)\n",
      "\n",
      "=== Case18257 LLM Response ===\n",
      "```json\n",
      "{\n",
      "  \"Q0\": \"BRCA1/2\",\n",
      "  \"Q1\": \"Yes\",\n",
      "  \"Q2\": \"Not Specified\",\n",
      "  \"Q3\": \"Yes\",\n",
      "  \"Q4\": \"Yes\",\n",
      "  \"Q5\": \"Yes\",\n",
      "  \"Q6\": \"No\",\n",
      "  \"Q7\": \"81162\",\n",
      "  \"Q8\": \"No\"\n",
      "}\n",
      "```\n",
      "==================================================\n",
      "âœ… QnA result saved to /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/Case18257_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Case18257 Successfully executed QnA with gpt-5-mini.\n"
     ]
    }
   ],
   "source": [
    "executor = QnAExecutorWithPDF(\n",
    "    questions_list=questions_list,\n",
    "    llm_model=\"chatgpt\", \n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "models_to_test = [\"gpt-5-mini\", \"gpt-5\"]\n",
    "failed_cases = [] \n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"\\nğŸ”„ Starting evaluation with model: {model}\")\n",
    "    for case in case_ex:\n",
    "        case_id = case[\"id\"]\n",
    "        try:\n",
    "            result = executor.run_qna_with_pdf(case_data=case, qna_model=model)\n",
    "            if result:\n",
    "                print(f\"âœ… {case_id} Success\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ {case_id} Skipped (no PDF)\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {case_id} Failed: {e}\")\n",
    "            failed_cases.append({\"case_id\": case_id, \"model\": model, \"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_qna_jsons_to_csv(base_folder_path, model_name):\n",
    "    \"\"\"Merge all QnA JSON results for a specific model into a single CSV\"\"\"\n",
    "    \n",
    "    # Model-specific folder path\n",
    "    model_folder_path = os.path.join(base_folder_path, model_name)\n",
    "    \n",
    "    # Check if model folder exists\n",
    "    if not os.path.exists(model_folder_path):\n",
    "        print(f\"âš ï¸ Model folder not found: {model_folder_path}\")\n",
    "        return None\n",
    "    \n",
    "    all_data = []\n",
    "    pattern = f\"_{model_name}_qna_result_sample.json\"\n",
    "\n",
    "    # Check all files in the model folder\n",
    "    for file in os.listdir(model_folder_path):\n",
    "        if file.endswith(pattern):\n",
    "            # Extract case_id (case123_modelname_qna_result_sample.json -> case123)\n",
    "            case_id = file.replace(f\"_{model_name}_qna_result_sample.json\", \"\")\n",
    "            json_path = os.path.join(model_folder_path, file)\n",
    "\n",
    "            try:\n",
    "                with open(json_path, \"r\", encoding='utf-8') as f:\n",
    "                    result = json.load(f)\n",
    "                    \n",
    "                # Flatten each JSON result\n",
    "                flat_result = {\"case_id\": case_id}\n",
    "                \n",
    "                for k, v in result.items():\n",
    "                    if isinstance(v, list):\n",
    "                        # Convert list to string separated by semicolons\n",
    "                        flat_result[k] = \"; \".join(map(str, v))\n",
    "                    elif isinstance(v, dict):\n",
    "                        # Convert dictionary to string\n",
    "                        flat_result[k] = str(v)\n",
    "                    else:\n",
    "                        flat_result[k] = v\n",
    "\n",
    "                all_data.append(flat_result)\n",
    "                print(f\"âœ… Processed: {file}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"â— Failed to parse {file}: {e}\")\n",
    "\n",
    "    # Save as CSV file\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Natural sort by case_id (case1, case2, ..., case10, case11)\n",
    "        def natural_sort_key(case_id):\n",
    "            # Extract number from case123 -> 123\n",
    "            numbers = re.findall(r'\\d+', case_id)\n",
    "            return int(numbers[0]) if numbers else 0\n",
    "        \n",
    "        df['sort_key'] = df['case_id'].apply(natural_sort_key)\n",
    "        df = df.sort_values('sort_key').drop('sort_key', axis=1)\n",
    "        \n",
    "        # Define desired column order\n",
    "        desired_columns = [\n",
    "            'case_id', 'Q0', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'md5_hash'\n",
    "        ]\n",
    "\n",
    "        # Select only existing columns\n",
    "        existing_columns = [col for col in desired_columns if col in df.columns]\n",
    "        remaining_columns = [col for col in df.columns if col not in existing_columns]\n",
    "        final_columns = existing_columns + remaining_columns\n",
    "        \n",
    "        # Reorder columns in desired order\n",
    "        df = df[final_columns]\n",
    "\n",
    "        # Save CSV in the model folder\n",
    "        output_csv_path = os.path.join(model_folder_path, f\"{model_name}_merged_results_sample.csv\")\n",
    "        df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        print(f\"\\nâœ… Successfully merged {len(all_data)} files for model: {model_name}!\")\n",
    "        print(f\"ğŸ“ CSV saved to: {output_csv_path}\")\n",
    "        print(f\"ğŸ“Š Total records: {len(all_data)}\")\n",
    "        print(f\"ğŸ“‹ Columns in order: {list(df.columns)}\")\n",
    "        \n",
    "        return output_csv_path\n",
    "    else:\n",
    "        print(f\"âš ï¸ No valid QnA result files found in: {model_folder_path}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_case_accuracies(csv_path, ground_truth):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    questions = [\"Q0\", \"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\", \"Q6\", \"Q7\", \"Q8\"]\n",
    "    \n",
    "    case_results = []\n",
    "    for _, row in df.iterrows():\n",
    "        case_id = row[\"case_id\"]\n",
    "        gt = ground_truth.get(case_id, {})\n",
    "        \n",
    "        correct = sum(1 for q in questions if str(row.get(q, \"\")).strip() == str(gt.get(q, \"\")).strip())\n",
    "        total = 9\n",
    "        accuracy = (correct / total) * 100\n",
    "        \n",
    "        case_results.append({\n",
    "            \"case_id\": case_id,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total, \n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "    \n",
    "    return case_results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_model(csv_path, gold_answers, model_name):\n",
    "    \"\"\"Evaluate a single model's results and return case-level statistics DataFrame.\"\"\"\n",
    "    \n",
    "    # Check existence of the CSV file\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "    \n",
    "    # Check gold_answers \n",
    "    if not isinstance(gold_answers, dict):\n",
    "        raise ValueError(\"gold_answers must be a dictionary\")\n",
    "    \n",
    "    df_results = pd.read_csv(csv_path)\n",
    "    case_level_stats = []\n",
    "    \n",
    "    print(f\"ğŸ”„ Evaluating {len(df_results)} cases for model: {model_name}\")\n",
    "    \n",
    "    for _, row in df_results.iterrows():\n",
    "        case_id = row['case_id']\n",
    "        \n",
    "        gold_result = gold_answers.get(case_id)\n",
    "        if gold_result is None:\n",
    "            print(f\"âš ï¸ No gold standard found for {case_id}\")\n",
    "            continue\n",
    "        \n",
    "        predicted_result = row.to_dict()\n",
    "        \n",
    "        correct_count = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        # Only evaluate Q0-Q8 questions\n",
    "        for qid in [\"Q0\", \"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\", \"Q6\", \"Q7\", \"Q8\"]:\n",
    "            if qid not in gold_result:\n",
    "                continue\n",
    "                \n",
    "            pred_answer = predicted_result.get(qid, \"\")\n",
    "            gold_answer = gold_result.get(qid, \"\")\n",
    "            \n",
    "            # Handle NaN cases\n",
    "            if pd.isna(pred_answer):\n",
    "                pred_answer = \"\"\n",
    "            if pd.isna(gold_answer):\n",
    "                gold_answer = \"\"\n",
    "\n",
    "            # Handle list cases\n",
    "            if isinstance(pred_answer, list):\n",
    "                pred_answer = \", \".join(map(str, pred_answer))\n",
    "            if isinstance(gold_answer, list):\n",
    "                gold_answer = \", \".join(map(str, gold_answer))\n",
    "            \n",
    "            pred_answer = str(pred_answer).strip()\n",
    "            gold_answer = str(gold_answer).strip()\n",
    "\n",
    "            is_correct = pred_answer == gold_answer\n",
    "            score = 1 if is_correct else 0\n",
    "            \n",
    "            total_count += 1\n",
    "            correct_count += score\n",
    "        \n",
    "        accuracy = correct_count / total_count * 100 if total_count > 0 else 0\n",
    "        \n",
    "        case_stats = {\n",
    "            \"case_id\": case_id,\n",
    "            \"model_name\": model_name,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"total_count\": total_count\n",
    "        }\n",
    "        \n",
    "        case_level_stats.append(case_stats)\n",
    "        print(f\"âœ… {case_id}: {accuracy:.2f}% accuracy ({correct_count}/{total_count})\")\n",
    "    \n",
    "    if not case_level_stats:\n",
    "        print(f\"âš ï¸ No cases were successfully evaluated for {model_name}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return pd.DataFrame(case_level_stats)\n",
    "\n",
    "def compare_multiple_models(model_results, output_dir):\n",
    "    \"\"\"Compare multiple models and generate hierarchical analysis files.\"\"\"\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    individual_dir = os.path.join(output_dir, \"individual_models\")\n",
    "    os.makedirs(individual_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Save individual model case-level statistics\n",
    "    all_model_data = []\n",
    "    model_summary_stats = []\n",
    "    \n",
    "    for model_name, case_df in model_results.items():\n",
    "        if case_df.empty:\n",
    "            continue\n",
    "            \n",
    "        # Save individual model case statistics\n",
    "        individual_path = os.path.join(individual_dir, f\"case_statistics_{model_name}.csv\")\n",
    "        case_df.to_csv(individual_path, index=False)\n",
    "        print(f\"âœ… Individual model statistics saved: {individual_path}\")\n",
    "        \n",
    "        # Collect data for comparison\n",
    "        all_model_data.append(case_df)\n",
    "        \n",
    "        # Calculate summary statistics for each model\n",
    "        accuracies = case_df[\"accuracy\"].values\n",
    "        model_stats = {\n",
    "            \"model_name\": model_name,\n",
    "            \"total_cases\": len(case_df),\n",
    "            \"mean_accuracy\": accuracies.mean(),\n",
    "            \"std_accuracy\": accuracies.std(),\n",
    "            \"min_accuracy\": accuracies.min(),\n",
    "            \"max_accuracy\": accuracies.max(),\n",
    "            \"median_accuracy\": np.median(accuracies)\n",
    "        }\n",
    "        model_summary_stats.append(model_stats)\n",
    "\n",
    "    # 2. Create model comparison file (model-wise summary statistics)\n",
    "    if model_summary_stats:\n",
    "        model_comparison_df = pd.DataFrame(model_summary_stats)\n",
    "        \n",
    "        # Add rankings\n",
    "        model_comparison_df[\"accuracy_rank\"] = model_comparison_df[\"mean_accuracy\"].rank(ascending=False, method=\"min\")\n",
    "        model_comparison_df = model_comparison_df.sort_values(\"mean_accuracy\", ascending=False)\n",
    "        \n",
    "        # Save model comparison file\n",
    "        comparison_path = os.path.join(output_dir, \"model_comparison.csv\")\n",
    "        model_comparison_df.to_csv(comparison_path, index=False)\n",
    "        print(f\"âœ… Model comparison saved: {comparison_path}\")\n",
    "    \n",
    "    # 3. Create overall analysis file (experiment wide statistics)\n",
    "    if all_model_data:\n",
    "        # Combine all models' data for overall experiment analysis\n",
    "        combined_df = pd.concat(all_model_data, ignore_index=True)\n",
    "        \n",
    "        # Calculate overall experiment statistics (across all models and cases)\n",
    "        all_accuracies = combined_df[\"accuracy\"].values\n",
    "        total_questions = combined_df[\"total_count\"].sum()\n",
    "        total_correct = combined_df[\"correct_count\"].sum()\n",
    "        \n",
    "        overall_experiment_stats = {\n",
    "            \"total_models\": len(model_results),\n",
    "            \"total_cases\": len(combined_df),\n",
    "            \"total_questions_answered\": total_questions,\n",
    "            \"total_correct_answers\": total_correct,\n",
    "            \"overall_accuracy\": (total_correct / total_questions * 100) if total_questions > 0 else 0,\n",
    "            \"mean_case_accuracy\": all_accuracies.mean(),\n",
    "            \"std_case_accuracy\": all_accuracies.std(),\n",
    "            \"min_case_accuracy\": all_accuracies.min(),\n",
    "            \"max_case_accuracy\": all_accuracies.max(),\n",
    "            \"median_case_accuracy\": np.median(all_accuracies),\n",
    "            \"cases_with_perfect_score\": len(combined_df[combined_df[\"accuracy\"] == 100]),\n",
    "            \"cases_with_zero_score\": len(combined_df[combined_df[\"accuracy\"] == 0])\n",
    "        }\n",
    "        \n",
    "        overall_df = pd.DataFrame([overall_experiment_stats])\n",
    "        \n",
    "        # Save overall analysis file\n",
    "        overall_path = os.path.join(output_dir, \"overall_analysis.csv\")\n",
    "        overall_df.to_csv(overall_path, index=False)\n",
    "        print(f\"âœ… Overall analysis saved: {overall_path}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nğŸ“Š Multi-Model QnA Performance Summary:\")\n",
    "        print(f\"Total models evaluated: {len(model_summary_stats)}\")\n",
    "        print(f\"\\nğŸ† Model Rankings by Mean Accuracy:\")\n",
    "        for idx, row in model_comparison_df.iterrows():\n",
    "            print(f\"{int(row['accuracy_rank'])}. {row['model_name']}: {row['mean_accuracy']:.2f}% Â± {row['std_accuracy']:.2f}%\")\n",
    "    \n",
    "    return model_comparison_df if 'model_comparison_df' in locals() else None, overall_df if 'overall_df' in locals() else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_csv_results(model_csvs, gold_answers, output_dir):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models and generate hierarchical analysis files.\n",
    "    \n",
    "    Args:\n",
    "        model_csvs (dict): Dictionary of {model_name: csv_path}\n",
    "        gold_answers (dict): Gold standard answers\n",
    "        output_dir (str): Output directory path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate each model individually\n",
    "    model_results = {}\n",
    "    for model_name, csv_path in model_csvs.items():\n",
    "        print(f\"\\nğŸš€ Starting evaluation for model: {model_name}\")\n",
    "        case_df = evaluate_single_model(csv_path, gold_answers, model_name)\n",
    "        if not case_df.empty:\n",
    "            model_results[model_name] = case_df\n",
    "    \n",
    "    # Compare multiple models and generate analysis files\n",
    "    if model_results:\n",
    "        comparison_df, overall_df = compare_multiple_models(model_results, output_dir)\n",
    "        return model_results, comparison_df, overall_df\n",
    "    else:\n",
    "        print(\"âš ï¸ No models were successfully evaluated\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245aafb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Processing model: gpt-4o\n",
      "âš ï¸ No valid QnA result files found in: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample\n",
      "âš ï¸ No CSV file generated for gpt-4o\n",
      "\n",
      "ğŸ”„ Processing model: gpt-5-nano\n",
      "âš ï¸ No valid QnA result files found in: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample\n",
      "âš ï¸ No CSV file generated for gpt-5-nano\n",
      "\n",
      "ğŸ”„ Processing model: gpt-5-mini\n",
      "âœ… Processed: Case19321_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Processed: Case8051_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Processed: Case18257_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Processed: Case10363_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Processed: Case4512_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Processed: Case7376_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Processed: Case10917_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Processed: Case11124_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Processed: Case9349_gpt-5-mini_qna_result_sample.json\n",
      "âœ… Processed: Case10451_gpt-5-mini_qna_result_sample.json\n",
      "\n",
      "âœ… Successfully merged 10 files!\n",
      "ğŸ“ CSV saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/gpt-5-mini_merged_results_sample.csv\n",
      "ğŸ“Š Total records: 10\n",
      "ğŸ“‹ Columns in order: ['case_id', 'Q0', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'pdf_used', 'md5_hash', 'match_status']\n",
      "âœ… CSV merged for gpt-5-mini: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/gpt-5-mini_merged_results_sample.csv\n",
      "ğŸ”„ Evaluating 10 cases from /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/gpt-5-mini_merged_results_sample.csv\n",
      "âœ… Case4512 ğŸ“„: 77.78% accuracy (7/9)\n",
      "âœ… Case7376 ğŸ“„: 55.56% accuracy (5/9)\n",
      "âœ… Case8051 ğŸ“„: 66.67% accuracy (6/9)\n",
      "âœ… Case9349 ğŸ“„: 66.67% accuracy (6/9)\n",
      "âœ… Case10363 ğŸ“„: 55.56% accuracy (5/9)\n",
      "âœ… Case10451 ğŸ“„: 88.89% accuracy (8/9)\n",
      "âœ… Case10917 ğŸ“„: 66.67% accuracy (6/9)\n",
      "âœ… Case11124 ğŸ“„: 77.78% accuracy (7/9)\n",
      "âœ… Case18257 ğŸ“„: 77.78% accuracy (7/9)\n",
      "âœ… Case19321 ğŸ“„: 88.89% accuracy (8/9)\n",
      "âœ… Case-level statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/case_level_statistics_gpt-5-mini.csv\n",
      "âœ… Overall statistics saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/overall_statistics_gpt-5-mini.csv\n",
      "âœ… Match status analysis saved to: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/match_status_analysis_gpt-5-mini.csv\n",
      "\n",
      "ğŸ“Š QnA Accuracy Evaluation Summary:\n",
      "Total cases evaluated: 10\n",
      "QnA Accuracy (Mean %, Std): 72.22%, 11.39%\n",
      "Min accuracy: 55.56%\n",
      "Max accuracy: 88.89%\n",
      "Median accuracy: 72.22%\n",
      "\n",
      "ğŸ“„ Match Status Statistics:\n",
      "ğŸ“„ Exact Match: 10\n",
      "â“ Unknown MD5: 0\n",
      "ğŸ“ No File Found: 0\n",
      "\n",
      "ğŸ¯ Performance by Match Status:\n",
      "ğŸ“„ Exact Match: 10 cases, 72.22% Â± 12.00%\n",
      "âœ… Evaluation completed for gpt-5-mini\n",
      "\n",
      "ğŸ”„ Processing model: gpt-5\n",
      "âš ï¸ No valid QnA result files found in: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample\n",
      "âš ï¸ No CSV file generated for gpt-5\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample\"\n",
    "output_dir = \"/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample\"\n",
    "model_csvs = {}\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"\\nğŸ”„ Converting JSON to CSV for model: {model}\")\n",
    "    csv_path = merge_qna_jsons_to_csv(base_path, model)\n",
    "    if csv_path:\n",
    "        model_csvs[model] = csv_path\n",
    "        print(f\"âœ… CSV ready for {model}: {csv_path}\")\n",
    "    else:\n",
    "        print(f\"âŒ Failed to create CSV for {model}\")\n",
    "\n",
    "if model_csvs and ground_truth:\n",
    "    output_dir = os.path.join(base_path, \"..\", \"evaluation_results\")\n",
    "    \n",
    "    model_results, comparison_df, overall_df = evaluate_csv_results(\n",
    "        model_csvs=model_csvs,\n",
    "        gold_answers=ground_truth,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Analysis complete! Results saved to: {output_dir}\")\n",
    "else:\n",
    "    print(\"âŒ Could not run evaluation - missing CSV files or gold answers\")\n",
    "\n",
    "if failed_cases:\n",
    "    print(f\"\\nâš ï¸ Failed Cases Summary ({len(failed_cases)} total):\")\n",
    "    for fail in failed_cases:\n",
    "        print(f\"  - {fail['case_id']} ({fail['qna_model']}): {fail['error']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f54f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed gpt-5-mini: 10 cases\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š OVERALL QUESTION ACCURACY (All Models Combined)\n",
      "============================================================\n",
      "Q0: 1.000 (10/10)\n",
      "Q1: 0.300 (3/10)\n",
      "Q2: 0.800 (8/10)\n",
      "Q3: 0.900 (9/10)\n",
      "Q4: 0.600 (6/10)\n",
      "Q5: 0.800 (8/10)\n",
      "Q6: 0.400 (4/10)\n",
      "Q7: 0.800 (8/10)\n",
      "Q8: 0.900 (9/10)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š MODEL-WISE QUESTION ACCURACY\n",
      "============================================================\n",
      "     Model  Q0  Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8\n",
      "gpt-5-mini 1.0 0.3 0.8 0.9 0.6 0.8 0.4 0.8 0.9\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š QUESTION DIFFICULTY RANKING\n",
      "============================================================\n",
      "Question  Overall_Accuracy Correct_Total Difficulty_Level\n",
      "      Q0               1.0         10/10             Easy\n",
      "      Q3               0.9          9/10             Easy\n",
      "      Q8               0.9          9/10             Easy\n",
      "      Q2               0.8          8/10           Medium\n",
      "      Q5               0.8          8/10           Medium\n",
      "      Q7               0.8          8/10           Medium\n",
      "      Q4               0.6          6/10           Medium\n",
      "      Q6               0.4          4/10             Hard\n",
      "      Q1               0.3          3/10             Hard\n",
      "\n",
      "âœ… Overall results saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/overall_question_accuracy_20250822_142212.csv\n",
      "âœ… Model-wise results saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/model_wise_question_accuracy_20250822_142212.csv\n",
      "âœ… Difficulty ranking saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/question_difficulty_ranking_20250822_142212.csv\n",
      "âœ… Detailed results saved: /home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/sample/detailed_question_analysis_20250822_142212.json\n"
     ]
    }
   ],
   "source": [
    "def analyze_question_accuracy(folder_path, ground_truth, models_to_test, output_dir):\n",
    "    \"\"\"Analyze question-wise accuracy - overall and by model\"\"\"\n",
    "    \n",
    "    # Store results\n",
    "    all_results = {}\n",
    "    question_columns = ['Q0', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8']\n",
    "    \n",
    "    # Collect data for each model\n",
    "    for model in models_to_test:\n",
    "        csv_path = os.path.join(folder_path, f\"{model}_merged_results_sample.csv\")\n",
    "        \n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            model_accuracy = {}\n",
    "            \n",
    "            # Calculate accuracy for each question\n",
    "            for question in question_columns:\n",
    "                if question in df.columns:\n",
    "                    correct_count = 0\n",
    "                    total_count = 0\n",
    "                    \n",
    "                    for _, row in df.iterrows():\n",
    "                        case_id = row['case_id']\n",
    "                        if case_id in ground_truth:\n",
    "                            predicted = str(row[question]).strip()\n",
    "                            actual = str(ground_truth[case_id][question]).strip()\n",
    "                            \n",
    "                            total_count += 1\n",
    "                            if predicted == actual:\n",
    "                                correct_count += 1\n",
    "                    \n",
    "                    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "                    model_accuracy[question] = {\n",
    "                        'accuracy': accuracy,\n",
    "                        'correct': correct_count,\n",
    "                        'total': total_count\n",
    "                    }\n",
    "            \n",
    "            all_results[model] = model_accuracy\n",
    "            print(f\"âœ… Processed {model}: {len(df)} cases\")\n",
    "    \n",
    "    # 1. Overall analysis across all models\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š OVERALL QUESTION ACCURACY (All Models Combined)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    overall_stats = {}\n",
    "    for question in question_columns:\n",
    "        total_correct = sum([all_results[model][question]['correct'] \n",
    "                            for model in all_results.keys() \n",
    "                            if question in all_results[model]])\n",
    "        total_cases = sum([all_results[model][question]['total'] \n",
    "                          for model in all_results.keys() \n",
    "                          if question in all_results[model]])\n",
    "        \n",
    "        overall_accuracy = total_correct / total_cases if total_cases > 0 else 0\n",
    "        overall_stats[question] = {\n",
    "            'accuracy': overall_accuracy,\n",
    "            'correct': total_correct,\n",
    "            'total': total_cases\n",
    "        }\n",
    "        \n",
    "        print(f\"{question}: {overall_accuracy:.3f} ({total_correct}/{total_cases})\")\n",
    "    \n",
    "    # 2. Model-wise analysis\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š MODEL-WISE QUESTION ACCURACY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create results table\n",
    "    results_df = pd.DataFrame()\n",
    "    for model in models_to_test:\n",
    "        if model in all_results:\n",
    "            model_row = {'Model': model}\n",
    "            for question in question_columns:\n",
    "                if question in all_results[model]:\n",
    "                    accuracy = all_results[model][question]['accuracy']\n",
    "                    model_row[question] = accuracy\n",
    "                else:\n",
    "                    model_row[question] = 0\n",
    "            results_df = pd.concat([results_df, pd.DataFrame([model_row])], ignore_index=True)\n",
    "    \n",
    "    print(results_df.round(3).to_string(index=False))\n",
    "    \n",
    "    # 3. Question difficulty ranking (easiest to hardest)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š QUESTION DIFFICULTY RANKING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    difficulty_ranking = sorted(overall_stats.items(), \n",
    "                               key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "    \n",
    "    difficulty_df = pd.DataFrame([\n",
    "        {\n",
    "            'Question': q,\n",
    "            'Overall_Accuracy': stats['accuracy'],\n",
    "            'Correct_Total': f\"{stats['correct']}/{stats['total']}\",\n",
    "            'Difficulty_Level': 'Easy' if stats['accuracy'] > 0.8 else 'Medium' if stats['accuracy'] > 0.5 else 'Hard'\n",
    "        }\n",
    "        for q, stats in difficulty_ranking\n",
    "    ])\n",
    "    \n",
    "    print(difficulty_df.to_string(index=False))\n",
    "    \n",
    "    # 4. Save results to files\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save overall results\n",
    "    overall_df = pd.DataFrame([\n",
    "        {\n",
    "            'Question': q,\n",
    "            'Accuracy': stats['accuracy'],\n",
    "            'Correct': stats['correct'],\n",
    "            'Total': stats['total']\n",
    "        }\n",
    "        for q, stats in overall_stats.items()\n",
    "    ])\n",
    "    \n",
    "    overall_output_path = os.path.join(output_dir, f\"overall_question_accuracy_{timestamp}.csv\")\n",
    "    overall_df.to_csv(overall_output_path, index=False)\n",
    "    print(f\"\\nâœ… Overall results saved: {overall_output_path}\")\n",
    "    \n",
    "    # Save model-wise results\n",
    "    model_output_path = os.path.join(output_dir, f\"model_wise_question_accuracy_{timestamp}.csv\")\n",
    "    results_df.to_csv(model_output_path, index=False)\n",
    "    print(f\"âœ… Model-wise results saved: {model_output_path}\")\n",
    "    \n",
    "    # Save difficulty ranking\n",
    "    difficulty_output_path = os.path.join(output_dir, f\"question_difficulty_ranking_{timestamp}.csv\")\n",
    "    difficulty_df.to_csv(difficulty_output_path, index=False)\n",
    "    print(f\"âœ… Difficulty ranking saved: {difficulty_output_path}\")\n",
    "    \n",
    "    # Save detailed results (JSON format for further analysis)\n",
    "    detailed_output_path = os.path.join(output_dir, f\"detailed_question_analysis_{timestamp}.json\")\n",
    "    with open(detailed_output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… Detailed results saved: {detailed_output_path}\")\n",
    "    \n",
    "    return overall_stats, results_df, difficulty_df\n",
    "\n",
    "# Run analysis\n",
    "overall_stats, model_results, difficulty_ranking = analyze_question_accuracy(\n",
    "    folder_path=folder_path,\n",
    "    ground_truth=ground_truth,\n",
    "    models_to_test=models_to_test,\n",
    "    output_dir=output_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddc061c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_868/2262711693.py:9: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  case_df_sorted = case_df.sort_values('case_id', key=lambda x: x.str.extract('(\\d+)').astype(int)[0])\n"
     ]
    }
   ],
   "source": [
    "def create_evaluation_figures(case_df, overall_df, output_dir):\n",
    "    \"\"\"Create various evaluation figures based on the case_df and overall_df data.\"\"\"\n",
    "    \n",
    "    # Font\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    \n",
    "    # 1. Bar chart of accuracy by case\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    case_df_sorted = case_df.sort_values('case_id', key=lambda x: x.str.extract('(\\d+)').astype(int)[0])\n",
    "    \n",
    "    colors = ['#ff7f7f' if acc < 70 else '#ffb347' if acc < 80 else '#90EE90' for acc in case_df_sorted['accuracy']]\n",
    "    \n",
    "    bars = plt.bar(range(len(case_df_sorted)), case_df_sorted['accuracy'], \n",
    "                   color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.xlabel('Case ID')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy by Case')\n",
    "    plt.xticks(range(len(case_df_sorted)), case_df_sorted['case_id'], rotation=45)\n",
    "    plt.ylim(0, 105)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    bar_path = os.path.join(output_dir, \"accuracy_by_case.png\")\n",
    "    plt.savefig(bar_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Case accuracy bar chart saved to: {bar_path}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Histogram of accuracy distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(case_df['accuracy'], bins=15, alpha=0.7, color='skyblue', \n",
    "             edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    mean_acc = case_df['accuracy'].mean()\n",
    "    plt.axvline(mean_acc, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {mean_acc:.1f}%')\n",
    "    \n",
    "    plt.xlabel('Accuracy (%)')\n",
    "    plt.ylabel('Number of Cases')\n",
    "    plt.title(f'Distribution of Case Accuracy\\n(Mean: {mean_acc:.1f}%, Std: {case_df[\"accuracy\"].std():.1f}%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    hist_path = os.path.join(output_dir, \"accuracy_distribution.png\")\n",
    "    plt.savefig(hist_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Accuracy distribution saved to: {hist_path}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Bar charts for correct and total question counts by case\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 3-1. Bar chart for correct answers\n",
    "    bars1 = ax1.bar(range(len(case_df_sorted)), case_df_sorted['correct_count'], color='lightgreen', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    for i, bar in enumerate(bars1):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax1.set_xlabel('Case ID')\n",
    "    ax1.set_ylabel('Correct Count')\n",
    "    ax1.set_title('Number of Correct Answers by Case')\n",
    "    ax1.set_xticks(range(len(case_df_sorted)))\n",
    "    ax1.set_xticklabels(case_df_sorted['case_id'], rotation=45)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3-2. Bar chart for total questions\n",
    "    bars2 = ax2.bar(range(len(case_df_sorted)), case_df_sorted['total_count'], \n",
    "                    color='lightcoral', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    for i, bar in enumerate(bars2):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax2.set_xlabel('Case ID')\n",
    "    ax2.set_ylabel('Total Questions')\n",
    "    ax2.set_title('Total Number of Questions by Case')\n",
    "    ax2.set_xticks(range(len(case_df_sorted)))\n",
    "    ax2.set_xticklabels(case_df_sorted['case_id'], rotation=45)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    count_path = os.path.join(output_dir, \"question_counts_by_case.png\")\n",
    "    plt.savefig(count_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Question counts chart saved to: {count_path}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Pie chart of performance categories\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    high_perf = len(case_df[case_df['accuracy'] >= 85])\n",
    "    mid_perf = len(case_df[(case_df['accuracy'] >= 70) & (case_df['accuracy'] < 85)])\n",
    "    low_perf = len(case_df[case_df['accuracy'] < 70])\n",
    "    \n",
    "    labels = ['High (â‰¥85%)', 'Medium (70-84%)', 'Low (<70%)']\n",
    "    sizes = [high_perf, mid_perf, low_perf]\n",
    "    colors = ['#90EE90', '#ffb347', '#ff7f7f']\n",
    "    \n",
    "    non_zero = [(label, size, color) for label, size, color in zip(labels, sizes, colors) if size > 0]\n",
    "    if non_zero:\n",
    "        labels, sizes, colors = zip(*non_zero)\n",
    "        \n",
    "        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', \n",
    "                startangle=90, explode=[0.05]*len(sizes))\n",
    "        plt.title('Performance Distribution by Category')\n",
    "        plt.axis('equal')\n",
    "        \n",
    "        pie_path = os.path.join(output_dir, \"performance_categories.png\")\n",
    "        plt.savefig(pie_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ“Š Performance categories pie chart saved to: {pie_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    # 5. Overall evaluation summary\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    stats_text = f\"\"\"\n",
    "    ğŸ“Š Overall Evaluation Statistics\n",
    "    \n",
    "    Total Cases Evaluated: {overall_df.iloc[0]['total_cases']}\n",
    "    \n",
    "    Accuracy Statistics:\n",
    "    â€¢ Mean: {overall_df.iloc[0]['mean_accuracy']:.2f}%\n",
    "    â€¢ Standard Deviation: {overall_df.iloc[0]['std_accuracy']:.2f}%\n",
    "    â€¢ Minimum: {overall_df.iloc[0]['min_accuracy']:.2f}%\n",
    "    â€¢ Maximum: {overall_df.iloc[0]['max_accuracy']:.2f}%\n",
    "    â€¢ Median: {overall_df.iloc[0]['median_accuracy']:.2f}%\n",
    "    â€¢ 25th Percentile: {overall_df.iloc[0]['q25_accuracy']:.2f}%\n",
    "    â€¢ 75th Percentile: {overall_df.iloc[0]['q75_accuracy']:.2f}%\n",
    "    \n",
    "    Performance Categories:\n",
    "    â€¢ High Performance (â‰¥85%): {high_perf} cases\n",
    "    â€¢ Medium Performance (70-84%): {mid_perf} cases\n",
    "    â€¢ Low Performance (<70%): {low_perf} cases\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
    "    \n",
    "    plt.title('Evaluation Summary Statistics', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    summary_path = os.path.join(output_dir, \"evaluation_summary.png\")\n",
    "    plt.savefig(summary_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š Evaluation summary saved to: {summary_path}\")\n",
    "    plt.show()\n",
    "    plt.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b75089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_csv_results_with_figures(csv_path, gold_answers, output_dir):\n",
    "    \"\"\"Evaluate CSV results and generate visualization figures.\"\"\"\n",
    "    \n",
    "    case_df, overall_df = evaluate_csv_results(csv_path, gold_answers, output_dir)\n",
    "    \n",
    "    if case_df is not None and overall_df is not None:\n",
    "        print(f\"\\nğŸ¨ Generating visualization figures...\")\n",
    "        create_evaluation_figures(case_df, overall_df, output_dir)\n",
    "        print(f\"âœ… All figures saved to: {output_dir}\")\n",
    "    \n",
    "    return case_df, overall_df\n",
    "\n",
    "\n",
    "# case_df, overall_df = evaluate_csv_results_with_figures(csv_path, ground_truth, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
