{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0198ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d37779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_df = pd.read_csv('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/final/all_rag_matched_results_merged_with_accuracy.csv')\n",
    "baseline_df = pd.read_csv('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/RAG/final/all_baseline_matched_results_merged_with_accuracy.csv')\n",
    "\n",
    "with open ('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/dataset/ground_truth.json', 'r') as f:\n",
    "    gt = json.load(f)\n",
    "\n",
    "gpt5mini_onedoc = pd.read_csv('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/final/eval/gpt-5-mini_evaluation.csv')\n",
    "gpt5_onedoc = pd.read_csv('/home/cptaswadu/new-rescue/RESCUE-n8n/eval/insurance/results/LLM_QnA/one_doc/final/eval/gpt-5_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d0b4c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QnA: gpt-5-mini, Retrieval: gpt-5-mini, Threshold: 10 -> 36 cases\n",
      "QnA: gpt-5-mini, Retrieval: gpt-5-mini, Threshold: 30 -> 40 cases\n",
      "QnA: gpt-5-mini, Retrieval: gpt-5, Threshold: 10 -> 37 cases\n",
      "QnA: gpt-5-mini, Retrieval: gpt-5, Threshold: 30 -> 39 cases\n",
      "QnA: gpt-5, Retrieval: gpt-5-mini, Threshold: 10 -> 37 cases\n",
      "QnA: gpt-5, Retrieval: gpt-5-mini, Threshold: 30 -> 39 cases\n",
      "QnA: gpt-5, Retrieval: gpt-5, Threshold: 10 -> 36 cases\n",
      "QnA: gpt-5, Retrieval: gpt-5, Threshold: 30 -> 39 cases\n",
      "[{'Case811', 'Case2007', 'Case2162', 'Case15570', 'Case15202', 'Case2666', 'Case19162', 'Case13983', 'Case2847', 'Case6747', 'Case5674', 'Case7447', 'Case14195', 'Case5834', 'Case22', 'Case14226', 'Case3223', 'Case4604', 'Case9773', 'Case8051', 'Case3494', 'Case10075', 'Case11795', 'Case12792', 'Case4196', 'Case6867', 'Case9349', 'Case4262', 'Case15458', 'Case17595', 'Case14017', 'Case18821', 'Case9408', 'Case3581', 'Case16126', 'Case17196'}, {'Case811', 'Case2007', 'Case2162', 'Case15570', 'Case14408', 'Case15202', 'Case2666', 'Case19162', 'Case13983', 'Case2847', 'Case6747', 'Case14155', 'Case7447', 'Case14195', 'Case5834', 'Case22', 'Case8485', 'Case14226', 'Case3223', 'Case4604', 'Case9773', 'Case8051', 'Case3494', 'Case10075', 'Case11795', 'Case12792', 'Case14141', 'Case4196', 'Case6867', 'Case9349', 'Case4262', 'Case15458', 'Case17595', 'Case14017', 'Case18821', 'Case9408', 'Case3581', 'Case13', 'Case16126', 'Case17196'}, {'Case811', 'Case2007', 'Case2162', 'Case15570', 'Case15202', 'Case2666', 'Case19162', 'Case13983', 'Case2847', 'Case6747', 'Case5674', 'Case7447', 'Case14195', 'Case5834', 'Case22', 'Case14226', 'Case3223', 'Case4604', 'Case9773', 'Case8051', 'Case3494', 'Case10075', 'Case11795', 'Case12792', 'Case4196', 'Case6867', 'Case9349', 'Case4262', 'Case15458', 'Case17595', 'Case14017', 'Case18821', 'Case9408', 'Case3581', 'Case13', 'Case16126', 'Case17196'}, {'Case811', 'Case2007', 'Case2162', 'Case15570', 'Case14408', 'Case2666', 'Case19162', 'Case13983', 'Case2847', 'Case6747', 'Case14155', 'Case5674', 'Case7447', 'Case14195', 'Case5834', 'Case22', 'Case8485', 'Case14226', 'Case3223', 'Case4604', 'Case9773', 'Case8051', 'Case3494', 'Case10075', 'Case11795', 'Case12792', 'Case14141', 'Case4196', 'Case6867', 'Case9349', 'Case4262', 'Case15458', 'Case17595', 'Case14017', 'Case9408', 'Case3581', 'Case13', 'Case16126', 'Case17196'}, {'Case811', 'Case2007', 'Case2162', 'Case15570', 'Case15202', 'Case2666', 'Case19162', 'Case13983', 'Case2847', 'Case6747', 'Case5674', 'Case7447', 'Case14195', 'Case5834', 'Case22', 'Case14226', 'Case3223', 'Case4604', 'Case9773', 'Case8051', 'Case3494', 'Case10075', 'Case11795', 'Case12792', 'Case4196', 'Case6867', 'Case9349', 'Case4262', 'Case15458', 'Case17595', 'Case14017', 'Case18821', 'Case9408', 'Case3581', 'Case13', 'Case16126', 'Case17196'}, {'Case811', 'Case2007', 'Case2162', 'Case15570', 'Case14408', 'Case15202', 'Case2666', 'Case19162', 'Case13983', 'Case2847', 'Case6747', 'Case14155', 'Case5674', 'Case7447', 'Case14195', 'Case5834', 'Case22', 'Case8485', 'Case14226', 'Case3223', 'Case4604', 'Case9773', 'Case8051', 'Case3494', 'Case10075', 'Case11795', 'Case14141', 'Case4196', 'Case6867', 'Case9349', 'Case4262', 'Case15458', 'Case17595', 'Case14017', 'Case18821', 'Case9408', 'Case3581', 'Case13', 'Case17196'}, {'Case811', 'Case2007', 'Case2162', 'Case15570', 'Case15202', 'Case2666', 'Case19162', 'Case13983', 'Case2847', 'Case6747', 'Case5674', 'Case7447', 'Case14195', 'Case5834', 'Case22', 'Case14226', 'Case3223', 'Case4604', 'Case9773', 'Case8051', 'Case3494', 'Case10075', 'Case11795', 'Case12792', 'Case4196', 'Case6867', 'Case9349', 'Case4262', 'Case15458', 'Case17595', 'Case14017', 'Case9408', 'Case3581', 'Case13', 'Case16126', 'Case17196'}, {'Case811', 'Case2007', 'Case2162', 'Case15570', 'Case14408', 'Case15202', 'Case2666', 'Case19162', 'Case13983', 'Case2847', 'Case6747', 'Case14155', 'Case5674', 'Case7447', 'Case14195', 'Case5834', 'Case22', 'Case8485', 'Case14226', 'Case3223', 'Case4604', 'Case9773', 'Case8051', 'Case3494', 'Case10075', 'Case11795', 'Case12792', 'Case14141', 'Case6867', 'Case9349', 'Case4262', 'Case15458', 'Case17595', 'Case14017', 'Case9408', 'Case3581', 'Case13', 'Case16126', 'Case17196'}]\n"
     ]
    }
   ],
   "source": [
    "rag_top1 = rag_df[rag_df['top_k'] == 1]\n",
    "baseline_top1 = baseline_df[baseline_df['top_k'] == 1]\n",
    "\n",
    "case_sets = []\n",
    "for qna_model in ['gpt-5-mini', 'gpt-5']:\n",
    "    for retrieval_model in ['gpt-5-mini', 'gpt-5']:\n",
    "        for threshold in [10, 30]:\n",
    "            cases = rag_top1[\n",
    "                (rag_top1['retrieval_count'] == threshold) & \n",
    "                (rag_top1['retrieval_model'] == retrieval_model) &\n",
    "                (rag_top1['qna_model'] == qna_model)\n",
    "            ]['case_id'].tolist()\n",
    "            case_sets.append(set(cases))\n",
    "            print(f\"QnA: {qna_model}, Retrieval: {retrieval_model}, Threshold: {threshold} -> {len(cases)} cases\")\n",
    "print(case_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a1b6c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common case_ids across all 8 combinations: 30\n"
     ]
    }
   ],
   "source": [
    "common_cases = case_sets[0]\n",
    "for case_set in case_sets[1:]:\n",
    "    common_cases = common_cases & case_set\n",
    "print(f\"Common case_ids across all {len(case_sets)} combinations: {len(common_cases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be1131dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    QnA Model Retrieval Model  Top K  Top C Baseline Internal-QA  \\\n",
      "0  GPT-5-Mini      gpt-5-mini      1     10   60.37%      71.11%   \n",
      "1  GPT-5-Mini      gpt-5-mini      1     30   61.11%      73.70%   \n",
      "2  GPT-5-Mini           gpt-5      1     10   62.59%      71.11%   \n",
      "3  GPT-5-Mini           gpt-5      1     30   61.48%      75.56%   \n",
      "4       GPT-5      gpt-5-mini      1     10   60.37%      67.04%   \n",
      "5       GPT-5      gpt-5-mini      1     30   59.63%      68.52%   \n",
      "6       GPT-5           gpt-5      1     10   59.63%      68.15%   \n",
      "7       GPT-5           gpt-5      1     30   57.78%      66.67%   \n",
      "\n",
      "  One-Document-QA  \n",
      "0          72.96%  \n",
      "1          72.96%  \n",
      "2          72.96%  \n",
      "3          72.96%  \n",
      "4          68.15%  \n",
      "5          68.15%  \n",
      "6          68.15%  \n",
      "7          68.15%  \n"
     ]
    }
   ],
   "source": [
    "def get_accuracies(threshold, retrieval_model, qna_model, case_ids):\n",
    "    # Internal-QA\n",
    "    rag_subset = rag_top1[\n",
    "        (rag_top1['case_id'].isin(case_ids)) &\n",
    "        (rag_top1['retrieval_count'] == threshold) & \n",
    "        (rag_top1['retrieval_model'] == retrieval_model) &\n",
    "        (rag_top1['qna_model'] == qna_model)\n",
    "    ]\n",
    "    internal_qa_acc = rag_subset['accuracy'].mean()\n",
    "    \n",
    "    # Baseline\n",
    "    baseline_subset = baseline_top1[\n",
    "        (baseline_top1['case_id'].isin(case_ids)) &\n",
    "        (baseline_top1['retrieval_count'] == threshold) & \n",
    "        (baseline_top1['retrieval_model'] == retrieval_model) &\n",
    "        (baseline_top1['qna_model'] == qna_model)\n",
    "    ]\n",
    "    baseline_acc = baseline_subset['accuracy'].mean()\n",
    "    \n",
    "    # One-Document-QA\n",
    "    if qna_model == 'gpt-5':\n",
    "        onedoc_acc = gpt5_onedoc[gpt5_onedoc['case_id'].isin(case_ids)]['accuracy'].mean()\n",
    "    else:\n",
    "        onedoc_acc = gpt5mini_onedoc[gpt5mini_onedoc['case_id'].isin(case_ids)]['accuracy'].mean()\n",
    "\n",
    "    return baseline_acc, internal_qa_acc, onedoc_acc\n",
    "\n",
    "results = []\n",
    "for qna_model in ['gpt-5-mini', 'gpt-5']:\n",
    "    for retrieval_model in ['gpt-5-mini', 'gpt-5']:\n",
    "        for threshold in [10, 30]:\n",
    "            baseline, internal, onedoc = get_accuracies(threshold, retrieval_model, qna_model, common_cases)\n",
    "            qna_name = 'GPT-5-Mini' if qna_model == 'gpt-5-mini' else 'GPT-5'\n",
    "            results.append({\n",
    "                'QnA Model': qna_name,\n",
    "                'Retrieval Model': retrieval_model,\n",
    "                'Top K': 1,\n",
    "                'Top C': threshold,\n",
    "                'Baseline': f\"{baseline:.2f}%\",\n",
    "                'Internal-QA': f\"{internal:.2f}%\",\n",
    "                'One-Document-QA': f\"{onedoc:.2f}%\"\n",
    "            })\n",
    "\n",
    "# Create DataFrame and display\n",
    "df = pd.DataFrame(results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10608155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-5-mini, Threshold: 10 -> 36 cases\n",
      "Model: gpt-5-mini, Threshold: 30 -> 40 cases\n",
      "Model: gpt-5     , Threshold: 10 -> 36 cases\n",
      "Model: gpt-5     , Threshold: 30 -> 39 cases\n"
     ]
    }
   ],
   "source": [
    "case_sets = []\n",
    "for model in ['gpt-5-mini', 'gpt-5']:\n",
    "    for threshold in [10, 30]:\n",
    "        # retrieval_model과 qna_model이 같은 경우만\n",
    "        cases = rag_top1[\n",
    "            (rag_top1['retrieval_count'] == threshold) & \n",
    "            (rag_top1['retrieval_model'] == model) &\n",
    "            (rag_top1['qna_model'] == model)\n",
    "        ]['case_id'].tolist()\n",
    "        case_sets.append(set(cases))\n",
    "        print(f\"Model: {model:10s}, Threshold: {threshold} -> {len(cases):2d} cases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ff73fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common case_ids across all 4 combinations: 33\n"
     ]
    }
   ],
   "source": [
    "common_cases = case_sets[0]\n",
    "for case_set in case_sets[1:]:\n",
    "    common_cases = common_cases & case_set\n",
    "print(f\"Common case_ids across all {len(case_sets)} combinations: {len(common_cases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27c0fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model  Top K  Top C Baseline Internal-QA One-Document-QA\n",
      "0  GPT-5-Mini      1     10   61.62%      72.05%          73.40%\n",
      "1  GPT-5-Mini      1     30   61.95%      74.07%          73.40%\n",
      "2       GPT-5      1     10   60.27%      69.36%          69.70%\n",
      "3       GPT-5      1     30   59.26%      68.01%          69.70%\n"
     ]
    }
   ],
   "source": [
    "def get_accuracies(threshold, model, case_ids):\n",
    "    \"\"\"\n",
    "    model: retrieval_model과 qna_model 모두에 사용 (같은 값)\n",
    "    \"\"\"\n",
    "    # Internal-QA\n",
    "    rag_subset = rag_top1[\n",
    "        (rag_top1['case_id'].isin(case_ids)) &\n",
    "        (rag_top1['retrieval_count'] == threshold) & \n",
    "        (rag_top1['retrieval_model'] == model) &\n",
    "        (rag_top1['qna_model'] == model)\n",
    "    ]\n",
    "    internal_qa_acc = rag_subset['accuracy'].mean()\n",
    "    \n",
    "    # Baseline\n",
    "    baseline_subset = baseline_top1[\n",
    "        (baseline_top1['case_id'].isin(case_ids)) &\n",
    "        (baseline_top1['retrieval_count'] == threshold) & \n",
    "        (baseline_top1['retrieval_model'] == model) &\n",
    "        (baseline_top1['qna_model'] == model)\n",
    "    ]\n",
    "    baseline_acc = baseline_subset['accuracy'].mean()\n",
    "    \n",
    "    # One-Document-QA\n",
    "    if model == 'gpt-5':\n",
    "        onedoc_acc = gpt5_onedoc[gpt5_onedoc['case_id'].isin(case_ids)]['accuracy'].mean()\n",
    "    else:\n",
    "        onedoc_acc = gpt5mini_onedoc[gpt5mini_onedoc['case_id'].isin(case_ids)]['accuracy'].mean()\n",
    "    \n",
    "    return baseline_acc, internal_qa_acc, onedoc_acc\n",
    "\n",
    "# Build results\n",
    "results = []\n",
    "for model in ['gpt-5-mini', 'gpt-5']:\n",
    "    for threshold in [10, 30]:\n",
    "        baseline, internal, onedoc = get_accuracies(threshold, model, common_cases)\n",
    "        model_name = 'GPT-5-Mini' if model == 'gpt-5-mini' else 'GPT-5'\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Top K': 1,\n",
    "            'Top C': threshold,\n",
    "            'Baseline': f\"{baseline:.2f}%\",\n",
    "            'Internal-QA': f\"{internal:.2f}%\",\n",
    "            'One-Document-QA': f\"{onedoc:.2f}%\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40ec95d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model  Top K  Top C Baseline Internal-QA One-Document-QA  N Cases\n",
      "0  GPT-5-Mini      1     10   61.73%      71.60%          72.84%       36\n",
      "1  GPT-5-Mini      1     30   61.94%      73.33%          71.67%       40\n",
      "2       GPT-5      1     10   60.19%      69.14%          69.75%       36\n",
      "3       GPT-5      1     30   59.83%      68.09%          70.09%       39\n"
     ]
    }
   ],
   "source": [
    "def get_accuracies(threshold, model):\n",
    "    \"\"\"\n",
    "    Internal-QA에서 필터링된 케이스를 그대로 baseline, one-doc에도 사용\n",
    "    \"\"\"\n",
    "    # Internal-QA에서 케이스 선택\n",
    "    rag_subset = rag_top1[\n",
    "        (rag_top1['retrieval_count'] == threshold) & \n",
    "        (rag_top1['retrieval_model'] == model) &\n",
    "        (rag_top1['qna_model'] == model)\n",
    "    ]\n",
    "    case_ids = rag_subset['case_id'].tolist()  # 여기서 케이스 결정!\n",
    "    internal_qa_acc = rag_subset['accuracy'].mean()\n",
    "    \n",
    "    # 같은 케이스들로 Baseline 계산\n",
    "    baseline_subset = baseline_top1[\n",
    "        (baseline_top1['case_id'].isin(case_ids)) &\n",
    "        (baseline_top1['retrieval_count'] == threshold) & \n",
    "        (baseline_top1['retrieval_model'] == model) &\n",
    "        (baseline_top1['qna_model'] == model)\n",
    "    ]\n",
    "    baseline_acc = baseline_subset['accuracy'].mean()\n",
    "    \n",
    "    # 같은 케이스들로 One-Document-QA 계산\n",
    "    if model == 'gpt-5':\n",
    "        onedoc_acc = gpt5_onedoc[gpt5_onedoc['case_id'].isin(case_ids)]['accuracy'].mean()\n",
    "    else:\n",
    "        onedoc_acc = gpt5mini_onedoc[gpt5mini_onedoc['case_id'].isin(case_ids)]['accuracy'].mean()\n",
    "    \n",
    "    return baseline_acc, internal_qa_acc, onedoc_acc, len(case_ids)\n",
    "\n",
    "results = []\n",
    "for model in ['gpt-5-mini', 'gpt-5']:\n",
    "    for threshold in [10, 30]:\n",
    "        baseline, internal, onedoc, n_cases = get_accuracies(threshold, model)\n",
    "        model_name = 'GPT-5-Mini' if model == 'gpt-5-mini' else 'GPT-5'\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Top K': 1,\n",
    "            'Top C': threshold,\n",
    "            'Baseline': f\"{baseline:.2f}%\",\n",
    "            'Internal-QA': f\"{internal:.2f}%\",\n",
    "            'One-Document-QA': f\"{onedoc:.2f}%\",\n",
    "            'N Cases': n_cases\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8cd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62c58e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPT-5-Mini QnA ===\n",
      "Retrieval: gpt-5-mini, Threshold: 10, Cases: 36\n",
      "Retrieval: gpt-5-mini, Threshold: 30, Cases: 40\n",
      "Retrieval: gpt-5, Threshold: 10, Cases: 37\n",
      "Retrieval: gpt-5, Threshold: 30, Cases: 39\n",
      "\n",
      "=== GPT-5 QnA ===\n",
      "Retrieval: gpt-5-mini, Threshold: 10, Cases: 37\n",
      "Retrieval: gpt-5-mini, Threshold: 30, Cases: 39\n",
      "Retrieval: gpt-5, Threshold: 10, Cases: 36\n",
      "Retrieval: gpt-5, Threshold: 30, Cases: 39\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# GPT-5-Mini with different retrieval models\n",
    "print(\"=== GPT-5-Mini QnA ===\")\n",
    "for retrieval_model in ['gpt-5-mini', 'gpt-5']:\n",
    "    for threshold in [10, 30]:\n",
    "        result = get_accuracies(threshold, retrieval_model, 'gpt-5-mini')\n",
    "        if result:\n",
    "            results.append({\n",
    "                'QnA Model': 'GPT-5-Mini',\n",
    "                'Retrieval Model': retrieval_model,\n",
    "                'Top K': 1,\n",
    "                'Top C': threshold,\n",
    "                'Baseline': f\"{result['baseline']:.2f}%\",\n",
    "                'Internal-QA': f\"{result['internal_qa']:.2f}%\",\n",
    "                'One-Document-QA': f\"{result['one_doc']:.2f}%\",\n",
    "                'N Cases': result['n_cases']\n",
    "            })\n",
    "            print(f\"Retrieval: {retrieval_model}, Threshold: {threshold}, Cases: {result['n_cases']}\")\n",
    "\n",
    "# GPT-5 with different retrieval models\n",
    "print(\"\\n=== GPT-5 QnA ===\")\n",
    "for retrieval_model in ['gpt-5-mini', 'gpt-5']:\n",
    "    for threshold in [10, 30]:\n",
    "        result = get_accuracies(threshold, retrieval_model, 'gpt-5')\n",
    "        if result:\n",
    "            results.append({\n",
    "                'QnA Model': 'GPT-5',\n",
    "                'Retrieval Model': retrieval_model,\n",
    "                'Top K': 1,\n",
    "                'Top C': threshold,\n",
    "                'Baseline': f\"{result['baseline']:.2f}%\",\n",
    "                'Internal-QA': f\"{result['internal_qa']:.2f}%\",\n",
    "                'One-Document-QA': f\"{result['one_doc']:.2f}%\",\n",
    "                'N Cases': result['n_cases']\n",
    "            })\n",
    "            print(f\"Retrieval: {retrieval_model}, Threshold: {threshold}, Cases: {result['n_cases']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0750ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies(threshold, qna_model, case_ids):\n",
    "    # Internal-QA\n",
    "    rag_subset = rag_top1[\n",
    "        (rag_top1['case_id'].isin(case_ids)) &\n",
    "        (rag_top1['retrieval_count'] == threshold) & \n",
    "        (rag_top1['qna_model'] == qna_model)\n",
    "    ]\n",
    "    internal_qa_acc = rag_subset['accuracy'].mean()\n",
    "    \n",
    "    # Baseline\n",
    "    baseline_subset = baseline_top1[\n",
    "        (baseline_top1['case_id'].isin(case_ids)) &\n",
    "        (baseline_top1['retrieval_count'] == threshold) & \n",
    "        (baseline_top1['qna_model'] == qna_model)\n",
    "    ]\n",
    "    baseline_acc = baseline_subset['accuracy'].mean()\n",
    "    \n",
    "    # One-Document-QA\n",
    "    if qna_model == 'gpt-5':\n",
    "        onedoc_acc = gpt5_onedoc[gpt5_onedoc['case_id'].isin(case_ids)]['accuracy'].mean()\n",
    "    else:\n",
    "        onedoc_acc = gpt5mini_onedoc[gpt5mini_onedoc['case_id'].isin(case_ids)]['accuracy'].mean()\n",
    "    \n",
    "    return baseline_acc, internal_qa_acc, onedoc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9568b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_top1 = rag_df[rag_df['top_k'] == 1]\n",
    "baseline_top1 = baseline_df[baseline_df['top_k'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "197b1658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies(threshold, qna_model):\n",
    "    # Get Internal-QA cases\n",
    "    rag_subset = rag_top1[\n",
    "        (rag_top1['retrieval_count'] == threshold) & \n",
    "        (rag_top1['qna_model'] == qna_model)\n",
    "    ]\n",
    "    case_ids = rag_subset['case_id'].tolist()\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    internal_qa_acc = rag_subset['accuracy'].mean()\n",
    "    \n",
    "    baseline_subset = baseline_top1[\n",
    "        (baseline_top1['case_id'].isin(case_ids)) &\n",
    "        (baseline_top1['retrieval_count'] == threshold) & \n",
    "        (baseline_top1['qna_model'] == qna_model)\n",
    "    ]\n",
    "    baseline_acc = baseline_subset['accuracy'].mean()\n",
    "    \n",
    "    if qna_model == 'gpt-5':\n",
    "        onedoc_acc = gpt5_onedoc[gpt5_onedoc['case_id'].isin(case_ids)]['accuracy'].mean()\n",
    "    else:\n",
    "        onedoc_acc = gpt5mini_onedoc[gpt5mini_onedoc['case_id'].isin(case_ids)]['accuracy'].mean()\n",
    "    \n",
    "    return baseline_acc, internal_qa_acc, onedoc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3647d917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t10\t62.40%\t71.39%\t72.07%\n",
      "1\t30\t62.45%\t73.70%\t71.54%\n"
     ]
    }
   ],
   "source": [
    "for threshold in [10, 30]:\n",
    "    baseline, internal, onedoc = get_accuracies(threshold, 'gpt-5-mini')\n",
    "    print(f\"1\\t{threshold}\\t{baseline:.2f}%\\t{internal:.2f}%\\t{onedoc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce36799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t10\t60.73%\t68.95%\t69.67%\n",
      "1\t30\t59.97%\t68.95%\t70.19%\n"
     ]
    }
   ],
   "source": [
    "for threshold in [10, 30]:\n",
    "    baseline, internal, onedoc = get_accuracies(threshold, 'gpt-5')\n",
    "    print(f\"1\\t{threshold}\\t{baseline:.2f}%\\t{internal:.2f}%\\t{onedoc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
