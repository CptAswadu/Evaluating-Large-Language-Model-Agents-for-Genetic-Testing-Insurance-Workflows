# ğŸ“Š Insurance Workflow Experiment Results

This folder contains experimental outputs for the evaluating LLM agents for genetic testing insurance workflow.
Raw experiment outputs are organized by iteration for retrieval tasks (name & policy), and QA task while patient-policy matching task stored in configurations.

---

# name_retrieval

Contains results for **insurance payer (in-network provider) retrieval experiments**.

## ğŸ“‚ Structure
```bash
name_retrieval/
â”œâ”€â”€ final/
â”‚   â”œâ”€â”€ iteration_1/
â”‚   â”œâ”€â”€ iteration_2/
â”‚   â””â”€â”€ iteration_3/
â””â”€â”€ evaluation_results/
```

## ğŸ”¹ final/iteration_{#}/

Raw outputs for each experimental iteration.

Each iteration stores:
- Model-specific outputs
  - Prompt-specific results (e.g., baseline, explicit_source)
- Retrieved provider names

These represent the direct outputs of LLM-based payer name generation experiments.

### Directory hierarchy

- **Model level**  
'perplexity', `gpt-4o`, 'gpt-4o-oct', `gpt-5-mini`

- **Prompt level**  
`baseline`, `explicit`

## ğŸ”¹ evaluation_results/

Processed evaluation outputs for **payer name retrieval** experiments.
This directory contains analysis-ready tables that summarize performance by
(model Ã— prompt Ã— iteration) and include GPT-4o matching criteria.

---

# policy_retrieval

Contains outputs of **insurance policy document retrieval experiments**.

These experiments evaluate whether LLMs can correctly retrieve official
genetic testing coverage policy documents.

## ğŸ“‚ Structure
```bash
policy_retrieval/
â””â”€â”€ final/
    â”œâ”€â”€ iteration_1/
    â”œâ”€â”€ iteration_2/
    â”œâ”€â”€ iteration_3/
    â”œâ”€â”€ all_assessments.csv
    â”œâ”€â”€ all_links.csv
    â”œâ”€â”€ payer_results.csv
    â””â”€â”€ policy_experiment_result.csv
```
---

## ğŸ”¹ final/iteration_{#}/

Raw policy retrieval outputs for each independent experiment run.
Each iteration follows a hierarchical structure of model_prompt_payer.


### Directory hierarchy

- **Model level**  
`gpt-4o`, `gpt-5-mini`

- **Prompt level**  
`baseline`, `keyword`, `verified`

- **Insurance provider level**  
`Aetna`, `Blue_Cross_and_Blue_Shield_Federal_Employee_Program`, `Cigna`, `United_Healthcare`

---

### Files within each provider folder

Each provider folder typically contains:

- `*_raw_response.txt`  
  Full raw LLM output for the retrieval query.

- `*_result.json`  
  Structured extraction of the LLM response, including identified links and metadata.

- `downloaded/`  
  Downloaded policy files (PDF or HTML) retrieved during the experiment.

- `downloaded_pdfs.csv`  
  List of downloaded documents with metadata (file name, source URL, status).

- `links_summary.csv`  
  Summary of all retrieved candidate links before filtering or verification.

- `md5_comparison_results.csv` (if applicable)  
  MD5 hash comparison results used to verify document uniqueness and match with ground truth.

---

## ğŸ”¹ Aggregated Result Files

The following CSV files summarize results across all iterations:

### all_assessments.csv
Contains detailed assessment records for every experiment run,
including prompt, model, insurance provider, and retrieval outcome.

### all_links.csv
Comprehensive list of all retrieved links across experiments.

### policy_experiment_result.csv
Overall retrieval experiment summary statistics,
used for reporting and manuscript figures.

---

# patient_policy_match

Contains results for **patient-policy matching**.

## ğŸ“‚ Structure
```bash
patient_policy_match/
â””â”€â”€ top1_10retrieve_gpt_5_mini_gpt_5_mini_update/
â”‚   â””â”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â””â”€â”€ top1/
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top1_matched_docs.csv
â”‚   â”‚    â”‚       â””â”€â”€ top1_unmatched_docs.csv
â””â”€â”€ top1_10retrieve_gpt_5_mini_header_openai_small/
â”‚   â””â”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â””â”€â”€ top1/
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top1_matched_docs.csv
â”‚   â”‚    â”‚       â””â”€â”€ top1_unmatched_docs.csv
â””â”€â”€ top1_10retrieve_gpt_5_mini_policy_openai_small/
â”‚   â””â”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â””â”€â”€ top1/
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top1_matched_docs.csv
â”‚   â”‚    â”‚       â””â”€â”€ top1_unmatched_docs.csv
â””â”€â”€ top1_30retrieve_gpt_5_mini_gpt_5_mini_update/
â”‚   â””â”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â””â”€â”€ top1/
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top1_matched_docs.csv
â”‚   â”‚    â”‚       â””â”€â”€ top1_unmatched_docs.csv
â””â”€â”€ top1_30retrieve_gpt_5_mini_header_openai_small/
â”‚   â””â”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â””â”€â”€ top1/
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top1_matched_docs.csv
â”‚   â”‚    â”‚       â””â”€â”€ top1_unmatched_docs.csv
â””â”€â”€ top3_10retrieve_gpt_5_mini_gpt_5_mini_update/
â”‚   â””â”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â””â”€â”€ top3/
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top3_docs.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top3_matched_docs.csv
â”‚   â”‚    â”‚       â””â”€â”€ top3_unmatched_docs.csv
â””â”€â”€ top3_10retrieve_gpt_5_mini_header_openai_small/
â”‚   â””â”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â””â”€â”€ top3/
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top3_docs.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top3_matched_docs.csv
â”‚   â”‚    â”‚       â””â”€â”€ top3_unmatched_docs.csv
â””â”€â”€ top3_10retrieve_gpt_5_mini_policy_openai_small/
â”‚   â””â”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â””â”€â”€ top1/
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_failed_cases.json
â”‚   â”‚    â”‚       â”œâ”€â”€ top3_docs.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top3_matched_docs.csv
â”‚   â”‚    â”‚       â””â”€â”€ top3_unmatched_docs.csv
â””â”€â”€ top3_30retrieve_gpt_5_mini_gpt_5_mini_update/
â”‚   â””â”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â””â”€â”€ top3/
â”‚   â”‚    â”‚       â”œâ”€â”€ correct_cases.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ incorrect_cases.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top3_docs.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top3_matched_docs.csv
â”‚   â”‚    â”‚       â””â”€â”€ top3_unmatched_docs.csv
â””â”€â”€ top3_30retrieve_gpt_5_mini_header_openai_small/
â”‚   â””â”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â””â”€â”€ top3/
â”‚   â”‚    â”‚       â”œâ”€â”€ correct_cases.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ incorrect_cases.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top3_docs.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ top3_matched_docs.csv
â”‚   â”‚    â”‚       â””â”€â”€ top3_unmatched_docs.csv
â””â”€â”€ whole_policy/
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚    â””â”€â”€ gpt_5_mini/
â”‚   â”‚    â”‚   â”œâ”€â”€ top1/
â”‚   â”‚    â”‚   â”‚   â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚   â”‚   â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚   â”‚   â””â”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚   â””â”€â”€ top3/
â”‚   â”‚    â”‚       â”œâ”€â”€ matching_summary.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rank_change_detail.csv
â”‚   â”‚    â”‚       â”œâ”€â”€ rerank_orders.csv
â”‚   â”‚    â”‚       â””â”€â”€ top3_docs.csv
â”‚   â””â”€â”€ rank_change_summary.csv
```

## ğŸ”¹ Experimental configurations

Each subfolder corresponds to a **patientâ€“policy matching** run under a specific retrieval + embedding + input setting.

### Naming pattern

#### 1) SentenceTransformer embedding (baseline embedding backbone)
- `top{k}_{c}retrieve_{rerank_model}_{QA_model}_update`  
  Uses **SentenceTransformer embeddings** with **header (policy summarization) input**.

- `whole_policy/`  
  Uses **SentenceTransformer embeddings** with **whole-policy text input**.

#### 2) OpenAI embedding (text-embedding-3-small)
- `top{k}_{c}retrieve_{rerank_model}_header_openai_small`  
  Uses **OpenAI text-embedding-3-small** with **header (policy summarization) input**.

- `top{k}_{c}retrieve_{rerank_model}_policy_openai_small`  
  Uses **OpenAI text-embedding-3-small** with **whole-policy text input**.

### Folder contents

Within each configuration, outputs are stored under:

- `retrieval/{rerank_model}/top{k}/`  
  Matching artifacts for the given `top{k}` setting, including:
  - `matching_summary.csv`: aggregate match statistics for the run
  - `rank_change_detail.csv`: rank-change comparison between cosine similarity based ranking and LLM reranking
  - `rerank_orders.csv`: raw results of LLM reranking
  - `top{k}_matched_docs.csv`, `top{k}_unmatched_docs.csv`: matched vs unmatched document selections
  - (optional) `correct_cases.csv`, `incorrect_cases.csv`: case lists for downstream QA (when generated)
  - (optional) `rerank_failed_cases.json`: rerank failure diagnostics

In addition, `whole_policy/` includes:
- `rank_change_summary.csv`: summary of reranking stability across top-k settings
---

# LLM_QnA

Contains results for **insurance coverage Question Answering**.

## ğŸ“‚ Structure
```bash
LLM_QnA/
â””â”€â”€ RAG/
â”‚   â””â”€â”€ final/
â”‚   â”‚   â””â”€â”€ final_qna_results/
â”‚   â”‚   â”‚   â”œâ”€â”€ open_ai/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ gpt_5_mini_gpt_5_mini/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ baseline/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ iter1/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ qna_raw/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_id.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_qna_requests.jsonl
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ results.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ iter2/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ qna_raw/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_id.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_qna_requests.jsonl
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ results.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ iter3/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ qna_raw/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ batch_id.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ batch_qna_requests.jsonl
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚       â””â”€â”€ results.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ iter1/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ all_correct/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ qna_raw/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_id.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_qna_requests.jsonl
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ results.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ all_incorrect/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ qna_raw/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_id.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_qna_requests.jsonl
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ results.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ iter2/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ all_correct/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ qna_raw/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_id.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_qna_requests.jsonl
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ results.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ all_incorrect/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ qna_raw/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_id.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_qna_requests.jsonl
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ results.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ iter3/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚      â”œâ”€â”€ all_correct/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚      â”‚   â”œâ”€â”€ qna_raw/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚      â”‚   â”œâ”€â”€ batch_id.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚      â”‚   â”œâ”€â”€ batch_qna_requests.jsonl
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚      â”‚   â””â”€â”€ results.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚      â””â”€â”€ all_incorrect/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚         â”œâ”€â”€ qna_raw/
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚         â”œâ”€â”€ batch_id.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚         â”œâ”€â”€ batch_qna_requests.jsonl
â”‚   â”‚   â”‚   â”‚   â”‚   â”‚         â””â”€â”€ results.csv
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ batch_summary_all_correct_openai.json
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ batch_summary_all_incorrect_openai.json
â”‚   â”‚   â”‚   â”œâ”€â”€ ST/
â”‚   â”‚   â”‚   â”œâ”€â”€ final_all_results_combined.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ final_file_updated.csv
â”‚   â”‚   â”‚   â””â”€â”€ final_results_with_accuracy_updated.csv
```

## ğŸ”¹ LLM_QnA Experimental Structure

The `LLM_QnA` module evaluates downstream question-answering performance under different retrieval conditions and embedding backbones.

### Embedding backbones

- `open_ai/`  
  Uses **OpenAI text-embedding-3-small** for patientâ€“policy matching.

- `ST/`  
  Uses **SentenceTransformer embeddings** for patientâ€“policy matching.

For both backbones, the downstream **QnA model remains fixed** (e.g., `gpt_5_mini`).

---
### Experimental modes
Within each model configuration:

#### 1ï¸âƒ£ Baseline
- No policy document provided.
- The LLM answers based solely on the patient case narrative.
- Represents an LLM-only condition.

Each configuration is repeated:
- iter1
- iter2
- iter3

Each iteration contains:
- `qna_raw/` â€“ raw JSON responses
- `batch_id.txt` â€“ OpenAI batch job identifier
- `batch_qna_requests.jsonl` â€“ submitted requests
- `results.csv` â€“ structured answer extraction

#### 2ï¸âƒ£ RAG (Documented)
Conducted under two controlled document conditions:

- `all_correct/`  
  Every sample is paired with its **ground-truth policy document**.

- `all_incorrect/`  
  Every sample is paired with a **mismatch policy document** (top-ranked by cosine similarity excluding the ground-truth).

Ouput structure and files are same with the Baseline
---

### Final aggregated outputs

- `final_all_results_combined.csv`  
  Aggregates all iteration-level results across baseline and RAG settings.

- `final_file_updated.csv`  
  Adds count-based summary statistics derived from the combined file.

- `final_results_with_accuracy_updated.csv`  
  Includes computed performance metrics:
  - Accuracy
  - Adjusted Accuracy  
  Used for final statistical analysis and manuscript tables.

---
